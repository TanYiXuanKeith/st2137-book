---
title: "Linear Regression"
---

```{r setup}
#| echo: false
#| message: false
#| warning: false
library(knitr)
library(reticulate)
if(startsWith(osVersion, "Win")){
  use_virtualenv("C:/Users/stavg/penvs/p310/")
} else {
  use_virtualenv("~/NUS/coursesTaught/penvs/p310/")
}
```

## Introduction

Regression analysis is a technique for investigating and modeling
the relationship between variables like X and Y. Here are some examples:

1. Within a country, we may wish to use per capita income (X) to estimate the 
   life expectancy (Y) of residents.
2. We may wish to use the size of a crab claw (X) to estimate the closing force 
   that it can exert (Y).
3. We may wish to use the height of a person (X) to estimate their weight (Y).

In all the above cases, we refer to $X$ as the explanatory or independent
variable. It is also sometimes referred to as a predictor. $Y$ is referred to as
the response or dependent variable. In this topic, we shall first introduce the
case of simple linear regression, where we model the $Y$ on a single $X$. In
later sections, we shall model the $Y$ on multiple $X$'s. This latter technique
is referred to as multiple linear regression.

Regression models are used for two primary purposes:

1. To understand how certain explanatory variables affect the response variable.
   This aim is typically known as *estimation*, since the primary focus is on 
   estimating the unknown parameters of the model.
2. To predict the response variable for new values of the explanatory variables.
   This is referred to as *prediction*.
   
In our course, we shall focus on the estimation aim, since prediction models require a
paradigm of their own, and are best learnt alongside a larger suite of models
e.g. decision trees, support vector machines, etc.

In subsequent sections, we shall revisit a couple of datasets from earlier topics
to run linear regression models  on them. 

::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-1}

### Concrete Data: Flow on Water

Recall the concrete dataset that we first encountered in the topic on summarising 
data. We shall go on to fit a linear regression to understand the relationship
between the output of the flow test, and the amount of water used to create the 
concrete.

```{r r-concrete-1}
#| fig-align: center
#| echo: false
#| label: fig-flow-water
#| fig-cap: "Scatterplot with simple linear regression model"
library(lattice)
concrete <- read.csv("data/concrete+slump+test/slump_test.data")
names(concrete)[c(1,11)] <- c("id", "Comp.Strength")
xyplot(FLOW.cm. ~ Water, type=c("p", "r"), data= concrete, main="Concrete data")
```

It does appear that there is a trend in the scatter plot. We shall figure out 
how to estimate this line in this topic.

:::

<br>

::: {style="background-color: #D5D1D1; padding: 20px" #exm-bike-1}

### Bike Rental Data

In the introduction to SAS, we encountered data on bike rentals in the USA over a
period of 2 years. Here, we shall attempt to model the number of registered users 
on the number of casual users.

```{r r-bike-1}
#| fig-align: center
#| echo: false
#| label: fig-reg-casual
#| fig-cap: Scatterplot of registered vs. casual bike renters
bike2 <- read.csv("data/bike2.csv")
xyplot(registered ~ casual, groups=workingday, type=c("p", "r"), data=bike2,
       auto.key = TRUE, main="Registered vs. Casual, by Working Day Status")
```

Contingent on whether the day is a working one or not, it does appear that the 
trendline is different.

:::

{{< pagebreak >}}

## Simple Linear Regression

### Formal Set-up {#sec-formal-slr}

The simple linear regression model is applicable when we have observations
$(X_i, Y_i)$ for $n$ individuals. For now, let's assume both the $X$ and $Y$ 
variables are quantitative.

The simple linear regression model is given by 

$$
Y_i = \beta_0 + \beta_1 X_i + e_i
$${#eq-slr-model}
where 

* $\beta_0$ is intercept term, 
* $\beta_1$ is the slope, and 
* $e_i$ is an error term, specific to each individual in the dataset.

$\beta_0$ and $\beta_1$ are unknown constants that need to be estimated from the
data. There is an implicit assumption in the formulation of the model that there
is a linear relationship between $Y_i$ and $X_i$. In terms of distributions, we
assume that the $e_i$ are i.i.d Normal.

$$
e_i \sim N(0, \sigma^2), \; i =1\ldots, n
$$ {#eq-error-term}

The constant variance assumption is also referred to as homoscedascity (homo-skee-das-city). 
The validity of the above assumptions will have to be checked after the model is fitted.
All in all, the assumptions imply that:

1. $E(Y_i | X_i) = \beta_0 + \beta_1 X_i$, for $i=1, \ldots, n$.
2. $Var(Y_i | X_i) = Var(e_i) = \sigma^2$, for $i=1, \ldots, n$.
3. The $Y_i$ are independent.
4. The $Y_i$'s are Normally distributed.

### Estimation {#sec-slr-estimation}

Before deploying or using the model, we need to estimate optimal values to use 
for the unknown $\beta_0$ and $\beta_1$. We shall introduce the method of 
Ordinary Least Squares (OLS) for the estimation. Let us define the 
*error Sum of Squares* to be 

$$
SS_E = S(\beta_0, \beta_1) = \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2
$$ {#eq-sse}

Then the OLS estimates of $\beta_0$ and $\beta_1$ are given by 
$$
\mathop{\arg \min}_{\beta_0, \beta_1} \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2
$$
The minimisation above can be carried out analytically, by taking partial 
derivative with respect to the two parameters and setting them to 0.

\begin{eqnarray*}
\frac{\partial S}{\partial \beta_0}  &=& -2 \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i) = 0 \\
\frac{\partial S}{\partial \beta_1}  &=& -2 \sum_{i=1}^n X_i (Y_i - \beta_0 - \beta_1 X_i) = 0 
\end{eqnarray*}

Solving and simplifying, we arrive at the following:
\begin{eqnarray*}
\hat{\beta_1} &=& \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} \\
\hat{\beta_0} &=& \bar{Y} - \hat{\beta_0} \bar{X}
\end{eqnarray*} 
where $\bar{Y} = (1/n)\sum Y_i$ and $\bar{X} = (1/n)\sum X_i$.

If we define the following sums:
\begin{eqnarray*}
S_{XY} &=& \sum_{i=1}^n X_i Y_i - \frac{(\sum_{i=1}^n X_i )(\sum_{i=1}^n Y_i )}{n} \\
S_{XX} &=& \sum_{i=1}^n X_i^2 - \frac{(\sum_{i=1}^n X_i )^2}{n}
\end{eqnarray*}
then a form convenient for computation of $\hat{\beta_1}$ is 
$$
\hat{\beta_1} = \frac{S_{XY}}{S_{XX}}
$$

Once we have the estimates, we can use @eq-slr-model to compute *fitted* values
for each observation, corresponding to our best guess of the mean of the
distributions from which the observations arose:
$$
\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i, \quad i = 1, \ldots, n
$$
As always, we can form residuals as the deviations from fitted values. 
$$
r_i = Y_i - \hat{Y}_i
$$ {#eq-res-def}
Residuals are our best guess at the unobserved error terms $e_i$. Squaring the
residuals and summing over all observations, we can arrive at the following
decomposition, which is very similar to the one in the ANOVA model:

$$
\underbrace{\sum_{i=1}^n (Y_i  - \bar{Y})^2}_{SS_T} =  
\underbrace{\sum_{i=1}^n (Y_i  - \hat{Y_i})^2}_{SS_{Res}} +
\underbrace{\sum_{i=1}^n (\hat{Y_i}  - \bar{Y})^2}_{SS_{Reg}}
$$

where 

* $SS_T$ is known as the total sum of squares.
* $SS_{Res}$ is known as the residual sum of squares.
* $SS_{Reg}$ is known as the regression sum of squares.

In our model, recall from @eq-error-term that we had assumed equal variance for 
all our observations. We can estimate $\sigma^2$ with
$$
\hat{\sigma^2} = \frac{SS_{Res}}{n-2}
$$
Our distributional assumptions lead to the following for our estimates
$\hat{\beta_0}$ and $\hat{\beta_1}$:

\begin{eqnarray}
\hat{\beta_0} &\sim& N(\beta_0,\; \sigma^2(1/n + \bar{X}^2/S_{XX})) \\
\hat{\beta_1} &\sim& N(\beta_1,\; \sigma^2/S_{XX})
\end{eqnarray}

The above are used to construct confidence intervals for $\beta_0$ and $\beta_1$,
based on $t$-distributions.

## Hypothesis Test for Model Significance {#sec-slr-F}

The first test that we introduce here is to test if the coefficient $\beta_1$ is
significantly different from 0. It is essentially a test of whether it was 
worthwhile to use a regression model of the form in @eq-slr-model, instead of a 
simple mean to represent the data.

The null and alternative hypotheses are:

\begin{eqnarray*}
H_0 &:& \beta_1 = 0\\
H_1 &:& \beta_1 \ne 0
\end{eqnarray*}

The test statistic is 

$$
F_0 = \frac{SS_{Reg}/1}{SS_{Res}/(n-2)}
$$ {#eq-f-stat}

Under the null hypothesis, $F_0 \sim F_{1,n-2}$. 

It is also possible to perform this same test as a $t$-test, using the result earlier.
The statement of the hypotheses is equivalent to the $F$-test. The test statistic
$$
T_0 = \frac{\hat{\beta_1}}{\sqrt{\hat{\sigma^2}/S_{XX}}}
$$ {#eq-t-stat}
Under $H_0$, the distribution of $T_0$ is $t_{n-2}$. This $t$-test and the earlier 
$F$-test in this section are *identical*. It can be proved that $F_0 = T_0^2$; the 
obtained $p$-values will be identical.

### Coefficient of Determination, $R^2$

The coefficient of determination $R^2$ is defined as

$$
R^2 = 1 - \frac{SS_{Res}}{SS_T} = \frac{SS_{Reg}}{SS_T}
$$
It can be interpreted as the proportion of variation in $Yi$, explained by 
the inclusion of $X_i$. Since $0 \le SS_{Res} \le SS_T$, we can easily prove that 
$0 \le R^2 \le 1$.  The larger the value of $R^2$ is, the better the model is.

When we get to the case of multiple linear regression, take note that simply 
including more variables in the model will increase $R^2$. This is undesirable; it 
is preferable to have a parsimonious model that explains the response variable well.


::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-2}

### Concrete Data Model

In this example, we focus on the estimation of the model parameters for the two 
variables we introduced in @exm-concrete-1

::: {.panel-tabset}

#### R code 

```{r r-concrete-lm}
#R 
concrete <- read.csv("data/concrete+slump+test/slump_test.data")
names(concrete)[c(1,11)] <- c("id", "Comp.Strength")
lm_flow_water <- lm(FLOW.cm. ~ Water, data=concrete)
summary(lm_flow_water)
```

#### Python code 


```{python py-concrete-lm}
#Python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols

concrete = pd.read_csv("data/concrete+slump+test/slump_test.data")
concrete.rename(columns={'No':'id', 
                         'Compressive Strength (28-day)(Mpa)':'Comp_Strength',
                         'FLOW(cm)': 'Flow'},
                inplace=True)
lm_flow_water = ols('Flow ~ Water', data=concrete).fit()
print(lm_flow_water.summary())
```

#### SAS output

![](figs/sas_concrete_flow_water_reg.png){fig-align="center" width=450}

:::

From the output, we can note that the *estimated* model for Flow ($Y$) against
Water ($X$) is:
$$
Y = -58.73 + 0.55 X
$$
The estimates are $\hat{\beta_0} = -58.73$ and $\hat{\beta_1} = 0.55$. This is the 
precise equation that was plotted in @fig-flow-water. The $R^2$ is labelled as 
"Multiple R-squared" in the R output. The value is 0.3995, which means that about 
40% of the variation in $Y$ is explained by $X$. 

A simple interpretation[^1] of the model is as follows:

> For every 1 unit increase in Water, there is an average associated increase in
> Flow rate of 0.55 units.

[^1]: This interpretation has to be taken very cautiously, especially when there are
      other explanatory variables in the model.
      
To obtain confidence intervals for the parameters, we can use the following code in
R. The Python summary already contains the confidence intervals.
      
::: {.panel-tabset}

#### R code 

```{r r-concrete-lm-ci}
#R 
confint(lm_flow_water)
```

:::

We can read off that the 95% Confidence intervals are:

* For $\beta_0$: (-85.08, -32.37)
* For $\beta_1$: (0.42, 0.68)

:::

<br>


::: {style="background-color: #D5D1D1; padding: 20px" #exm-bike-2}

### Bike Data F-test

In this example, we shall fit a simple linear regression model to the bike 
data, *constrained to the non-working days*. In other words, we shall focus on 
fitting just the blue line, from the blue points, in @fig-reg-casual.

::: {.panel-tabset}

#### R code 

```{r r-bike-lm}
#R 
bike2 <- read.csv("data/bike2.csv")
bike2_sub <- bike2[bike2$workingday == "no", ]
lm_reg_casual <- lm(registered ~ casual, data=bike2_sub)
anova(lm_reg_casual)
```

#### Python code 

```{python py-bike-lm}
#Python
bike2 = pd.read_csv("data/bike2.csv")
bike2_sub = bike2[bike2.workingday == "no"]

lm_reg_casual = ols('registered ~ casual', bike2_sub).fit()
anova_tab = sm.stats.anova_lm(lm_reg_casual,)
anova_tab
```

#### SAS output

![](figs/sas_bike_reg_casual_reg.png){fig-align="center" width=450}

:::

The output above includes the sum-of-squares that we need to perform the
$F$-test outlined in @sec-slr-F. From the output table, we can see that
$SS_{Reg} = 237654556$ and $SS_{Res} = 147386970$. The value of $F_0$ for this
dataset is 369.25. The $p$-value is extremely small ($2 \times 10^{-16}$),
indicating strong evidence against $H_0$, i.e. that $\beta_1 = 0$.


:::

<br>

Actually, if you observe carefully in @exm-concrete-2, the output from R
contains both the $t$-test for significance of $\beta_1$, and the $F$-test
statistic based on sum-of-squares. The $p$-value in both cases is 
$8.10 \times 10^{1-3}$. 

In linear regression, we almost always wish to use the model to understand what 
the mean of future observations would be. In the concrete case, we may wish to 
use the model to understand how the Flow test output values change as the amount 
of Water in the mixture changes. This is because, based on our formulation,

$$
E(Y | X) = \beta_0 + \beta_1 X
$$

After estimating the parameters, we would have:
$$
\widehat{E(Y | X)} = \hat{\beta_0} + \hat{\beta_1} X
$$

Thus we can vary the values of $X$ to study how the mean of $Y$ changes. Here 
is how we can do so in the concrete model for data.

::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-3}

### Concrete Data Predicted Means

In order to create the predicted means, we shall have to create a dataframe with
the new values for which we require the predictions. We are first going to set 
up a new matrix of $X$-values corresponding to the desired range.

::: {.panel-tabset}

#### R code 

```{r r-concrete-pred}
#| fig-align: center
#R 
new_df <- data.frame(Water = seq(160, 240, by = 5))
conf_intervals <- predict(lm_flow_water, new_df, interval="conf")

plot(concrete$Water, concrete$FLOW.cm., ylim=c(0, 100),
     xlab="Water", ylab="Flow", main="Confidence and Prediction Intervals")
abline(lm_flow_water, col="red")
lines(new_df$Water, conf_intervals[,"lwr"], col="red", lty=2)
lines(new_df$Water, conf_intervals[,"upr"], col="red", lty=2)
legend("bottomright", legend=c("Fitted line", "Lower/Upper CI"), 
       lty=c(1,2), col="red")
```

#### Python code 

```{python py-concrete-pred}
#| fig-align: center
# Python
new_df = sm.add_constant(pd.DataFrame({'Water' : np.linspace(160,240, 10)}))

predictions_out = lm_flow_water.get_prediction(new_df)

ax = concrete.plot(x='Water', y='Flow', kind='scatter', alpha=0.5 )
ax.set_title('Flow vs. Water');
ax.plot(new_df.Water, predictions_out.conf_int()[:, 0].reshape(-1), 
        color='blue', linestyle='dashed');
ax.plot(new_df.Water, predictions_out.conf_int()[:, 1].reshape(-1), 
        color='blue', linestyle='dashed');
ax.plot(new_df.Water, predictions_out.predicted, color='blue');
```

#### SAS Output

![](figs/sas_concrete_flow_water_reg_fitplot.png){fig-align="center" width=450}


:::

The fitted line is the straight line formed using $\hat{\beta_0}$ and
$\hat{\beta_1}$.  The dashed lines are 95% Confidence Intervals for $E(Y|X)$,
for varying values of $X$. They are formed by joining up the lower bounds and 
the upper bounds separately. Notice how the limits get wider the further away 
we are from $\bar{X} \approx 200$.

:::


{{< pagebreak >}}

## Multiple Linear Regression

### Formal Setup

When we have more than 1 explanatory variable, we turn to multiple linear 
regression - generalised version of what we have been dealing with so far. We would
still have observed information from $n$ individuals, but for each one, we now 
observe a vector of values:
$$
Y_i, \, X_{1,i},  \, X_{2,i}, \ldots, \, X_{p-1,i},  X_{p,i}
$$
In other words, we observe $p$ independent variables and 1 response variable for 
each individual in our dataset. The analogous equation to @eq-slr-model is
$$
Y_i = \beta_0 + \beta_1 X_{1,i} + \cdots + \beta_p  X_{p,i} + e
$$ {#eq-mlr-model}

It is easier to write things with matrices for multiple linear regression:

$$
\textbf{Y} = \begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}, \;
\textbf{X} = \begin{bmatrix}
1 & X_{1,1} & X_{2,1} & \cdots &X_{p,1}\\
1 & X_{1,2} & X_{2,2} & \cdots &X_{p,2}\\
\vdots & \vdots & \vdots & {} & \vdots \\
1 & X_{1,n} & X_{2,n} & \cdots &X_{p,n}\\
\end{bmatrix}, \;
\boldsymbol{ \beta } = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{bmatrix}, \;
\boldsymbol{e} = \begin{bmatrix}
e_1 \\
e_2 \\
\vdots \\
e_n 
\end{bmatrix}
$$

With the above matrices, we can re-write @eq-mlr-model as 
$$
\textbf{Y} = \textbf{X} \boldsymbol{\beta} + \textbf{e}
$$
We retain the same distributional assumptions as in @sec-formal-slr.

### Estimation 

Similar to @sec-slr-estimation, we can define $SS_E$ to be 
$$
SS_E = S(\beta_0, \beta_1,\ldots,\beta_p) = \sum_{i=1}^n (Y_i - \beta_0 - 
\beta_1 X_{1,i} - \cdots - \beta_p X_{p,i} )^2
$$ {#eq-mlr-sse}

Minimising the above cost function leads to the OLS estimates:
$$
\hat{\boldsymbol{\beta}} =  (\textbf{X}'\textbf{X})^{-1} \textbf{X}'\textbf{Y} 
$$
The fitted values can be computed with
$$
\hat{\textbf{Y}} = \textbf{X} \hat{\boldsymbol{\beta}} =
\textbf{X} (\textbf{X}'\textbf{X})^{-1} \textbf{X}'\textbf{Y} 
$$
Residuals are obtained as 
$$
\textbf{r} = \textbf{Y} - \hat{\textbf{Y}}
$$
Finally, we estimate $\sigma^2$ using 
$$
\hat{\sigma^2} = \frac{SS_{Res}}{n-p} = \frac{\textbf{r}' \textbf{r}}{n-p}
$$

### Coefficient of Determination, $R^2$

In the case of multiple linear regression, $R^2$ is calculated exactly as in
simple linear regression, and its interpretation remains the same:
$$
R^2 = 1 - \frac{SS_{Res}}{SS_T}
$$

However, note that $R^2$ can be inflated simply by adding more terms to the
model (even insignificant terms). Thus, we use the adjusted $R^2$, which penalizes 
us for adding more and more terms to the model:
$$
R^2_{adj} = 1 - \frac{SS_{Res}/(n-p)}{SS_T/(n-1)}
$$

### Hypothesis Tests

The $F$-test in the multiple linear regression helps determine if our regression
model provides any advantage over the simple mean model. The null and
alternative hypotheses are:

\begin{eqnarray*}
H_0 &:& \beta_1 = \beta_2 = \cdots = \beta_p = 0\\
H_1 &:& \beta_j \ne 0 \text{ for at least one } j \in \{1, 2, \ldots, p\}
\end{eqnarray*}

The test statistic is 

$$
F_1 = \frac{SS_{Reg}/p}{SS_{Res}/(n-p-1)}
$$ {#eq-f-stat}

Under the null hypothesis, $F_0 \sim F_{p,n-p-1}$. 

It is also possible to test for the significance of individual $\beta$ terms, using 
a $t$-test. The output is typically given for all the coefficients in a table.
The statement of the hypotheses pertaining to these tests is:
\begin{eqnarray*}
H_0 &:& \beta_j = 0\\
H_1 &:& \beta_j \ne 0 
\end{eqnarray*}

However, note that these $t$-tests are partial because it should be interpreted as
a test of the contribution of $\beta_j$, *given that all other terms are already in the model*.


::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-4}

### Concrete Data Multiple Linear Regression

In this second model for concrete, we add a second predictor variable, Slag. The 
updated model is 
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + e
$$
where $X_1$ corresponds to Water, and $X_2$ corresponds to Slag.

::: {.panel-tabset}

#### R code 

```{r r-concrete-lm-2}
# R 
lm_flow_water_slag <- lm(FLOW.cm. ~ Water + Slag, data=concrete)
summary(lm_flow_water_slag)
```


#### Python code

```{python py-concrete-lm-2}
# Python
lm_flow_water_slag = ols('Flow ~ Water + Slag', data=concrete).fit()
print(lm_flow_water_slag.summary())
```

#### SAS output 

![](figs/sas_concrete_flow_water_slag_reg.png){fig-align="center" width=450}

:::

The $F$-test is now concerned with the hypotheses:
\begin{eqnarray*}
H_0 &:& \beta_1 = \beta_2 = 0\\
H_1 &:& \beta_1 \ne 0 \text{ or } \beta_2 \ne 0
\end{eqnarray*}

From the output above, we can see that $F_1 = 49.17$, with a corresponding 
$p$-value of $1.3 \times 10^{-15}$. The individual $t$-tests for the coefficients all
indicate significant differences from 0. The final estimated model can be written 
as
$$
Y = -50.27 + 0.54 X_1 - 0.09 X_2
$$
Notice that the coefficients have changed slightly from the model in
@exm-concrete-2. Notice also that we have an improved $R^2$ of 0.50. However, as
we pointed out earlier, we should be using the adjusted $R^2$, which adjusts for
the additional variable included. This value is 0.49.

While we seem to have found a better model than before, we still have to assess
if all the assumptions listed in @sec-formal-slr have been met. We shall do so
in subsequent sections.

:::

{{< pagebreak >}}

## Indicator Variables

### Including a Categorical Variable {#sec-indic}

The explanatory variables in a linear regression model do not need to be continuous.
Categorical variables can also be included in the model. In order to include them,
they have to be coded using dummy variables. 

For instance, suppose that we wish to include gender in a model as $X_3$. There 
are only two possible genders in our dataset: Female and Male. We can represent
$X_3$ as an indicator variable, with

$$
X_{3,i} = 
\begin{cases}
1 & \text{individual $i$ is male}\\
0 & \text{individual $i$ is female}
\end{cases}
$$

The model (without subscripts for the $n$ individuals) is then:
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + e
$$
For females, the value of $X_3$ is 0. Hence the model reduces to
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + e
$$
On the other hand, for males, the model reduces to 
$$
Y = (\beta_0 + \beta_3) + \beta_1 X_1 + \beta_2 X_2 + e
$$
The difference between the two models is in the intercept. The other 
coefficients remain the same. 

In general, if the categorical variable has $a$ levels, we will need $a-1$
columns of indicator variables to represent it. This is in contrast to machine
learning models which use one-hot encoding. The latter encoding results in
columns that are linearly dependent if we include an intercept term in the
model.


::: {style="background-color: #D5D1D1; padding: 20px" #exm-bike-3}

### Bike Data Working Day

In this example, we shall improve on the simple linear regression model 
from @exm-bike-2. Instead of a single model for just non-working days, we shall
fit separate models for working and non-working days by including that variable
as a categorical one.

::: {.panel-tabset}

#### R code 

```{r r-bike-lm-2}
# R 
lm_reg_casual2 <- lm(registered ~ casual + workingday, data=bike2)
summary(lm_reg_casual2)
```


#### Python code

```{python py-bike-lm-2}
# Python
lm_reg_casual2 = ols('registered ~ casual + workingday', bike2).fit()
print(lm_reg_casual2.summary())
```

#### SAS output 

![](figs/sas_bike_reg_casual_workday_reg.png){fig-align="center" width=450}

:::

The estimated model is now 
$$
Y = 605 + 1.72 X_1 + 2330 X_2
$$

But $X_2 =1$ for working days and $X_2=0$ for non-working days. This results in 
two *separate* models for the two types of days:

$$
Y = 
\begin{cases}
605 + 1.72 X_1, & \text{for non-working days} \\
2935 + 1.72 X_1, & \text{for working days}
\end{cases}
$$

We can plot the two models on the scatterplot to see how they work better than 
the original model.

```{r r-plot-bike-lm2}
#| echo: false
#| fig-align: center
#| 

plot(x=bike2$casual, y=bike2$registered, 
     col=ifelse(bike2$workingday == "yes", "salmon", "deepskyblue4"),
     main="Comparing fitted models", cex=0.8,
     xlab="Casual", ylab="Registered")
abline(lm_reg_casual, col="deepskyblue4", lty=2)
est_coef <- coef(lm_reg_casual2)
abline(est_coef[1], est_coef[2], col="deepskyblue4")
abline(est_coef[1]+est_coef[3], est_coef[2], col="salmon")
legend("bottomright", legend=c("lm_reg_casual", "lm_reg_causal2"),
       lty=c(1,2), cex=0.7)
#legend("bottom", legend=c("no", "yes"), pch=1,
#       col=c("deepskyblue4", "salmon"), cex=0.7)
```

The dashed line corresponds to the earlier model, from @exm-bike-3. With the new
model, we have fitted separate intercepts to the two days, but the same slope. The 
benefit of fitting the model in this way, instead of breaking up the data into 
two portions and a different model on each one is that we use the entire dataset
to estimate the variability. If we wish to fit separate intercepts *and* slopes, 
we need to include an *interaction term*, which is what the next subsection is about.

:::

## Interaction term

A more complex model arises from an interaction between two terms. Here, we shall
consider an interaction between a continuous variable and a categorical 
explanatory variable. Suppose that we have three predictors: height ($X_1$),
weight ($X_2$) and gender ($X_3$). As spelt out in @sec-indic, we should use 
indicator variables to represent $X_3$ in the model. 

If we were to include an *interaction* between gender and weight, we would  be
allowing for a males and females to have separate coefficients for $X_2$. Here is
what the model would appear as:
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_2 X_3 + e
$$
Remember that $X_3$ will be 1 for males and 0 for females. The simplified 
equation for males would be:

$$
Y = (\beta_0 + \beta_3) + \beta_1 X_1 + (\beta_2 + \beta_4) X_2 + e
$$
For females, it would be:
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + e
$$
Both the intercept and coefficient of $X_2$ are different now. Recall that in 
@sec-indic, only the intercept term was different.

::: {style="background-color: #D5D1D1; padding: 20px" #exm-bike-4}

### Bike Data Working Day

Finally, we shall include an interaction in the model for bike rentals,
resulting in separate intercepts and separate slopes.

::: {.panel-tabset}

#### R code 

```{r r-bike-lm-3}
# R 
lm_reg_casual3 <- lm(registered ~ casual * workingday, data=bike2)
summary(lm_reg_casual3)
```


#### Python code

```{python py-bike-lm-3}
# Python
lm_reg_casual3 = ols('registered ~ casual * workingday', bike2).fit()
print(lm_reg_casual3.summary())
```

#### SAS output 

![](figs/sas_bike_reg_casual_workday_interaction_reg.png){fig-align="center" width=450}

:::

Notice that $R^2_{adj}$ has increased from 50.8% to 60.7%. The estimated models 
for each type of day are:

$$
Y = 
\begin{cases}
1362 + 1.16 X_1, & \text{for non-working days} \\
2168 + 2.97 X_1, & \text{for working days}
\end{cases}
$$


Here is visualisation of the lines that have been estimated for each sub-group 
of day. This is the image that we had earlier on @fig-reg-casual.

```{r r-plot-bike-lm3}
#| echo: false
#| fig-align: center
#| 

plot(x=bike2$casual, y=bike2$registered, 
     col=ifelse(bike2$workingday == "yes", "salmon", "deepskyblue4"),
     main="Interaction model", cex=0.8,
     xlab="Casual", ylab="Registered")
est_coef <- coef(lm_reg_casual3)
abline(est_coef[1], est_coef[2], col="deepskyblue4")
abline(est_coef[1]+est_coef[3], est_coef[2]+est_coef[4], col="salmon")
```

:::

{{< pagebreak >}}

## Residual Diagnostics

Recall from @eq-res-def that residuals are computed as 
$$
r_i = Y_i - \hat{Y_i}
$$
Residual analysis is a standard approach for identifying how we can improve a
model. In the case of linear regression, we can use the residuals to assess if
the distributional assumptions hold. We can also use residuals to identify
influential points that are masking the general trend of other points. Finally,
residuals can provided some direction on how to improve the model.

### Standardised Residuals

It can be shown that the variance of the residuals is in fact not constant! Let us 
define the hat-matrix as 
$$
\textbf{H} = \textbf{X}(\textbf{X}'\textbf{X} )^{-1} \textbf{X}'
$$
The diagonal values of $\textbf{H}$ will be denoted $h_{ii}$, for $i = 1, \ldots, n$. 
It can then be shown that 
$$
Var(r_i) = \sigma^2 (1 - h_{ii}), \quad Cov(r_i, r_j) = -\sigma^2 h_{ij}
$$
As such, we use the standardised residuals when checking if the assumption of Normality
has been met.

$$
r_{i,std}  = \frac{r_i}{\hat{\sigma}\sqrt{1 - h_{ii}}}
$$
If the model fits well, standardised residuals should look similar to a $N(0,1)$
distribution. In addition, large values of the standardised residual indicate
potential outlier points.

By the way, $h_{ii}$ is also referred to as the *leverage* of a point. It is a
measure of the potential influence of a point (on the parameters, and future
predictions). $h_{ii}$ is a value between 0 and 1. For a model with $p$
parameters, the average $h_{ii}$ should be should be $p/n$. We consider points
for whom $h_{ii} > 2 \times p/n$ to be high leverage points.

In the literature and in textbooks, you will see mention of residuals, standardised 
residuals and studentised residuals. While they differ in definitions slightly, they
typically yield the same information. Hence we shall stick to standardised residuals
for our course.

### Normality

::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-5}

### Concrete Data Normality Check

::: {.panel-tabset}

#### R code 

```{r r-concrete-qq-1}
#| layout-ncol: 2
# R 
r_s <- rstandard(lm_flow_water_slag)
hist(r_s)
qqnorm(r_s)
qqline(r_s)
```


#### Python code

```{python py-concrete-qq-1}
#| fig-align: center
# Python
r_s = pd.Series(lm_flow_water_slag.resid_pearson)
r_s.hist()
```

:::

While it does appear that we have slightly left-skewed data, the departure from
Normality seems to arise mostly from a thinner tail on the right.

```{r r-concrete-shap}
#| collapse: true
shapiro.test(r_s)
ks.test(r_s, "pnorm")
```

At 5% level, the two Normality tests do not agree on the result either. In any case,
please do keep in mind where Normality is needed most: in the hypothesis tests.
The estimated model is still valid - it is the best fitting line according to
the least-squares criteria.

:::

### Scatterplots

To understand the model fit better, a set of scatterplots are typically made. These
are plots of standardised residuals (on the $y$-axis) against

* fitted values 
* explanatory variables, one at a time.
* potential variables.

Residuals are meant to contain only the information that our model cannot explain.
Hence, if the model is good, the residuals should only contain random noise. There 
should be no apparent pattern to them. If we find such a pattern in one of the above
plots, we would have some clue as to how we could improve the model.

We typically inspect the plots for the following patterns:

![](figs/regression_residuals.png)

1. A pattern like the one on the extreme left is ideal. Residuals are 
   randomly distributed around zero; there is no pattern or trend in the 
   plot.
2. The second plot is something rarely seen. It would probably appear if we 
   were to plot residuals against a *new* variable that is not currently in the 
   model. If we observe this plot, we should then include this variable in the model.
3. This plot indicates we should include a quadratic term in the model.
4. The wedge shape (or funnel shape) indicates that we do not have 
   homoscedascity. The solution to this is either a transformation of the response,
   or weighted least squares. You will cover these in your linear models class.
   
   
::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-5}

### Concrete Data Residual Plots

::: {.panel-tabset}

#### R code 

```{r r-concrete-resids}
#| fig-align: center
#| fig-height: 4
#| fig-width: 10
opar <- par(mfrow=c(1,3))
plot(x=fitted(lm_flow_water_slag), r_s, main="Fitted")
plot(x=concrete$Water, r_s, main="X1")
plot(x=concrete$Slag, r_s, main="X2")
par(opar)
```

#### SAS Plots

![](figs/sas_concrete_flow_water_reg_res1.png){fig-align="center" width=450}


:::

While the plots of residuals versus explanatory variables look satisfactory, 
the plot of the residual versus fitted values appears to have funnel shape.
Coupled with the observations about the deviations from Normality of the 
residuals in @exm-concrete-4 (thin right-tail), we might want to try a square 
transform of the response.

:::

### Influential Points

The influence of a point on the inference can be judged by how much the inference
changes with and without the point. For instance to assess if point $i$ is 
influential on coefficient $j$:

1.  Estimate the model coefficients with all the data points.
2.  Leave out the observations $(Y_i , X_i)$ one at a time and re-estimate 
    the model coefficients.
3.  Compare the $\beta$'s from step 2 with the original estimate from step 1.

While the above method assesses influence on parameter estimates, Cook's distance
performs a similar iteration to assess the influence on the fitted values. Cook's
distance values greater than 1 indicate possibly influential points.

::: {style="background-color: #D5D1D1; padding: 20px" #exm-concrete-6}

### Concrete Data Influential Points

::: {.panel-tabset}

#### R code 
```{r r-concrete-infl}
infl <- influence.measures(lm_flow_water_slag)
summary(infl)
```

The set of 6 points above appear to be influencing the covariance matrix 
of the parameter estimates greatly. To proceed, we would typically leave these 
observations out one-at-a-time to study the impact on our eventual decision.

#### SAS Output 

![](figs/sas_concrete_flow_water_reg_res2.png){fig-align="center" width=450}

::: 

:::


## Further Reading

The topic of linear regression is vast. It is an extremely well-established
technique with numerous variations for a multitude of scenarios. Even a single
course on it (ST3131) will not have sufficient time to cover all of it's 
capabilities. Among topics that will be useful in your career are :

* Generalised additive models, which allow the use of piecewise polynomials 
  for flexible modeling of non-linear functions.
* Generalised linear models, for modeling non-numeric response.
* Generalised least squares, to handle correlated observations, and so on.
* Principal component regression, to handle the issue of multicollinearity
  (correlated predictors).
  
The textbooks @draper1998applied and @james2013introduction are excellent reference
books for this topic.

## References

### Website References {#sec-web-ref}

3. [Stats models documentation](https://www.statsmodels.org/stable/regression.html)
6. [Diagnostics](https://www.zeileis.org/teaching/AER/Ch-Validation-handout.pdf)
6. [On residual plots](https://online.stat.psu.edu/stat462/node/120/)
