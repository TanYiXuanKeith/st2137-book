{
  "hash": "820a036cdbb56161001a5970209c9758",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Introduction\n\nRegression analysis is a technique for investigating and modeling\nthe relationship between variables like X and Y. Here are some examples:\n\n1. Within a country, we may wish to use per capita income (X) to estimate the \n   life expectancy (Y) of residents.\n2. We may wish to use the size of a crab claw (X) to estimate the closing force \n   that it can exert (Y).\n3. We may wish to use the height of a person (X) to estimate their weight (Y).\n\nIn all the above cases, we refer to $X$ as the explanatory or independent\nvariable. It is also sometimes referred to as a predictor. $Y$ is referred to as\nthe response or dependent variable. In this topic, we shall first introduce the\ncase of simple linear regression, where we model the $Y$ on a single $X$. In\nlater sections, we shall model the $Y$ on multiple $X$'s. This latter technique\nis referred to as multiple linear regression.\n\nRegression models are used for two primary purposes:\n\n1. To understand how certain explanatory variables affect the response variable.\n   This aim is typically known as *estimation*, since the primary focus is on \n   estimating the unknown parameters of the model.\n2. To predict the response variable for new values of the explanatory variables.\n   This is referred to as *prediction*.\n   \nIn our course, we shall focus on the estimation aim, since prediction models require a\nparadigm of their own, and are best learnt alongside a larger suite of models\ne.g. decision trees, support vector machines, etc.\n\nIn subsequent sections, we shall revisit a couple of datasets from earlier topics\nto run linear regression models  on them. \n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-1}\n\n### Concrete Data: Flow on Water\n\nRecall the concrete dataset that we first encountered in the topic on summarising \ndata. We shall go on to fit a linear regression to understand the relationship\nbetween the output of the flow test, and the amount of water used to create the \nconcrete.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Scatterplot with simple linear regression model](09-regression_files/figure-html/fig-flow-water-1.png){#fig-flow-water fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\nIt does appear that there is a trend in the scatter plot. We shall figure out \nhow to estimate this line in this topic.\n\n:::\n\n<br>\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-bike-1}\n\n### Bike Rental Data\n\nIn the introduction to SAS, we encountered data on bike rentals in the USA over a\nperiod of 2 years. Here, we shall attempt to model the number of registered users \non the number of casual users.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Scatterplot of registered vs. casual bike renters](09-regression_files/figure-html/fig-reg-casual-1.png){#fig-reg-casual fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\nContingent on whether the day is a working one or not, it does appear that the \ntrendline is different.\n\n:::\n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n## Simple Linear Regression\n\n### Formal Set-up {#sec-formal-slr}\n\nThe simple linear regression model is applicable when we have observations\n$(X_i, Y_i)$ for $n$ individuals. For now, let's assume both the $X$ and $Y$ \nvariables are quantitative.\n\nThe simple linear regression model is given by \n\n$$\nY_i = \\beta_0 + \\beta_1 X_i + e_i\n$${#eq-slr-model}\nwhere \n\n* $\\beta_0$ is intercept term, \n* $\\beta_1$ is the slope, and \n* $e_i$ is an error term, specific to each individual in the dataset.\n\n$\\beta_0$ and $\\beta_1$ are unknown constants that need to be estimated from the\ndata. There is an implicit assumption in the formulation of the model that there\nis a linear relationship between $Y_i$ and $X_i$. In terms of distributions, we\nassume that the $e_i$ are i.i.d Normal.\n\n$$\ne_i \\sim N(0, \\sigma^2), \\; i =1\\ldots, n\n$$ {#eq-error-term}\n\nThe constant variance assumption is also referred to as homoscedascity (homo-skee-das-city). \nThe validity of the above assumptions will have to be checked after the model is fitted.\nAll in all, the assumptions imply that:\n\n1. $E(Y_i | X_i) = \\beta_0 + \\beta_1 X_i$, for $i=1, \\ldots, n$.\n2. $Var(Y_i | X_i) = Var(e_i) = \\sigma^2$, for $i=1, \\ldots, n$.\n3. The $Y_i$ are independent.\n4. The $Y_i$'s are Normally distributed.\n\n### Estimation {#sec-slr-estimation}\n\nBefore deploying or using the model, we need to estimate optimal values to use \nfor the unknown $\\beta_0$ and $\\beta_1$. We shall introduce the method of \nOrdinary Least Squares (OLS) for the estimation. Let us define the \n*error Sum of Squares* to be \n\n$$\nSS_E = S(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n$$ {#eq-sse}\n\nThen the OLS estimates of $\\beta_0$ and $\\beta_1$ are given by \n$$\n\\mathop{\\arg \\min}_{\\beta_0, \\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\n$$\nThe minimisation above can be carried out analytically, by taking partial \nderivative with respect to the two parameters and setting them to 0.\n\n\\begin{eqnarray*}\n\\frac{\\partial S}{\\partial \\beta_0}  &=& -2 \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i) = 0 \\\\\n\\frac{\\partial S}{\\partial \\beta_1}  &=& -2 \\sum_{i=1}^n X_i (Y_i - \\beta_0 - \\beta_1 X_i) = 0 \n\\end{eqnarray*}\n\nSolving and simplifying, we arrive at the following:\n\\begin{eqnarray*}\n\\hat{\\beta_1} &=& \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\\\\n\\hat{\\beta_0} &=& \\bar{Y} - \\hat{\\beta_0} \\bar{X}\n\\end{eqnarray*} \nwhere $\\bar{Y} = (1/n)\\sum Y_i$ and $\\bar{X} = (1/n)\\sum X_i$.\n\nIf we define the following sums:\n\\begin{eqnarray*}\nS_{XY} &=& \\sum_{i=1}^n X_i Y_i - \\frac{(\\sum_{i=1}^n X_i )(\\sum_{i=1}^n Y_i )}{n} \\\\\nS_{XX} &=& \\sum_{i=1}^n X_i^2 - \\frac{(\\sum_{i=1}^n X_i )^2}{n}\n\\end{eqnarray*}\nthen a form convenient for computation of $\\hat{\\beta_1}$ is \n$$\n\\hat{\\beta_1} = \\frac{S_{XY}}{S_{XX}}\n$$\n\nOnce we have the estimates, we can use @eq-slr-model to compute *fitted* values\nfor each observation, corresponding to our best guess of the mean of the\ndistributions from which the observations arose:\n$$\n\\hat{Y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} X_i, \\quad i = 1, \\ldots, n\n$$\nAs always, we can form residuals as the deviations from fitted values. \n$$\nr_i = Y_i - \\hat{Y}_i\n$$ {#eq-res-def}\nResiduals are our best guess at the unobserved error terms $e_i$. Squaring the\nresiduals and summing over all observations, we can arrive at the following\ndecomposition, which is very similar to the one in the ANOVA model:\n\n$$\n\\underbrace{\\sum_{i=1}^n (Y_i  - \\bar{Y})^2}_{SS_T} =  \n\\underbrace{\\sum_{i=1}^n (Y_i  - \\hat{Y_i})^2}_{SS_{Res}} +\n\\underbrace{\\sum_{i=1}^n (\\hat{Y_i}  - \\bar{Y})^2}_{SS_{Reg}}\n$$\n\nwhere \n\n* $SS_T$ is known as the total sum of squares.\n* $SS_{Res}$ is known as the residual sum of squares.\n* $SS_{Reg}$ is known as the regression sum of squares.\n\nIn our model, recall from @eq-error-term that we had assumed equal variance for \nall our observations. We can estimate $\\sigma^2$ with\n$$\n\\hat{\\sigma^2} = \\frac{SS_{Res}}{n-2}\n$$\nOur distributional assumptions lead to the following for our estimates\n$\\hat{\\beta_0}$ and $\\hat{\\beta_1}$:\n\n\\begin{eqnarray}\n\\hat{\\beta_0} &\\sim& N(\\beta_0,\\; \\sigma^2(1/n + \\bar{X}^2/S_{XX})) \\\\\n\\hat{\\beta_1} &\\sim& N(\\beta_1,\\; \\sigma^2/S_{XX})\n\\end{eqnarray}\n\nThe above are used to construct confidence intervals for $\\beta_0$ and $\\beta_1$,\nbased on $t$-distributions.\n\n## Hypothesis Test for Model Significance {#sec-slr-F}\n\nThe first test that we introduce here is to test if the coefficient $\\beta_1$ is\nsignificantly different from 0. It is essentially a test of whether it was \nworthwhile to use a regression model of the form in @eq-slr-model, instead of a \nsimple mean to represent the data.\n\nThe null and alternative hypotheses are:\n\n\\begin{eqnarray*}\nH_0 &:& \\beta_1 = 0\\\\\nH_1 &:& \\beta_1 \\ne 0\n\\end{eqnarray*}\n\nThe test statistic is \n\n$$\nF_0 = \\frac{SS_{Reg}/1}{SS_{Res}/(n-2)}\n$$ {#eq-f-stat}\n\nUnder the null hypothesis, $F_0 \\sim F_{1,n-2}$. \n\nIt is also possible to perform this same test as a $t$-test, using the result earlier.\nThe statement of the hypotheses is equivalent to the $F$-test. The test statistic\n$$\nT_0 = \\frac{\\hat{\\beta_1}}{\\sqrt{\\hat{\\sigma^2}/S_{XX}}}\n$$ {#eq-t-stat}\nUnder $H_0$, the distribution of $T_0$ is $t_{n-2}$. This $t$-test and the earlier \n$F$-test in this section are *identical*. It can be proved that $F_0 = T_0^2$; the \nobtained $p$-values will be identical.\n\n### Coefficient of Determination, $R^2$\n\nThe coefficient of determination $R^2$ is defined as\n\n$$\nR^2 = 1 - \\frac{SS_{Res}}{SS_T} = \\frac{SS_{Reg}}{SS_T}\n$$\nIt can be interpreted as the proportion of variation in $Yi$, explained by \nthe inclusion of $X_i$. Since $0 \\le SS_{Res} \\le SS_T$, we can easily prove that \n$0 \\le R^2 \\le 1$.  The larger the value of $R^2$ is, the better the model is.\n\nWhen we get to the case of multiple linear regression, take note that simply \nincluding more variables in the model will increase $R^2$. This is undesirable; it \nis preferable to have a parsimonious model that explains the response variable well.\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-2}\n\n### Concrete Data Model\n\\index{Concrete slump!flow vs. water}\n\nIn this example, we focus on the estimation of the model parameters for the two \nvariables we introduced in @exm-concrete-1\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#R \nconcrete <- read.csv(\"data/concrete+slump+test/slump_test.data\")\nnames(concrete)[c(1,11)] <- c(\"id\", \"Comp.Strength\")\nlm_flow_water <- lm(FLOW.cm. ~ Water, data=concrete)\nsummary(lm_flow_water)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = FLOW.cm. ~ Water, data = concrete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.211 -10.836   2.734  11.031  22.163 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -58.72755   13.28635  -4.420 2.49e-05 ***\nWater         0.54947    0.06704   8.196 8.10e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.68 on 101 degrees of freedom\nMultiple R-squared:  0.3995,\tAdjusted R-squared:  0.3935 \nF-statistic: 67.18 on 1 and 101 DF,  p-value: 8.097e-13\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Python\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nconcrete = pd.read_csv(\"data/concrete+slump+test/slump_test.data\")\nconcrete.rename(columns={'No':'id', \n                         'Compressive Strength (28-day)(Mpa)':'Comp_Strength',\n                         'FLOW(cm)': 'Flow'},\n                inplace=True)\nlm_flow_water = ols('Flow ~ Water', data=concrete).fit()\nprint(lm_flow_water.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Flow   R-squared:                       0.399\nModel:                            OLS   Adj. R-squared:                  0.394\nMethod:                 Least Squares   F-statistic:                     67.18\nDate:                Fri, 18 Oct 2024   Prob (F-statistic):           8.10e-13\nTime:                        16:23:21   Log-Likelihood:                -414.60\nNo. Observations:                 103   AIC:                             833.2\nDf Residuals:                     101   BIC:                             838.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -58.7276     13.286     -4.420      0.000     -85.084     -32.371\nWater          0.5495      0.067      8.196      0.000       0.416       0.682\n==============================================================================\nOmnibus:                        6.229   Durbin-Watson:                   1.843\nProb(Omnibus):                  0.044   Jarque-Bera (JB):                5.873\nSkew:                          -0.523   Prob(JB):                       0.0530\nKurtosis:                       2.477   Cond. No.                     1.95e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.95e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### SAS output\n\n![](figs/sas_concrete_flow_water_reg.png){fig-align=\"center\" width=450}\n\n:::\n\nFrom the output, we can note that the *estimated* model for Flow ($Y$) against\nWater ($X$) is:\n$$\nY = -58.73 + 0.55 X\n$$\nThe estimates are $\\hat{\\beta_0} = -58.73$ and $\\hat{\\beta_1} = 0.55$. This is the \nprecise equation that was plotted in @fig-flow-water. The $R^2$ is labelled as \n\"Multiple R-squared\" in the R output. The value is 0.3995, which means that about \n40% of the variation in $Y$ is explained by $X$. \n\nA simple interpretation[^1] of the model is as follows:\n\n> For every 1 unit increase in Water, there is an average associated increase in\n> Flow rate of 0.55 units.\n\n[^1]: This interpretation has to be taken very cautiously, especially when there are\n      other explanatory variables in the model.\n      \nTo obtain confidence intervals for the parameters, we can use the following code in\nR. The Python summary already contains the confidence intervals.\n      \n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#R \nconfint(lm_flow_water)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %      97.5 %\n(Intercept) -85.0841046 -32.3709993\nWater         0.4164861   0.6824575\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\nWe can read off that the 95% Confidence intervals are:\n\n* For $\\beta_0$: (-85.08, -32.37)\n* For $\\beta_1$: (0.42, 0.68)\n\n:::\n\n<br>\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-bike-2}\n\n### Bike Data F-test\n\\index{Bike rentals!regression F-test}\n\nIn this example, we shall fit a simple linear regression model to the bike \ndata, *constrained to the non-working days*. In other words, we shall focus on \nfitting just the blue line, from the blue points, in @fig-reg-casual.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#R \nbike2 <- read.csv(\"data/bike2.csv\")\nbike2_sub <- bike2[bike2$workingday == \"no\", ]\nlm_reg_casual <- lm(registered ~ casual, data=bike2_sub)\nanova(lm_reg_casual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: registered\n           Df    Sum Sq   Mean Sq F value    Pr(>F)    \ncasual      1 237654556 237654556  369.25 < 2.2e-16 ***\nResiduals 229 147386970    643611                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Python\nbike2 = pd.read_csv(\"data/bike2.csv\")\nbike2_sub = bike2[bike2.workingday == \"no\"]\n\nlm_reg_casual = ols('registered ~ casual', bike2_sub).fit()\nanova_tab = sm.stats.anova_lm(lm_reg_casual,)\nanova_tab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             df        sum_sq       mean_sq           F        PR(>F)\ncasual      1.0  2.376546e+08  2.376546e+08  369.251728  1.183368e-49\nResidual  229.0  1.473870e+08  6.436112e+05         NaN           NaN\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### SAS output\n\n![](figs/sas_bike_reg_casual_reg.png){fig-align=\"center\" width=450}\n\n:::\n\nThe output above includes the sum-of-squares that we need to perform the\n$F$-test outlined in @sec-slr-F. From the output table, we can see that\n$SS_{Reg} = 237654556$ and $SS_{Res} = 147386970$. The value of $F_0$ for this\ndataset is 369.25. The $p$-value is extremely small ($2 \\times 10^{-16}$),\nindicating strong evidence against $H_0$, i.e. that $\\beta_1 = 0$.\n\n\n:::\n\n<br>\n\nActually, if you observe carefully in @exm-concrete-2, the output from R\ncontains both the $t$-test for significance of $\\beta_1$, and the $F$-test\nstatistic based on sum-of-squares. The $p$-value in both cases is \n$8.10 \\times 10^{1-3}$. \n\nIn linear regression, we almost always wish to use the model to understand what \nthe mean of future observations would be. In the concrete case, we may wish to \nuse the model to understand how the Flow test output values change as the amount \nof Water in the mixture changes. This is because, based on our formulation,\n\n$$\nE(Y | X) = \\beta_0 + \\beta_1 X\n$$\n\nAfter estimating the parameters, we would have:\n$$\n\\widehat{E(Y | X)} = \\hat{\\beta_0} + \\hat{\\beta_1} X\n$$\n\nThus we can vary the values of $X$ to study how the mean of $Y$ changes. Here \nis how we can do so in the concrete model for data.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-3}\n\n### Concrete Data Predicted Means\n\\index{Concrete slump!predicted means}\n\nIn order to create the predicted means, we shall have to create a dataframe with\nthe new values for which we require the predictions. We are first going to set \nup a new matrix of $X$-values corresponding to the desired range.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#R \nnew_df <- data.frame(Water = seq(160, 240, by = 5))\nconf_intervals <- predict(lm_flow_water, new_df, interval=\"conf\")\n\nplot(concrete$Water, concrete$FLOW.cm., ylim=c(0, 100),\n     xlab=\"Water\", ylab=\"Flow\", main=\"Confidence and Prediction Intervals\")\nabline(lm_flow_water, col=\"red\")\nlines(new_df$Water, conf_intervals[,\"lwr\"], col=\"red\", lty=2)\nlines(new_df$Water, conf_intervals[,\"upr\"], col=\"red\", lty=2)\nlegend(\"bottomright\", legend=c(\"Fitted line\", \"Lower/Upper CI\"), \n       lty=c(1,2), col=\"red\")\n```\n\n::: {.cell-output-display}\n![](09-regression_files/figure-html/r-concrete-pred-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n#### Python code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Python\nnew_df = sm.add_constant(pd.DataFrame({'Water' : np.linspace(160,240, 10)}))\n\npredictions_out = lm_flow_water.get_prediction(new_df)\n\nax = concrete.plot(x='Water', y='Flow', kind='scatter', alpha=0.5 )\nax.set_title('Flow vs. Water');\nax.plot(new_df.Water, predictions_out.conf_int()[:, 0].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.Water, predictions_out.conf_int()[:, 1].reshape(-1), \n        color='blue', linestyle='dashed');\nax.plot(new_df.Water, predictions_out.predicted, color='blue');\n```\n\n::: {.cell-output-display}\n![](09-regression_files/figure-html/py-concrete-pred-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n#### SAS Output\n\n![](figs/sas_concrete_flow_water_reg_fitplot.png){fig-align=\"center\" width=450}\n\n\n:::\n\nThe fitted line is the straight line formed using $\\hat{\\beta_0}$ and\n$\\hat{\\beta_1}$.  The dashed lines are 95% Confidence Intervals for $E(Y|X)$,\nfor varying values of $X$. They are formed by joining up the lower bounds and \nthe upper bounds separately. Notice how the limits get wider the further away \nwe are from $\\bar{X} \\approx 200$.\n\n:::\n\n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n## Multiple Linear Regression\n\n### Formal Setup\n\nWhen we have more than 1 explanatory variable, we turn to multiple linear \nregression - generalised version of what we have been dealing with so far. We would\nstill have observed information from $n$ individuals, but for each one, we now \nobserve a vector of values:\n$$\nY_i, \\, X_{1,i},  \\, X_{2,i}, \\ldots, \\, X_{p-1,i},  X_{p,i}\n$$\nIn other words, we observe $p$ independent variables and 1 response variable for \neach individual in our dataset. The analogous equation to @eq-slr-model is\n$$\nY_i = \\beta_0 + \\beta_1 X_{1,i} + \\cdots + \\beta_p  X_{p,i} + e\n$$ {#eq-mlr-model}\n\nIt is easier to write things with matrices for multiple linear regression:\n\n$$\n\\textbf{Y} = \\begin{bmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{bmatrix}, \\;\n\\textbf{X} = \\begin{bmatrix}\n1 & X_{1,1} & X_{2,1} & \\cdots &X_{p,1}\\\\\n1 & X_{1,2} & X_{2,2} & \\cdots &X_{p,2}\\\\\n\\vdots & \\vdots & \\vdots & {} & \\vdots \\\\\n1 & X_{1,n} & X_{2,n} & \\cdots &X_{p,n}\\\\\n\\end{bmatrix}, \\;\n\\boldsymbol{ \\beta } = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}, \\;\n\\boldsymbol{e} = \\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n \n\\end{bmatrix}\n$$\n\nWith the above matrices, we can re-write @eq-mlr-model as \n$$\n\\textbf{Y} = \\textbf{X} \\boldsymbol{\\beta} + \\textbf{e}\n$$\nWe retain the same distributional assumptions as in @sec-formal-slr.\n\n### Estimation \n\nSimilar to @sec-slr-estimation, we can define $SS_E$ to be \n$$\nSS_E = S(\\beta_0, \\beta_1,\\ldots,\\beta_p) = \\sum_{i=1}^n (Y_i - \\beta_0 - \n\\beta_1 X_{1,i} - \\cdots - \\beta_p X_{p,i} )^2\n$$ {#eq-mlr-sse}\n\nMinimising the above cost function leads to the OLS estimates:\n$$\n\\hat{\\boldsymbol{\\beta}} =  (\\textbf{X}'\\textbf{X})^{-1} \\textbf{X}'\\textbf{Y} \n$$\nThe fitted values can be computed with\n$$\n\\hat{\\textbf{Y}} = \\textbf{X} \\hat{\\boldsymbol{\\beta}} =\n\\textbf{X} (\\textbf{X}'\\textbf{X})^{-1} \\textbf{X}'\\textbf{Y} \n$$\nResiduals are obtained as \n$$\n\\textbf{r} = \\textbf{Y} - \\hat{\\textbf{Y}}\n$$\nFinally, we estimate $\\sigma^2$ using \n$$\n\\hat{\\sigma^2} = \\frac{SS_{Res}}{n-p} = \\frac{\\textbf{r}' \\textbf{r}}{n-p}\n$$\n\n### Coefficient of Determination, $R^2$\n\nIn the case of multiple linear regression, $R^2$ is calculated exactly as in\nsimple linear regression, and its interpretation remains the same:\n$$\nR^2 = 1 - \\frac{SS_{Res}}{SS_T}\n$$\n\nHowever, note that $R^2$ can be inflated simply by adding more terms to the\nmodel (even insignificant terms). Thus, we use the adjusted $R^2$, which penalizes \nus for adding more and more terms to the model:\n$$\nR^2_{adj} = 1 - \\frac{SS_{Res}/(n-p)}{SS_T/(n-1)}\n$$\n\n### Hypothesis Tests\n\nThe $F$-test in the multiple linear regression helps determine if our regression\nmodel provides any advantage over the simple mean model. The null and\nalternative hypotheses are:\n\n\\begin{eqnarray*}\nH_0 &:& \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\\\\nH_1 &:& \\beta_j \\ne 0 \\text{ for at least one } j \\in \\{1, 2, \\ldots, p\\}\n\\end{eqnarray*}\n\nThe test statistic is \n\n$$\nF_1 = \\frac{SS_{Reg}/p}{SS_{Res}/(n-p-1)}\n$$ {#eq-f-stat}\n\nUnder the null hypothesis, $F_0 \\sim F_{p,n-p-1}$. \n\nIt is also possible to test for the significance of individual $\\beta$ terms, using \na $t$-test. The output is typically given for all the coefficients in a table.\nThe statement of the hypotheses pertaining to these tests is:\n\\begin{eqnarray*}\nH_0 &:& \\beta_j = 0\\\\\nH_1 &:& \\beta_j \\ne 0 \n\\end{eqnarray*}\n\nHowever, note that these $t$-tests are partial because it should be interpreted as\na test of the contribution of $\\beta_j$, *given that all other terms are already in the model*.\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-4}\n\n### Concrete Data Multiple Linear Regression\n\\index{Concrete slump!flow vs. water, slag}\n\nIn this second model for concrete, we add a second predictor variable, Slag. The \nupdated model is \n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n$$\nwhere $X_1$ corresponds to Water, and $X_2$ corresponds to Slag.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R \nlm_flow_water_slag <- lm(FLOW.cm. ~ Water + Slag, data=concrete)\nsummary(lm_flow_water_slag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = FLOW.cm. ~ Water + Slag, data = concrete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.687 -10.746   2.010   9.224  23.927 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -50.26656   12.38669  -4.058 9.83e-05 ***\nWater         0.54224    0.06175   8.781 4.62e-14 ***\nSlag         -0.09023    0.02064  -4.372 3.02e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.6 on 100 degrees of freedom\nMultiple R-squared:  0.4958,\tAdjusted R-squared:  0.4857 \nF-statistic: 49.17 on 2 and 100 DF,  p-value: 1.347e-15\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python\nlm_flow_water_slag = ols('Flow ~ Water + Slag', data=concrete).fit()\nprint(lm_flow_water_slag.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Flow   R-squared:                       0.496\nModel:                            OLS   Adj. R-squared:                  0.486\nMethod:                 Least Squares   F-statistic:                     49.17\nDate:                Fri, 18 Oct 2024   Prob (F-statistic):           1.35e-15\nTime:                        16:23:22   Log-Likelihood:                -405.59\nNo. Observations:                 103   AIC:                             817.2\nDf Residuals:                     100   BIC:                             825.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -50.2666     12.387     -4.058      0.000     -74.841     -25.692\nWater          0.5422      0.062      8.781      0.000       0.420       0.665\nSlag          -0.0902      0.021     -4.372      0.000      -0.131      -0.049\n==============================================================================\nOmnibus:                        5.426   Durbin-Watson:                   2.029\nProb(Omnibus):                  0.066   Jarque-Bera (JB):                4.164\nSkew:                          -0.371   Prob(JB):                        0.125\nKurtosis:                       2.353   Cond. No.                     2.14e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.14e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### SAS output \n\n![](figs/sas_concrete_flow_water_slag_reg.png){fig-align=\"center\" width=450}\n\n:::\n\nThe $F$-test is now concerned with the hypotheses:\n\\begin{eqnarray*}\nH_0 &:& \\beta_1 = \\beta_2 = 0\\\\\nH_1 &:& \\beta_1 \\ne 0 \\text{ or } \\beta_2 \\ne 0\n\\end{eqnarray*}\n\nFrom the output above, we can see that $F_1 = 49.17$, with a corresponding \n$p$-value of $1.3 \\times 10^{-15}$. The individual $t$-tests for the coefficients all\nindicate significant differences from 0. The final estimated model can be written \nas\n$$\nY = -50.27 + 0.54 X_1 - 0.09 X_2\n$$\nNotice that the coefficients have changed slightly from the model in\n@exm-concrete-2. Notice also that we have an improved $R^2$ of 0.50. However, as\nwe pointed out earlier, we should be using the adjusted $R^2$, which adjusts for\nthe additional variable included. This value is 0.49.\n\nWhile we seem to have found a better model than before, we still have to assess\nif all the assumptions listed in @sec-formal-slr have been met. We shall do so\nin subsequent sections.\n\n:::\n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n## Indicator Variables\n\n### Including a Categorical Variable {#sec-indic}\n\nThe explanatory variables in a linear regression model do not need to be continuous.\nCategorical variables can also be included in the model. In order to include them,\nthey have to be coded using dummy variables. \n\nFor instance, suppose that we wish to include gender in a model as $X_3$. There \nare only two possible genders in our dataset: Female and Male. We can represent\n$X_3$ as an indicator variable, with\n\n$$\nX_{3,i} = \n\\begin{cases}\n1 & \\text{individual $i$ is male}\\\\\n0 & \\text{individual $i$ is female}\n\\end{cases}\n$$\n\nThe model (without subscripts for the $n$ individuals) is then:\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + e\n$$\nFor females, the value of $X_3$ is 0. Hence the model reduces to\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n$$\nOn the other hand, for males, the model reduces to \n$$\nY = (\\beta_0 + \\beta_3) + \\beta_1 X_1 + \\beta_2 X_2 + e\n$$\nThe difference between the two models is in the intercept. The other \ncoefficients remain the same. \n\nIn general, if the categorical variable has $a$ levels, we will need $a-1$\ncolumns of indicator variables to represent it. This is in contrast to machine\nlearning models which use one-hot encoding. The latter encoding results in\ncolumns that are linearly dependent if we include an intercept term in the\nmodel.\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-bike-3}\n\n### Bike Data Working Day\n\\index{Bike rentals!registered vs. casual, work day}\n\nIn this example, we shall improve on the simple linear regression model \nfrom @exm-bike-2. Instead of a single model for just non-working days, we shall\nfit separate models for working and non-working days by including that variable\nas a categorical one.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R \nlm_reg_casual2 <- lm(registered ~ casual + workingday, data=bike2)\nsummary(lm_reg_casual2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = registered ~ casual + workingday, data = bike2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3381.8  -674.8   -22.5   792.4  2683.6 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   6.052e+02  1.188e+02   5.095 4.45e-07 ***\ncasual        1.717e+00  6.893e-02  24.905  < 2e-16 ***\nworkingdayyes 2.332e+03  1.017e+02  22.921  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1094 on 728 degrees of freedom\nMultiple R-squared:  0.5099,\tAdjusted R-squared:  0.5086 \nF-statistic: 378.7 on 2 and 728 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python\nlm_reg_casual2 = ols('registered ~ casual + workingday', bike2).fit()\nprint(lm_reg_casual2.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             registered   R-squared:                       0.510\nModel:                            OLS   Adj. R-squared:                  0.509\nMethod:                 Least Squares   F-statistic:                     378.7\nDate:                Fri, 18 Oct 2024   Prob (F-statistic):          1.81e-113\nTime:                        16:23:22   Log-Likelihood:                -6150.8\nNo. Observations:                 731   AIC:                         1.231e+04\nDf Residuals:                     728   BIC:                         1.232e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept           605.2254    118.790      5.095      0.000     372.013     838.438\nworkingday[T.yes]  2331.7334    101.730     22.921      0.000    2132.015    2531.452\ncasual                1.7167      0.069     24.905      0.000       1.581       1.852\n==============================================================================\nOmnibus:                        2.787   Durbin-Watson:                   0.595\nProb(Omnibus):                  0.248   Jarque-Bera (JB):                2.479\nSkew:                          -0.059   Prob(JB):                        0.290\nKurtosis:                       2.740   Cond. No.                     4.05e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.05e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### SAS output \n\n![](figs/sas_bike_reg_casual_workday_reg.png){fig-align=\"center\" width=450}\n\n:::\n\nThe estimated model is now \n$$\nY = 605 + 1.72 X_1 + 2330 X_2\n$$\n\nBut $X_2 =1$ for working days and $X_2=0$ for non-working days. This results in \ntwo *separate* models for the two types of days:\n\n$$\nY = \n\\begin{cases}\n605 + 1.72 X_1, & \\text{for non-working days} \\\\\n2935 + 1.72 X_1, & \\text{for working days}\n\\end{cases}\n$$\n\nWe can plot the two models on the scatterplot to see how they work better than \nthe original model.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-regression_files/figure-html/r-plot-bike-lm2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\nThe dashed line corresponds to the earlier model, from @exm-bike-3. With the new\nmodel, we have fitted separate intercepts to the two days, but the same slope. The \nbenefit of fitting the model in this way, instead of breaking up the data into \ntwo portions and a different model on each one is that we use the entire dataset\nto estimate the variability. If we wish to fit separate intercepts *and* slopes, \nwe need to include an *interaction term*, which is what the next subsection is about.\n\n:::\n\n## Interaction term\n\nA more complex model arises from an interaction between two terms. Here, we shall\nconsider an interaction between a continuous variable and a categorical \nexplanatory variable. Suppose that we have three predictors: height ($X_1$),\nweight ($X_2$) and gender ($X_3$). As spelt out in @sec-indic, we should use \nindicator variables to represent $X_3$ in the model. \n\nIf we were to include an *interaction* between gender and weight, we would  be\nallowing for a males and females to have separate coefficients for $X_2$. Here is\nwhat the model would appear as:\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_2 X_3 + e\n$$\nRemember that $X_3$ will be 1 for males and 0 for females. The simplified \nequation for males would be:\n\n$$\nY = (\\beta_0 + \\beta_3) + \\beta_1 X_1 + (\\beta_2 + \\beta_4) X_2 + e\n$$\nFor females, it would be:\n$$\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + e\n$$\nBoth the intercept and coefficient of $X_2$ are different now. Recall that in \n@sec-indic, only the intercept term was different.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-bike-4}\n\n### Bike Data Working Day\n\\index{Bike rentals!interaction term}\n\nFinally, we shall include an interaction in the model for bike rentals,\nresulting in separate intercepts and separate slopes.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R \nlm_reg_casual3 <- lm(registered ~ casual * workingday, data=bike2)\nsummary(lm_reg_casual3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = registered ~ casual * workingday, data = bike2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4643.5  -733.0   -57.3   675.9  2532.1 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          1.363e+03  1.199e+02  11.361  < 2e-16 ***\ncasual               1.164e+00  7.383e-02  15.769  < 2e-16 ***\nworkingdayyes        8.063e+02  1.446e+02   5.578 3.44e-08 ***\ncasual:workingdayyes 1.819e+00  1.340e-01  13.575  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 977.6 on 727 degrees of freedom\nMultiple R-squared:  0.609,\tAdjusted R-squared:  0.6074 \nF-statistic: 377.5 on 3 and 727 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python\nlm_reg_casual3 = ols('registered ~ casual * workingday', bike2).fit()\nprint(lm_reg_casual3.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             registered   R-squared:                       0.609\nModel:                            OLS   Adj. R-squared:                  0.607\nMethod:                 Least Squares   F-statistic:                     377.5\nDate:                Fri, 18 Oct 2024   Prob (F-statistic):          9.38e-148\nTime:                        16:23:23   Log-Likelihood:                -6068.3\nNo. Observations:                 731   AIC:                         1.214e+04\nDf Residuals:                     727   BIC:                         1.216e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n============================================================================================\n                               coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nIntercept                 1362.6312    119.942     11.361      0.000    1127.158    1598.105\nworkingday[T.yes]          806.2675    144.551      5.578      0.000     522.480    1090.055\ncasual                       1.1643      0.074     15.769      0.000       1.019       1.309\ncasual:workingday[T.yes]     1.8186      0.134     13.575      0.000       1.556       2.082\n==============================================================================\nOmnibus:                        7.936   Durbin-Watson:                   0.528\nProb(Omnibus):                  0.019   Jarque-Bera (JB):               11.531\nSkew:                          -0.025   Prob(JB):                      0.00313\nKurtosis:                       3.613   Cond. No.                     5.72e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.72e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### SAS output \n\n![](figs/sas_bike_reg_casual_workday_interaction_reg.png){fig-align=\"center\" width=450}\n\n:::\n\nNotice that $R^2_{adj}$ has increased from 50.8% to 60.7%. The estimated models \nfor each type of day are:\n\n$$\nY = \n\\begin{cases}\n1362 + 1.16 X_1, & \\text{for non-working days} \\\\\n2168 + 2.97 X_1, & \\text{for working days}\n\\end{cases}\n$$\n\n\nHere is visualisation of the lines that have been estimated for each sub-group \nof day. This is the image that we had earlier on @fig-reg-casual.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-regression_files/figure-html/r-plot-bike-lm3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n## Residual Diagnostics\n\nRecall from @eq-res-def that residuals are computed as \n$$\nr_i = Y_i - \\hat{Y_i}\n$$\nResidual analysis is a standard approach for identifying how we can improve a\nmodel. In the case of linear regression, we can use the residuals to assess if\nthe distributional assumptions hold. We can also use residuals to identify\ninfluential points that are masking the general trend of other points. Finally,\nresiduals can provided some direction on how to improve the model.\n\n### Standardised Residuals\n\nIt can be shown that the variance of the residuals is in fact not constant! Let us \ndefine the hat-matrix as \n$$\n\\textbf{H} = \\textbf{X}(\\textbf{X}'\\textbf{X} )^{-1} \\textbf{X}'\n$$\nThe diagonal values of $\\textbf{H}$ will be denoted $h_{ii}$, for $i = 1, \\ldots, n$. \nIt can then be shown that \n$$\nVar(r_i) = \\sigma^2 (1 - h_{ii}), \\quad Cov(r_i, r_j) = -\\sigma^2 h_{ij}\n$$\nAs such, we use the standardised residuals when checking if the assumption of Normality\nhas been met.\n\n$$\nr_{i,std}  = \\frac{r_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}\n$$\nIf the model fits well, standardised residuals should look similar to a $N(0,1)$\ndistribution. In addition, large values of the standardised residual indicate\npotential outlier points.\n\nBy the way, $h_{ii}$ is also referred to as the *leverage* of a point. It is a\nmeasure of the potential influence of a point (on the parameters, and future\npredictions). $h_{ii}$ is a value between 0 and 1. For a model with $p$\nparameters, the average $h_{ii}$ should be should be $p/n$. We consider points\nfor whom $h_{ii} > 2 \\times p/n$ to be high leverage points.\n\nIn the literature and in textbooks, you will see mention of residuals, standardised \nresiduals and studentised residuals. While they differ in definitions slightly, they\ntypically yield the same information. Hence we shall stick to standardised residuals\nfor our course.\n\n### Normality\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-5}\n\n### Concrete Data Normality Check\n\\index{Concrete slump!residual Normality}\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# R \nr_s <- rstandard(lm_flow_water_slag)\nhist(r_s)\n```\n\n::: {.cell-output-display}\n![](09-regression_files/figure-html/r-concrete-qq-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nqqnorm(r_s)\nqqline(r_s)\n```\n\n::: {.cell-output-display}\n![](09-regression_files/figure-html/r-concrete-qq-1-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Python\nr_s = pd.Series(lm_flow_water_slag.resid_pearson)\nr_s.hist()\n```\n\n::: {.cell-output-display}\n![](09-regression_files/figure-html/py-concrete-qq-1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n:::\n\nWhile it does appear that we have slightly left-skewed data, the departure from\nNormality seems to arise mostly from a thinner tail on the right.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(r_s)\n## \n## \tShapiro-Wilk normality test\n## \n## data:  r_s\n## W = 0.97223, p-value = 0.02882\nks.test(r_s, \"pnorm\")\n## \n## \tAsymptotic one-sample Kolmogorov-Smirnov test\n## \n## data:  r_s\n## D = 0.08211, p-value = 0.491\n## alternative hypothesis: two-sided\n```\n:::\n\n\n\n\n\n\nAt 5% level, the two Normality tests do not agree on the result either. In any case,\nplease do keep in mind where Normality is needed most: in the hypothesis tests.\nThe estimated model is still valid - it is the best fitting line according to\nthe least-squares criteria.\n\n:::\n\n### Scatterplots\n\nTo understand the model fit better, a set of scatterplots are typically made. These\nare plots of standardised residuals (on the $y$-axis) against\n\n* fitted values \n* explanatory variables, one at a time.\n* potential variables.\n\nResiduals are meant to contain only the information that our model cannot explain.\nHence, if the model is good, the residuals should only contain random noise. There \nshould be no apparent pattern to them. If we find such a pattern in one of the above\nplots, we would have some clue as to how we could improve the model.\n\nWe typically inspect the plots for the following patterns:\n\n![](figs/regression_residuals.png)\n\n1. A pattern like the one on the extreme left is ideal. Residuals are \n   randomly distributed around zero; there is no pattern or trend in the \n   plot.\n2. The second plot is something rarely seen. It would probably appear if we \n   were to plot residuals against a *new* variable that is not currently in the \n   model. If we observe this plot, we should then include this variable in the model.\n3. This plot indicates we should include a quadratic term in the model.\n4. The wedge shape (or funnel shape) indicates that we do not have \n   homoscedascity. The solution to this is either a transformation of the response,\n   or weighted least squares. You will cover these in your linear models class.\n   \n   \n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-5}\n\n### Concrete Data Residual Plots\n\\index{Concrete slump!residual plots}\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nopar <- par(mfrow=c(1,3))\nplot(x=fitted(lm_flow_water_slag), r_s, main=\"Fitted\")\nplot(x=concrete$Water, r_s, main=\"X1\")\nplot(x=concrete$Slag, r_s, main=\"X2\")\n```\n\n::: {.cell-output-display}\n![](09-regression_files/figure-html/r-concrete-resids-1.png){fig-align='center' width=960}\n:::\n\n```{.r .cell-code}\npar(opar)\n```\n:::\n\n\n\n\n\n\n#### SAS Plots\n\n![](figs/sas_concrete_flow_water_reg_res1.png){fig-align=\"center\" width=450}\n\n\n:::\n\nWhile the plots of residuals versus explanatory variables look satisfactory, \nthe plot of the residual versus fitted values appears to have funnel shape.\nCoupled with the observations about the deviations from Normality of the \nresiduals in @exm-concrete-4 (thin right-tail), we might want to try a square \ntransform of the response.\n\n:::\n\n### Influential Points\n\nThe influence of a point on the inference can be judged by how much the inference\nchanges with and without the point. For instance to assess if point $i$ is \ninfluential on coefficient $j$:\n\n1.  Estimate the model coefficients with all the data points.\n2.  Leave out the observations $(Y_i , X_i)$ one at a time and re-estimate \n    the model coefficients.\n3.  Compare the $\\beta$'s from step 2 with the original estimate from step 1.\n\nWhile the above method assesses influence on parameter estimates, Cook's distance\nperforms a similar iteration to assess the influence on the fitted values. Cook's\ndistance values greater than 1 indicate possibly influential points.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-concrete-6}\n\n### Concrete Data Influential Points\n\\index{Concrete slump!influential points}\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninfl <- influence.measures(lm_flow_water_slag)\nsummary(infl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPotentially influential observations of\n\t lm(formula = FLOW.cm. ~ Water + Slag, data = concrete) :\n\n   dfb.1_ dfb.Watr dfb.Slag dffit cov.r   cook.d hat  \n60  0.03  -0.03    -0.01    -0.03  1.09_*  0.00   0.06\n69  0.19  -0.19    -0.22    -0.40  0.85_*  0.05   0.02\n83  0.02  -0.02     0.01    -0.03  1.09_*  0.00   0.06\n88 -0.02   0.02    -0.01     0.03  1.09_*  0.00   0.06\n93  0.00   0.00     0.00     0.00  1.10_*  0.00   0.06\n98  0.01  -0.01     0.01    -0.02  1.10_*  0.00   0.06\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe set of 6 points above appear to be influencing the covariance matrix \nof the parameter estimates greatly. To proceed, we would typically leave these \nobservations out one-at-a-time to study the impact on our eventual decision.\n\n#### SAS Output \n\n![](figs/sas_concrete_flow_water_reg_res2.png){fig-align=\"center\" width=450}\n\n::: \n\n:::\n\n\n## Further Reading\n\nThe topic of linear regression is vast. It is an extremely well-established\ntechnique with numerous variations for a multitude of scenarios. Even a single\ncourse on it (ST3131) will not have sufficient time to cover all of it's \ncapabilities. Among topics that will be useful in your career are :\n\n* Generalised additive models, which allow the use of piecewise polynomials \n  for flexible modeling of non-linear functions.\n* Generalised linear models, for modeling non-numeric response.\n* Generalised least squares, to handle correlated observations, and so on.\n* Principal component regression, to handle the issue of multicollinearity\n  (correlated predictors).\n  \nThe textbooks @draper1998applied and @james2013introduction are excellent reference\nbooks for this topic.\n\n## References\n\n### Website References {#sec-web-ref-09}\n\n3. [Stats models documentation](https://www.statsmodels.org/stable/regression.html)\n6. [Diagnostics](https://www.zeileis.org/teaching/AER/Ch-Validation-handout.pdf)\n6. [On residual plots](https://online.stat.psu.edu/stat462/node/120/)\n",
    "supporting": [
      "09-regression_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}