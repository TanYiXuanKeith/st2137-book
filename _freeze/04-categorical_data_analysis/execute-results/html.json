{
  "hash": "9de3157d01a5c30c5391110facc0152f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Categorical Data Analysis\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Introduction\n\nA variable is known as a *categorical variable* if each observation belongs to\none of a set of categories. Examples of categorical variables are: gender,\nreligion, race and type of residence.\n\nCategorical variables are typically modeled using discrete random variables, which \nare strictly defined in terms whether or not the support is countable. The alternative\nto categorical variables are quantitative variables, which are typically modeled \nusing continuous random variables. \n\nAnother method for distinguishing between quantitative and categorical variables\nis to ask if there is a meaningful distance between any two points in the data.\nIf such a distance is meaningful then we have quantitative data. For instance,\nit makes sense to compute the difference in systolic blood pressure between\nsubjects but it does not make sense to consider the mathematical operation\n(\"smoker\" - \"non-smoker\").\n\nIt is important to identify which type of data we have (quantitative or\ncategorical), since it affects the exploration techniques that we can apply.\n\nThere are two sub-types of categorical variables:\n\n* A categorical variable is *ordinal* if the observations can be ordered, but do not\nhave specific quantitative values.\n* A categorical variable is *nominal* if the observations can be classified into\ncategories, but the categories have no specific ordering.\n\nIn this topic, we shall discuss techniques for identifying the presence, and for \nmeasuring the strength, of the association between two categorical variables. \n\n## Contingency Tables\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-chest-pain}\n\n### Chest Pain and Gender\n\nSuppose that 1073 NUH patients who were at high risk for cardiovascular disease\n(CVD) were randomly sampled. They were then queried on two things:\n\n1. Had they experienced the onset of severe chest pain in the preceding 6\nmonths? (yes/no)\n2. What was their gender? (male/female)\n\nThe data would probably have been recorded in the following format (only first\nfew rows shown):\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Gender |Pain    |\n|:------|:-------|\n|female |no pain |\n|male   |no pain |\n|female |no pain |\n|male   |no pain |\n|female |no pain |\n|male   |pain    |\n\n\n:::\n:::\n\n\n\n\n\n\nHowever, it would probably be summarised and presented in this format, which is \nknown as a *contingency table*.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|       | pain| no pain|\n|:------|----:|-------:|\n|male   |   46|     474|\n|female |   37|     516|\n\n\n:::\n:::\n\n\n\n\n\n\nIn a contingency table, each observation from the dataset falls in exactly one \nof the cells. The sum of all entries in the cells equals the number of independent \nobservations in the dataset.\n:::\n\nAll the techniques we shall touch upon in this chapter are applicable to\ncontingency tables. \n\n## Tests for Independence\n\nIn the contingency table above, the two categorical variables are *Gender* and\n*Presence/absence of Pain*. With contingency tables, the main inferential task\nusually relates to assessing the association between the two categorical\nvariables.\n\n::: {.callout-note title=\"Independent Categorical Variables\"}\nIf two categorical variables are **independent**, then the joint distribution of \nthe variables would be equal to the product of the marginals. If two variables \nare not independent, we say that they are **associated**.\n:::\n\n### $\\chi^2$-Test for Independence \n\nThe $\\chi^2$-test uses the definition above to assess if two variables in a \ncontingency table are associated. The null and alternative hypotheses are \n\n\\begin{eqnarray*}\nH_0 &:& \\text{The two variables are indepdendent.}  \\\\\nH_1 &:& \\text{The two variables are not indepdendent.}\n\\end{eqnarray*}\n\nUnder the null hypothesis, we can estimate the joint distribution from the\nobserved marginal counts. Based on this estimated joint distribution, we then \ncompute *expected* counts (which may not be integers) for each cell. The test\nstatistic essentially compares the deviation of *observed* cell counts from the \nexpected cell counts. \n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-chest-pain-2}\n\n### Chest Pain and Gender Expected Counts\n\nContinuing from @exm-chest-pain, we can compute the estimated marginals using \nrow and column proportions\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n------------------------------------------------------------------------------ \nchest_tab (table)\n\nSummary: \nn: 1'073, rows: 2, columns: 2\n\nPearson's Chi-squared test (cont. adj):\n  X-squared = 1.4555, df = 1, p-value = 0.2276\nFisher's exact test p-value = 0.2089\n\n                                     \n                 pain   no pain   Sum\n                                     \nmale     freq      46       474   520\n         p.row   8.8%     91.2%     .\n         p.col  55.4%     47.9% 48.5%\n                                     \nfemale   freq      37       516   553\n         p.row   6.7%     93.3%     .\n         p.col  44.6%     52.1% 51.5%\n                                     \nSum      freq      83       990 1'073\n         p.row   7.7%     92.3%     .\n         p.col      .         .     .\n                                     \n\n----------\n' 95% conf. level\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn the output above, ignore the p-values for now.. We'll get to those in a\nminute. Let $X$ represent gender and $Y$ represent chest pain. Then from the\nestimated proportions in the \"Sum\" row and colums, we can read off the following\nestimated probabilities\n\n\\begin{eqnarray*}\n\\widehat{P}(X = \\text{male}) &=& 0.485  \\\\\n\\widehat{P}(Y = \\text{pain}) &=& 0.077\n\\end{eqnarray*}\n\nConsequently, *under $H_0$*, we would estimate\n$$\n\\widehat{P}(X = \\text{male},\\, Y= \\text{pain}) = 0.485 \\times 0.077 \\approx 0.04\n$$\n\nFrom a sample of size 1073, the expected count for this cell is then \n\n$$\n0.04 \\times 1073 = 42.92\n$$\n:::\n\nUsing the approach above, we can derive a general formula for the expected count\nin each cell:\n$$\n\\text{Expected count} = \\frac{\\text{Row total} \\times \\text{Column total}}{\\text{Total sample size}}\n$$\n\nThe formula for the $\\chi^2$-test statistic (with continuity correction) is:\n$$\n\\chi^2 = \\sum \\frac{(|\\text{expected} - \\text{observed} | - 0.50 )^2}{\\text{expected count}} \n$$ {#eq-chi-sq}\n\nThe sum is taken over every cell in the table. Hence in a $2\\times2$, as in @exm-chest-pain, \nthere would be 4 terms in the summation. \n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-chest-pain-3}\n\n### Chest Pain and Gender $\\chi^2$ Test\n\nLet us see how we can apply and interpret the $\\chi^2$-test for the data in \n@exm-chest-pain.\n\n::: {.panel-tabset}\n\n#### R code\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix(c(46, 37, 474, 516), nrow=2)\ndimnames(x) <- list(c(\"male\", \"female\"), c(\"pain\", \"no pain\"))\nchest_tab <- as.table(x)\n\nchisq_output <- chisq.test(chest_tab)\nchisq_output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  chest_tab\nX-squared = 1.4555, df = 1, p-value = 0.2276\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nchest_array = np.array([[46, 474], [37, 516]])\n\nchisq_output = stats.chi2_contingency(chest_array)\n\nprint(f\"The p-value is {chisq_output.pvalue:.3f}.\")\n## The p-value is 0.228.\nprint(f\"The test-statistic value is {chisq_output.statistic:.3f}.\")\n## The test-statistic value is 1.456.\n```\n:::\n\n\n\n\n\n\n:::\n\nSince the $p$-value is 0.2276, we would not reject the null hypothesis at significance \nlevel 5%. We do not have sufficient evidence to conclude that the variables are not \nindependent.\n\n\nTo extract the expected cell counts, we can use the following code: \n\n::: {.panel-tabset}\n\n#### R code\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq_output$expected\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           pain  no pain\nmale   40.22367 479.7763\nfemale 42.77633 510.2237\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchisq_output.expected_freq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 40.22367195, 479.77632805],\n       [ 42.77632805, 510.22367195]])\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n\nThe test statistic compares the above table to the *observed* table, earlier in\n@exm-chest-pain.\n:::\n\n::: {.callout-important}\nIt is only suitable to use the $\\chi^2$-test when all *expected cell counts* are \nlarger than 5.\n:::\n\n\n### Fisher's Exact Test\n\nWhen the condition above is not met, we turn to Fisher's Exact Test. The null \nand alternative hypothesis are the same, but the test statistic is not derived \nin the same way.\n\nIf the marginal totals are fixed, and the two variables are independent, it can\nbe shown that the individual cell counts arise from a variant of the hypergeometric \ndistribution. The test statistic is based on this observation. The $p$-value is \ntypically obtained by simulating tables with the same marginals as the observed dataset.\n\nUsing Fisher's test sidesteps the need for a large sample size (which aids in the \napproximation of the distribution of the earlier test statistic). Hence the \n\"Exact\" in the name of the test.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-claritin-1}\n\n### Claritin and Nervousness\n\nClaritin is a drug for treating allergies. However, it has a side effect of\ninducing nervousness in patients. From a sample of 450 subjects, 188 of them\nwere randomly assigned to take Claritin, and the remaining were assigned to take\nthe placebo. The observed data was as follows:\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|         | nervous| not nervous|\n|:--------|-------:|-----------:|\n|claritin |       4|         184|\n|placebo  |       2|         260|\n\n\n:::\n:::\n\n\n\n\n\n\nHere is the code to perform the Fisher Exact Test.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <-  matrix(c(4, 2, 184, 260), nrow=2)\ndimnames(y) <- list(c(\"claritin\", \"placebo\"), c(\"nervous\", \"not nervous\"))\nclaritin_tab <- as.table(y)\n\nfisher.test(claritin_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  claritin_tab\np-value = 0.2412\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.399349 31.473382\nsample estimates:\nodds ratio \n  2.819568 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclaritin_tab = np.array([[4, 184], [2, 260]])\n\nstats.fisher_exact(claritin_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSignificanceResult(statistic=2.8260869565217392, pvalue=0.24118420183193057)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\nAs the $p$-value is 0.2412, we again do not have sufficient evidence to reject the \nnull hypothesis and conclude that there is a significant association.\n\nBy the way, we can check (in R) to see that the $\\chi^2$-test would not have\nbeen appropriate:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(claritin_tab)$expected\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(claritin_tab): Chi-squared approximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          nervous not nervous\nclaritin 2.506667    185.4933\nplacebo  3.493333    258.5067\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n### $\\chi^2$-Test for $r \\times c$ Tables\n\nSo far, we have considered the situation of two categorical variables where each\none has only two outcomes (2x2 table). However, it is common that we want to\ncheck the association between two nominal variables where one of them or both\nhave more than 2 outcomes. Consider data given in the table below where both\nvariables are nominal.\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-pol-assoc-1}\n\n### Political Association and Gender\n\nConsider data given in the table below where both variables are nominal. It \ncross-classifies poll respondents according to their gender and their political \nparty affiliation.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|       | Dem| Ind| Rep|\n|:------|---:|---:|---:|\n|female | 762| 327| 468|\n|male   | 484| 239| 477|\n\n\n:::\n:::\n\n\n\n\n\n\nThe R code for applying the test is identical to before\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix(c(762,327,468,484,239,477), ncol=3, byrow=TRUE)\ndimnames(x) <- list(c(\"female\", \"male\"), \n                    c(\"Dem\", \"Ind\", \"Rep\"))\npolitical_tab <- as.table(x)\nchisq.test(political_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  political_tab\nX-squared = 30.07, df = 2, p-value = 2.954e-07\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn this case, there is strong evidence to reject $H_0$. At 5% level, we would\nreject the null hypothesis and conclude there is an association between gender\nand political affiliation.\n\n:::\n\nThe $\\chi^2$-test can be extended to tables larger than 2x2. In general, we might \nhave $r$ rows and $c$ columns. The null and alternative hypotheses are identical\nto the 2x2 case, and the test statistic is computed in the same way. However,\nunder the null hypothesis, the test statistic follows a $\\chi^2$ distribution with \n$(r-1)(c-1)$ degrees of freedom.\n\nThe $\\chi^2$-test is based on a model of independence - the expected counts are \nderived under this assumption. As such, it is possible to derive residuals and \nstudy them, to see where the data deviates from this model.\n\nWe define the *standardised residuals* to be \n\n$$\nr_{ij} = \\frac{n_{ij} - \\mu_{ij}}{\\sqrt{\\mu_{ij} (1 - p_{i+})(1 -p_{+j} )}}\n$$\nwhere \n\n* $n_{ij}$ is the observed cell count in row $i$ and column $j$ (cell $ij$).\n* $\\mu_{ij}$ is the *expected* cell count in row $i$ and column $j$\n* $p_{i+}$ is the marginal probability of row $i$\n* $p_{+j}$ is the marginal probability of column $j$.\n\nThe residuals can be obtained from the test output. Under $H_0$, the residuals\nshould be close to a standard Normal distribution. If the residual for a particular \ncell is very large (or small), we suspect that lack of fit (to the independence model)\ncomes from that cell.\n\nFor the political association table, the standardised residuals (from R) are:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(political_tab)$stdres\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Dem        Ind        Rep\nfemale  4.5020535  0.6994517 -5.3159455\nmale   -4.5020535 -0.6994517  5.3159455\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Measures of Association\n\nThis sections covers bivariate measures of association for contingency tables.\n\n### Odds Ratio\n\nThe most generally applicable measure of association, for 2x2 tables with\nnominal variables, is the Odds Ratio (OR). Suppose we have $X$ and $Y$ to be Bernoulli \nrandom variables with (population) success probabilities $p_1$ and $p_2$. \n\nWe define the odds of success for $X$ to be \n$$\n\\frac{p_1}{1-p_1}\n$$\nSimilarly, the odds of success for random variable $Y$ is $\\frac{p_2}{1-p_2}$. \n\nIn order to measure the strength of their association, we use the *odds ratio*:\n$$\n\\frac{p_1/ (1-p_1)}{p_2/(1-p_2)}\n$$\n\nThe odds ratio can take on any value from 0 to $\\infty$.\n\n* A value of 1 indicates no association between $X$ and $Y$. If $X$ and $Y$ were \n  independent, this is what we would observe.\n* Deviations from 1 indicate stronger association between the variables.\n* Note that deviations from 1 are not symmetric. For a given pair of variables,\n  an association of 0.25 or 4 is the same - it is just a matter of which variable\n  we put in the numerator odds.\n  \nDue to the above asymmetry, we often use the log-odds-ratio instead:\n$$\n\\log \\frac{p_1/ (1-p_1)}{p_2/(1-p_2)} \n$$\n\n* Log-odds-ratios can take values from $-\\infty$ to $\\infty$.\n* A value of 0 indicates no association between $X$ and $Y$.\n* Deviations from 0 indicate stronger association between the variables, and deviations \n  are now symmetric; a log-odds-ratio of -0.2 indicates the same *strength* as 0.2, just \n  the opposite direction.\n  \nTo obtain a confidence interval for the odds-ratio, we work with the log-odds ratio and \nthen exponentiate the resulting interval. Here are the steps:\n\n1. The sample data in a 2x2 table can be labelled as $n_{11}, n_{12}, n_{21}, n_{22}$.\n2. The *sample* odds ratio is \n$$\n\\widehat{OR} = \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}}\n$$\n3. For a large sample size, it can be shown that $\\log \\widehat{OR}$ follows a Normal \n   distribution. Hence a 95% confidence interval can be obtained through\n$$\n\\log \\frac{n_{11} \\times n_{22}}{n_{12} \\times n_{21}} \\pm z_{0.025} \n\\times ASE(\\log \\widehat{OR})\n$$\n\nwhere \n\n* the ASE (Asymptotic Standard Error) of the estimator is \n$$\n\\sqrt{\\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}} \n$$\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-chest-pain-4}\n\n### Chest Pain and Gender Odds Ratio\n\nLet us compute the confidence interval for the odds ratio in the chest pain and\ngender example from earlier.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\nOddsRatio(chest_tab,conf.level = .95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nodds ratio     lwr.ci     upr.ci \n 1.3534040  0.8626023  2.1234612 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code \n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\nchest_tab2 = sm.stats.Table2x2(chest_array)\n\nprint(chest_tab2.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate   SE   LCB    UCB  p-value\n--------------------------------------------------\nOdds ratio        1.353        0.863 2.123   0.188\nLog odds ratio    0.303 0.230 -0.148 0.753   0.188\nRisk ratio        1.322        0.872 2.004   0.188\nLog risk ratio    0.279 0.212 -0.137 0.695   0.188\n--------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::\n\n### For Ordinal Variables\n\nWhen both variables are ordinal, it is often useful to compute the strength \n(or lack) of any monotone trend association. It allows us to assess if\n\n> As the level of $X$ increases, responses on $Y$ tend to increase toward \n> higher levels, or responses on $Y$ tend to decrease towards lower levels.\n\nFor instance, perhaps job satisfaction tends to increase as income does. In this\nsection, we shall discuss a measure for ordinal variables, analogous to\nPearson's correlation for quantitative variables, that describes the degree to \nwhich the relationship is monotone. It is based on the idea of a concordant or\ndiscordant pair of subjects.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #def-con-dis-1}\n\n* A **pair of subjects** is *concordant* if the subject ranked higher on $X$ also \n  ranks higher on $Y$. \n* A **pair** is *discordant* if the subject ranking higher on $X$ ranks lower on $Y$.\n* A **pair** is *tied* if the subjects have the same classification on $X$ and/or $Y$.\n\n:::\n\nIf we let \n\n* $C$: number of concordant pairs in a dataset, and\n* $D$: number of discordant pairs in a dataset.\n\nThen if $C$ is much larger than $D$, we would have reason to believe that there \nis a strong positive association between the two variables. Here are two measures\nof association based on $C$ and $D$:\n\n1. Goodman-Kruskal $\\gamma$ is computed as \n$$\n\\gamma = \\frac{C - D}{C + D}\n$$\n2. Kendall $\\tau_b$ is \n$$\n\\tau_b = \\frac{C - D}{A}\n$$\n   where $A$ is a normalising constant that results in a measure that works better with ties,\n   and is less sensitive than $\\gamma$ to the cut-points defining the categories. \n   $\\gamma$ has the advantage that it is more easily interpretable.\n\nFor both measures, values close to 0 indicate a very weak trend, while values \nclose to 1 (or -1) indicate a strong positive (negative) association.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-job-income-1}\n\n### Job Satisfaction by Income\n\nConsider the following table, obtained from @agresti2012categorical. The original\ndata come from a nationwide survey conducted in the US in 1996.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix(c(1, 3, 10, 6,\n              2, 3, 10, 7,\n              1, 6, 14, 12,\n              0, 1,  9, 11), ncol=4, byrow=TRUE)\ndimnames(x) <- list(c(\"<15,000\", \"15,000-25,000\", \"25,000-40,000\", \">40,000\"), \n                    c(\"Very Dissat.\", \"Little Dissat.\", \"Mod. Sat.\", \"Very Sat.\"))\nus_svy_tab <- as.table(x)\n\noutput <- Desc(x, plotit = FALSE, verbose = 3)\noutput[[1]]$assocs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       estimate  lwr.ci  upr.ci\nContingency Coeff.       0.2419       -       -\nCramer V                 0.1439  0.0000  0.1693\nKendall Tau-b            0.1524 -0.0083  0.3130\nGoodman Kruskal Gamma    0.2211 -0.0085  0.4507\nStuart Tau-c             0.1395 -0.0082  0.2871\nSomers D C|R             0.1417 -0.0080  0.2915\nSomers D R|C             0.1638 -0.0116  0.3392\nPearson Correlation      0.1772 -0.0241  0.3647\nSpearman Correlation     0.1769 -0.0245  0.3645\nLambda C|R               0.0377  0.0000  0.2000\nLambda R|C               0.0159  0.0000  0.0693\nLambda sym               0.0259  0.0000  0.1056\nUncertainty Coeff. C|R   0.0311 -0.0076  0.0699\nUncertainty Coeff. R|C   0.0258 -0.0069  0.0585\nUncertainty Coeff. sym   0.0282 -0.0072  0.0637\nMutual Information       0.0508       -       -\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy import stats\n\nus_svy_tab = np.array([[1, 3, 10, 6], \n                      [2, 3, 10, 7],\n                      [1, 6, 14, 12],\n                      [0, 1,  9, 11]])\n\ndim1 = us_svy_tab.shape\nx = []; y=[]\nfor i in range(0, dim1[0]):\n    for j in range(0, dim1[1]):\n        for k in range(0, us_svy_tab[i,j]):\n            x.append(i)\n            y.append(j)\n\nstats.kendalltau(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSignificanceResult(statistic=0.15235215134659688, pvalue=0.0860294855671433)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\nThe output shows that both $\\gamma = 0.22$ and $\\tau_b =0.15$ are close to\nsignificant. The lower confidence limit is close to being positive.\n:::\n\n## Visualisations \n\n### Bar charts\n\nA common method for visualisation cross combinations of categorical data is \nto use a bar chart. \n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-pol-assoc-3}\n\n### Political Association and Gender Barchart\n\nHere is the R code to make the barchart for the political affiliations data,\ndirectly from the contingency table in @exm-pol-assoc-1.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lattice)\nbarchart(political_tab/rowSums(political_tab), \n         horizontal = FALSE)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-html/r-pol-3-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\nHowever, the downside is that it does not reflect that the marginal count for \nmales was much less than that for females. \n\n:::\n\nLet us turn to making bar charts with pandas and Python. We shall use the table \nfrom the Claritin example earlier in @exm-claritin-1:\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-claritin-2}\n\n### Claritin and Nervousness Barchart\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nclaritin_prop = claritin_tab/claritin_tab.sum(axis=1).reshape((2,1))\n\nxx = pd.DataFrame(claritin_prop, \n                  columns=['nervous', 'not_nervous'], \n                  index=['claritin', 'placebo'])\n\nxx.plot(kind='bar', stacked=True)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-html/py-claritin-2-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\n:::\n\n### Mosaic plots\n\nUnlike a bar chart, a mosaic plot reflects the count in each cell (through the\narea), along with the proportions of interest. Let us inspect how we can make\nmosaic plots for the political association data earlier.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-pol-assoc-2}\n\n### Political Association and Gender Mosaic Plot\n\nThe colours in the output for the R code reflect the sizes of the standardised \nresiduals displayed below @exm-pol-assoc-1.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmosaicplot(political_tab, shade=TRUE)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-html/r-pol-4-3.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom statsmodels.graphics.mosaicplot import mosaic\nimport matplotlib.pyplot as plt\n\npolitical_tab = np.asarray([[762,327,468], [484,239,477]])\nmosaic(political_tab, statistic=True, gap=0.05);\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-html/py-pol-4-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\n:::\n\n:::\n\n### Conditional Density Plots\n\nWhen we have one categorical and one quantitative variable, the kind of plot we \nmake really depends on which is the response, and which is the explanatory variable. If \nthe response variable is the quantitative one, it makes sense to create \nboxplots or histograms. However, if the response variable is a the categorical one,\nwe should really be making something along these lines:\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-heart-1}\n\nThe dataset at [UCI repository](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records) \ncontains records on 299 patients who had heart failure. The data was collected\nduring the follow-up period; each patient had 13 clinical features recorded. The\nprimary variable of interest was whether they died or not. Suppose we wished to\nplot how this varied with age (a quantitative variable):\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nheart_failure <- read.csv(\"data/heart+failure+clinical+records/heart_failure_clinical_records_dataset.csv\")\nspineplot(as.factor(DEATH_EVENT) ~ age, data=heart_failure)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-html/r-cd-1-3.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\nIt reflects how the probability of an event varies with the quantitative explanatory \nvariable. A smoothed version of this is known as the conditional density plot:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncdplot(as.factor(DEATH_EVENT) ~ age, data=heart_failure)\n```\n\n::: {.cell-output-display}\n![](04-categorical_data_analysis_files/figure-html/r-cd-2-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n\n\n:::\n\n## Further readings\n\nIn general, I have found that R packages seem to have a lot more measures of\nassociation for categorical variables. In Python, the measures are spread out \nacross packages.\n\nAbove, we have only scratched the surface of what is available. If you are keen,\ndo read up on\n\n1. Somer's D (for association between nominal and ordinal)\n2. Mutual Information (for association between all types of pairs of categorical variables)\n3. Polychoric correlation (for association between two ordinal variables)\n\nAlso, take note of how log odds ratios, $\\tau_b$ and $\\gamma$ work - they range\nbetween -1 to 1 (in general), and values close to 0 reflect weak association.\nValues of $a$ and $-a$ indicate the same *strength*, but different direction of\nassociation. This allows the same intuition that Pearson's correlation does.\nWhen you are presented with new metrics, try to understand them by asking\nsimilar questions about them.\n\n## References\n\n### Website References\n\n1. Documentation pages from statsmodels: \n   * [Mosaic plots](https://www.statsmodels.org/stable/generated/statsmodels.graphics.mosaicplot.mosaic.html#statsmodels.graphics.mosaicplot.mosaic)\n   * [Contingency tables](https://www.statsmodels.org/stable/contingency_tables.html#module-statsmodels.stats.contingency_tables)\n2. Documentation pages from scipy:\n   * [Fisher test with scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html) \n   * [$\\chi^2$-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)\n3. [Heart failure data](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records) \n4. [More information on Kendall $\\tau_b$ statistic](https://online.stat.psu.edu/stat509/lesson/18/18.3)\n5. The $\\chi^2$ test we studied is a *test of independence*. It is a variant of the \n   $\\chi^2$ goodness-of-fit test, which is used to assess if data come from a particular\n   distribution. It's just in our case, the presumed distribution is one with \n   independence between the groups. Read more about the \n   [goodness-of-fit test here](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/11%3A_Chi-Square_Tests_and_F-Tests/11.02%3A_Chi-Square_One-Sample_Goodness-of-Fit_Tests).\n",
    "supporting": [
      "04-categorical_data_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}