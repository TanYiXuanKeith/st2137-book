{
  "hash": "adc4c91f302d456879da98fae24012e1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulation\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n## Introduction {#sec-simulation}\n\nThe objective of any simulation study is to estimate an expectation $E(X)$.\nSimulation studies involve the use of a computer to generate independent copies \nof the random variable of interest $X$. Here are a couple of examples where \nsimulation studies would be applicable.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-ins-01}\n\n### Insurance Claims\n\nBefore the financial year begins, an insurance company has to decide how much\ncash to keep, in order to pay out the claims for that year. Suppose that \nclaims are independent of each other and are distributed as $Exp(1/200)$[^10-exp]\ndollars. Also suppose that the number of claims in a year is a Poisson\nrandom variable with mean 8.2.\n\nAn actuary has been asked to determine the size of the reserve fund that should\nbe set up, and he recommends $12,000. We might consider answering the following\nquestion using simulation: \n\n* What is the probability that the total claims will exceed the reserve fund?\n\nIf we let $Y$ be the random variable representing the total sum of claims, we\nare interested in estimating $P(Y > 12000)$. Since probabilities are expectations,\nwe can use simulation to estimate this value.\n\n[^10-exp]: $f_X(x) = \\frac{1}{200} \\exp(-x/200),\\; x > 0$\n\n:::\n\n<br>\n\nHere is a slightly more sophisticated example.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-sandwich-01}\n\n### Sandwich Shop Closing Time\n\nSuppose that you run a sandwich shop, which is open from 9am till 5pm. Your\nphilosophy has always been to serve every customer who has entered before 5pm,\neven if that requires you to stay back until they have been served. You\nwould like to estimate the mean amount of overtime you have to work.\n\nIf you are willing to assume that the inter-arrival times of customers is \n$Exp(3)$ hours, then it is possible to simulate this process to estimate the \nmean time that you would have to remain open, beyond 5pm.\n\n:::\n\nThe two examples above are known as Discrete Event Simulations. In our course, \nwe will not get to working with such scenarios. However, we will try to understand\nand familiarise ourselves with the basic building blocks of simulation studies.\nFor more knowledge, do enrol yourself in ST3247!\n\nThe basic steps in a simulation study are:\n\n1. Identify the random variable of interest and write a program to simulate it.\n2. Generate an iid sample $X_1, X_2, \\ldots, X_n$ using this program.\n3. Estimate $E(X)$ using $\\bar{X}$.\n\nThis is sometimes referred to as Monte Carlo Simulation.  Before proceeding,\nlet us refresh our knowledge of the properties of the sample mean.\n\n## Theory\n\nThere are two important theorems that simulation studies rely on. The first \nis the Strong Law of Large Numbers (SLLN).\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #thm-slln}\n\n### Strong Law of Large Numbers\n\nIf $X_1, X_2, \\ldots, X_n$ are independent and identically distributed with \n$E(X) < \\infty$, then \n$$\n\\bar{X} =\\frac{1}{n} \\sum_{i=1}^n X_i\\rightarrow E(X) \\quad \\text{with probability 1.}\n$$\n\n:::\n\nIn the simulation context, it means that as we generate more and more samples\n(i.e. increase $n$), our sample mean $\\bar{X}$ converges to the desired value \n$E(X)$, *no matter what the distribution of $X$ is.*\n\nThe second theorem that aids us is the Central Limit Theorem (CLT).\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #thm-clt}\n\n### Central Limit Theorem\n\nLet $X_1, X_2, \\ldots, X_n$ be i.i.d., and suppose that \n\n*  $-\\infty < E(X_1) = \\mu < \\infty$. \n* $Var(X_1) = \\sigma^2 < \\infty$.\n\nThen \n$$\n\\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sigma} \\Rightarrow N(0,1)\n$$\nwhere $\\Rightarrow$ denotes convergence in distribution.\n:::\n\nThis is sometimes informally interpreted to mean that when $n$ is\nlarge, $\\bar{X}$ is approximately Normal with mean $\\mu$ and variance\n$\\sigma^2/n$. In the simulation context, we can use this theorem to obtain a\nconfidence interval for the expectation that we are estimating.\n\nAlso take note of the following properties of the sample mean and variance:\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #thm-unbiased}\n\n### Sample Estimates\n\nIt can be shown that both the sample mean and sample standard deviation are \nunbiased estimators.\n$$\nE(\\bar{X}) = E(X), \\quad E(s^2) = \\sigma^2\n$$\nwhere $s^2 = \\frac{\\sum (X_i - \\bar{X})^2}{n-1}$.\n:::\n\nTo obtain a $(1-\\alpha)100%$ confidence interval for $\\mu$, we use the following \nformula, from the CLT:\n\n$$\n\\bar{X} \\pm z_{1-\\alpha/2} \\frac{s}{\\sqrt{n}}\n$$\n\nWhen our goal is to estimate a probability $p$, we have to introduce a\ncorresponding indicator variable $X$ such that \n\n$$\nX = \n\\begin{cases}\n1 & \\text{with probability $p$} \\\\\n0 & \\text{with probability $1- p$}\n\\end{cases}\n$$\n\nIn this case, the formula for the CI becomes \n$$\n\\bar{X} \\pm z_{1-\\alpha/2} \\sqrt{\\frac{\\bar{X}(1-\\bar{X})}{n}}\n$$\n\n## Generating Random Variables in R and Python\n\nBoth R and Python contain built-in routines for generating random variables from\ncommon \"named\" distributions, e.g. Normal, Uniform, Gamma, etc. All software\nthat can generate random variables utilise Pseudo-Random Number Generators\n(PRNG). These are routines that generate sequences of deterministic numbers with\nvery very long cycles. However, since they pass several tests of randomness,\nthey can be treated as truly random variables for all intents and purposes.\n\nIn both software, we can set the \"seed\". This initialises the random number\ngenerator. When we reset the seed, we can reproduce the stream of random\nvariables exactly. This feature exists:\n\n1. For debugging purposes,\n2. To allow us to study the sensitivity of our results to the seed.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-rng-1}\n\n### Random Variable Generation\n\n::: {.panel-tabset}\n\n#### R code \n\nIn R, all the functions for generating random variables begin with `r` (for \nrandom). Here are a few such functions:\n\n| Function name | Random Variable |\n|----------|----------|\n| `rnorm`  | Normal   |\n| `runif`  | Uniform  |\n| `rgamma` | Gamma    |\n| `rpois`  | Poisson  |\n| `rbinom` | Binomial |\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#R \nset.seed(2137)\n\nopar <- par(mfrow=c(1,3))\nY <- rgamma(50, 2, 3)\nhist(Y)\n\nW <- rpois(50, 1.3) # 50 obs from Pois(1.3)\nbarplot(table(W))\n\nZ <- rbinom(50, size=2, 0.3) # 50 obs from Binom(2, 0.3)\nbarplot(table(Z))\n```\n\n::: {.cell-output-display}\n![](10-simulation_files/figure-pdf/r-rng-1-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code}\npar(opar)\n```\n:::\n\n\n\n\n\n\n\n\n#### Python code \n\nIn Python, we can generate random variables using `numpy` and/or `scipy`. In our \ncourse, we shall use the `scipy` routines because its implementation is closer \nin spirit to R.\n\n| Function name | Random Variable |\n|----------|----------|\n| `norm`  | Normal   |\n| `uniform`  | Uniform  |\n| `gamma` | Gamma    |\n| `poisson`  | Poisson  |\n| `binom` | Binomial |\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Python\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import binom, gamma, norm, poisson\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(2137)\nfig, ax = plt.subplots(1, 3, figsize=(8,4))\n\nax1 = plt.subplot(131)\nr = gamma.rvs(2, 3, size=50, random_state=rng)\nax1.hist(r);\n\nax1 = plt.subplot(132)\nr = poisson.rvs(1.3, size=50, random_state=rng)\nax1.hist(r);\n\nax1 = plt.subplot(133)\nr = binom.rvs(2, 0.3, size=50, random_state=rng)\nax1.hist(r);\n```\n\n::: {.cell-output-display}\n![](10-simulation_files/figure-pdf/py-rng-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n## Monte-Carlo Integration\n\nSuppose we wish to evaluate\n\n$$\n\\int_{-\\infty}^{\\infty} h(x) f(x) \\text{d}x\n$$\nwhere $f(x)$ is a pdf. The integral above is in fact equal to $E(h(X))$, where\n$X \\sim f$. Hence we can use everything we have discussed so far, to evaluate\nthe expression using simulation! Everything depends on:\n\n1. Being able to introduce a pdf to the integral\n2. Being to able to simulate from that pdf. \n\n::: {.callout-important}\nIt is critical that the support of the pdf is the same as the range of integration.\n:::\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-mc-1}\n\n### Monte Carlo Integration over (0,1)\n\nSuppose Suppose we wish to evaluate\n$$\n\\theta = \\int_0^1 e^{2x} dx = \\int_0^1 e^{2x} \\cdot 1\\; dx \n$$\n\nWe can identify that this is equal to $E(h(X))$ where \n\n* $X \\sim Unif(0,1)$.   \n* $h(X) = e^{2x}$\n\nThus we can follow this pseudo-code:\n\n1. Generate $X_1,X_2,\\ldots,X_n \\sim Unif(0,1)$.\n2. Estimate the integral using\n$$\n\\frac{1}{n} \\sum_{i=1}^n e^{2 X_i}\n$$\n\nIn this simple case, we can in fact work out analytically that the integral is \nequal to\n$$\n\\frac{1}{n}(e^2 - 1) = 3.195\n$$\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2138)\nX <- runif(50000, 0, 1)\nhX <- exp(2*X)\n(mc_est <- mean(hX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.185626\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import uniform\n\nX = uniform.rvs(0,1, size=50000, random_state=rng)\nhX = np.exp(2*X)\nhX.mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.2090955591090915\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n## Simulation Studies\n\nIn this section, we shall touch on how we can use simulation in some scenarios that are\ncloser to the real world.\n\n### Confidence Intervals\n\nThe usual 95% confidence interval for a mean is given by \n$$\n\\bar{X} \\pm t_{0.025} s/\\sqrt{n}\n$$\nwhere $t_{0.025}$ is the 0.025 quantile of the t-distribution with $n-1$\ndegrees. The resulting interval should contain the true mean in 95% of the\nexperiments. However, it is derived under the assumption that the data is\nNormally distributed. Let us see if it still works if the data is from an\nasymmetric distribution: $Pois(0.5)$.\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-ci-1}\n\n### Coverage of Confidence Interval\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R \nset.seed(2139)\noutput_vec <- rep(0, length=100)\nn <- 20\nlambda <- 0.5\nfor(i in 1:length(output_vec)) {\n  X <- rpois(15, .5)\n  Xbar <- mean(X)\n  s <- sd(X)\n  t <- qt(0.975, n-1)\n  CI <- c(Xbar - t*s/sqrt(n), Xbar + t*s/sqrt(n))\n  if(CI[1] < lambda & CI[2] > lambda) {\n    output_vec[i] <- 1\n  }\n}\nmean(output_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.84\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrng = np.random.default_rng(2137)\noutput_vec = np.zeros(100)\nn = 20\nlambda_ = 0.5\nfor i in range(100):\n    X = poisson.rvs(0.5, size=15, random_state=rng)\n    Xbar = X.mean()\n    s = X.std()\n    t = norm.ppf(0.975)\n    CI = [Xbar - t*s/np.sqrt(n), Xbar + t*s/np.sqrt(n)]\n    if CI[0] < lambda_ and CI[1] > lambda_:\n        output_vec[i] = 1\noutput_vec.mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.86\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n\n### Type I Error\n\nConsider the independent two-sample $t$-test that we introduced in topic 7. The\nformal set-up also includes the assumption that our data arises from a Normal\ndistribution. According to the theory of the $t$-test, if both groups have the\nsame mean, we should *falsely* reject the null hypothesis 10% of the time if we\nperform it at 10% significance level. Let us assess if this is what actually\nhappens.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-t-test-1}\n\n### T-test Type I Error\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_one_test <- function(n=100) {\n  X <- rnorm(n)\n  Y <- rnorm(n)\n  t_test <- t.test(X, Y,var.equal = TRUE)\n  # extract the p-value from the t_test\n  if(t_test$p.value < 0.10) \n    return(1L) \n  else \n    return(0L)\n}\n\nset.seed(11)\noutput_vec <- vapply(1:2000, \n                     function(x) generate_one_test(), \n                     1L)\nmean(output_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.108\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef generate_one_test(n=100):\n    X = norm.rvs(0, 1, size=n, random_state=rng)\n    Y = norm.rvs(0, 1, size=n, random_state=rng)\n    t_test = stats.ttest_ind(X, Y, equal_var=True)\n    if t_test.pvalue < 0.10:\n        return 1\n    else:\n        return 0\noutput_vec = np.array([generate_one_test() for j in range(2000)])\noutput_vec.mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.102\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n### Newspaper Inventory\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-newspaper-1}\n\nSuppose that daily demand for newspaper is approximately gamma distributed, with \nmean 10,000 and variance 1,000,000. The newspaper prints and distributes 11,000 \ncopies each day. The profit on each newspaper sold is $1, and the loss on each \nunsold newspaper is 0.25. Formally, the daily profit function h is\n\n$$\nh(X) = \n\\begin{cases}\n11000 & \\text{if } X ≥ 11000 \\\\\n\\lfloor X \\rfloor + (11000 - \\lfloor X \\rfloor)(−0.25) & \\text{if } X < 11000\n\\end{cases}\n$$\n\nwhere $X$ represents the daily demand. Let us estimate the expected daily \nprofit using simulation.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R code to estimate the expected daily profit\nset.seed(2141)\nn <- 10000\nX <- rgamma(n, 100, rate=1/100)\nhX <- ifelse(X >= 11000, 11000, floor(X) + (11000 - floor(X)) * (-0.25))\n#mean(hX)\n\n# 90% CI for the mean\ns <- sd(hX)\nq1 <- qnorm(0.95)\nCI <- c(mean(hX) - q1*s/sqrt(n), mean(hX) + q1*s/sqrt(n))\ncat(\"The 90% CI for the mean is (\", format(CI[1], digits=2, nsmall=2), \", \", \n    format(CI[2], digits=2, nsmall=2), \").\\n\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe 90% CI for the mean is (9639.10, 9673.69).\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 10000\nX = gamma.rvs(100, scale=100, size=n, random_state=rng)\nhX = np.where(X >= 11000, 11000, np.floor(X) + (11000 - np.floor(X)) * (-0.25))\n\n# 90% CI for the mean\nXbar = hX.mean()\ns = hX.std()\nt = norm.ppf(0.95)\nCI = [Xbar - t*s/np.sqrt(n), Xbar + t*s/np.sqrt(n)]\nprint(f\"The 90% CI for the mean is ({CI[0]: .3f}, {CI[1]: .3f}).\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe 90% CI for the mean is ( 9623.529,  9658.387).\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n:::\n\n:::\n\n## Resampling Methods\n\nThe next section introduces two techniques that are based on resampling the data.\n\n### Permutation Test {#sec-perm-test}\n\nConsider the two-sample t-test that we introduced in topic 06. This parametric \ntest requires us to check if the data from the two groups came from Normal distributions.\nThe non-parametric version requires each group to have at least 10 observations.\nWhat if our data satisfies neither criteria? \n\nThe permutation test is applicable in such a situation. It makes no distributional\nassumptions whatsoever on the data. Here is pseudo-code for how it works:\n\n1. Compute the difference in group means. This observed difference is the test \n   statistic.\n2. Treating the observed values as fixed, combine the two vectors of observations.\n3. Permute the observations, and then re-assign them to the two groups.\n4. Compute the difference between the group means.\n5. Repeat steps 2 - 4 multiple times (order of 1000). \n\nThe $p$-value for the null hypothesis can be computed by computing the proportion of \nsimulated statistics that were larger in absolute value than the observed one.\n\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-abalone-1-repeat}\n### Abalone Data \n\\index{Abalone!permutation test}\n\nIn the abalone dataset, this was the output from the t-test:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabl <- read.csv(\"data/abalone_sub.csv\")\nx <- abl$viscera[abl$gender == \"M\"]\ny <- abl$viscera[abl$gender == \"F\"]\n\nt.test(x, y, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  x and y\nt = 0.91008, df = 98, p-value = 0.365\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.02336287  0.06294287\nsample estimates:\nmean of x mean of y \n  0.30220   0.28241 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThis would be the procedure for a permutation test:\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd1 <- mean(x)  - mean(y)\nprint(d1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01979\n```\n\n\n:::\n\n```{.r .cell-code}\ngenerate_one_perm <- function(x, y) {\n  n1 <- length(x)\n  n2 <- length(y)\n  xy <- c(x,y)\n  xy_sample <- sample(xy)\n  d1 <- mean(xy_sample[1:n1]) - mean(xy_sample[-(1:n1)])\n  d1\n}\nsampled_diff <- replicate(2000, generate_one_perm(x,y))\nhist(sampled_diff)\n```\n\n::: {.cell-output-display}\n![](10-simulation_files/figure-pdf/abalone-2-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code}\n(p_val <- 2*mean(sampled_diff > d1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.369\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n### Bootstrapping\n\nThe video on Canvas provides a very brief introduction to bootstrapping. One of\nthe greatest benefits of the bootstrap is the ability to provide confidence\nintervals for estimators for which we may not have the know-how to derive\nanalytic or asymptotic results.\n\nConsider obtaining a confidence interval for the trimmed mean, that we\nencountered in @sec-robust.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-chem-1}\n\\index{Copper!bootstrap CI}\n\nThis is how we can use bootstrapping to obtain a confidence interval for a \ntrimmed mean.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n\nmean(chem)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.280417\n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(chem)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  chem\nt = 3.9585, df = 23, p-value = 0.0006236\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.043523 6.517311\nsample estimates:\nmean of x \n 4.280417 \n```\n\n\n:::\n\n```{.r .cell-code}\n## [1] 4.280417\n```\n:::\n\n\n\n\n\n\n\n\nNotice how the CI from the non-robust technique is very wide.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(boot)\n\nstat_fn <- function(d, i) {\n  b <- mean(d[i], trim=0.1)\n  b\n}\nboot_out <- boot(chem, stat_fn, R = 1999, stype=\"i\")\n# Returns two types of bootstrap intervals:\nboot.ci(boot.out = boot_out, type=c(\"perc\", \"bca\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1999 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = c(\"perc\", \"bca\"))\n\nIntervals : \nLevel     Percentile            BCa          \n95%   ( 2.955,  4.700 )   ( 2.970,  4.773 )  \nCalculations and Intervals on Original Scale\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThe boot function requires a function (`stat_fn`) that computes the statistic\nfrom the bootstrapped sample.  Note that the intervals returned  from the trimmed \nmean are much narrower.\n\n:::\n\n\n## Summary\n\nIn this chapter, we have seen how simulation studies can be used to estimate \nexpectations. Although we have restricted ourselves to very straightforward\nexamples, the same principles can be applied to more complex scenarios. \n\nIn particular, there are three types of simulation models widely used to \nmodel complex systems:\n\n1. Agent-based models. These are regularly used to model the interactions of \n   agents in a system, e.g. humans.\n2. Discrete Event Simulations. These are used to model systems where events\n   occur at discrete points in time. Typically, these are systems where there is an\n   arrival of entities, a service, and a departure.\n3. Process modeling. This approach is used to model the flow of entities through\n   a system. They are sometimes also referred to as compartment models.\n\nPlease refer to the sections below for more information. For our course,\nplease be familiar with the basic building blocks of simulation studies:\n\n1. Generate iid samples, and then\n2. Compute the sample mean, and the CI for the true mean.\n\n## References\n\n### Website References {#sec-web-ref-10}\n\n1. Some GUI-based software for simulation:\n   * [Anylogic](https://www.anylogic.com/) A very very powerful software for \n     agent-based modeling.\n   * [Arena](https://www.rockwellautomation.com/en-us/products/software/arena-simulation.html) A discrete event simulator, with a free academic license.\n   * [Netlogo](https://ccl.northwestern.edu/netlogo/) An open source software for agent-based modeling.\n2. Python software:\n   * [simpy](https://simpy.readthedocs.io/en/latest/) for discrete event simulations.\n   * [mesa](https://github.com/projectmesa/mesa) for agent-based modeling.\n3. R software:\n   * [simmer](https://r-simmer.org/) for discrete event simulation\n4. [scipy documentation](https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions)\n5. [Introduction to Bootstrapping](https://online.stat.psu.edu/stat555/node/119/)",
    "supporting": [
      "10-simulation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}