{
  "hash": "6c15019fc5a4d41a9ffd0417830d960a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Summarising Numerical Data\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Introduction\n\nThis is the first topic where we are going to see how to perform statistical\nanalysis using two software: R and Python. Every language has it's strengths and\nweaknesses, so instead of attempting to exactly replicate the same chart/output\nin each software, we shall try to understand their respective approaches better.\n\nOur end goal is to analyse data - toward that end, we should be versatile and\nadaptable. Focus on learning how to be fast and productive in whichever\nenvironment you are told to work in. If you have a choice, then be aware of what\neach framework can do best, so that you can choose the right one.\n\nAlthough it is a simplistic view, the following diagram, @fig-sum-1, provides a\nuseful taxonomy of the types of columns we might have in our dataset. Having at\nleast an initial inkling of the type of data matters, because it helps decide\nwhat type of summary to generate or what type of plot to make.\n\n![Data types](figs/summ_data-01.png){#fig-sum-1 fig-alt=\"Data types\" fig-align=\"center\" width=\"60%\"}\n\nThere are two main ways of summarising data: numerically and graphically. This topic\nwill cover:\n\n1. Numerical summaries for univariate quantitative variables.\n2. Numerical summary for association between two quantitative variables.\n2. Useful graphs for univariate quantitative variables.\n\nTechniques for categorical variables will be covered in a subsequent topic.\n\nBefore proceeding, let us introduce one of the datasets that we'll be using in\nthis, and subsequent topics. The dataset comes from the \n[UCI Machine Learning Repository](https://archive.ics.uci.edu/), which is a \nvery useful place to get datasets for practice. \n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-stud-perf-intro}\n\n### Student Performance: Dataset Introduction\n\nThe particular dataset can be downloaded from \n[this page](https://archive.ics.uci.edu/dataset/320/student+performance). \nOnce you unzip the file, you will find two `csv` files in the `student/` folder:\n\n* `student-mat.csv` (performance in Mathematics)\n* `student-por.csv` (performance in Portugese)\n\nEach dataset was collected using school reports and questionnaires. Each row \ncorresponds to a student. The columns are attributes, including student grades,\ndemographic information, and other social and school-related information. Each \nfile corresponds to the students' performance in one of the two subjects. For more \ninformation, you can refer to @cortez2008using.\n\n| #  | Feature    | Description (Type)                                   | Details                                                                                                        |\n|----|------------|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| 1  | school     | student's school (binary)                            | \"GP\" - Gabriel Pereira, \"MS\" - Mousinho da Silveira                                                            |\n| 2  | sex        | student's sex (binary)                               | \"F\" - female, \"M\" - male                                                                                       |\n| 3  | age        | student's age (numeric)                              | from 15 to 22                                                                                                  |\n| 4  | address    | student's home address type (binary)                 | \"U\" - urban, \"R\" - rural                                                                                       |\n| 5  | famsize    | family size (binary)                                 | \"LE3\" - less or equal to 3, \"GT3\" - greater than 3                                                             |\n| 6  | Pstatus    | parent's cohabitation status (binary)                | \"T\" - living together, \"A\" - apart                                                                             |\n| 7  | Medu       | mother's education (numeric)                         | 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, 4 - higher education|\n| 8  | Fedu       | father's education (numeric)                         | 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, 4 - higher education|\n| 9  | Mjob       | mother's job (nominal)                               | teacher, health, civil services, at_home, other                                                                |\n| 10 | Fjob       | father's job (nominal)                               | teacher, health, civil services, at_home, other                                                                |\n| 11 | reason     | reason to choose this school (nominal)               | close to home, school reputation, course preference, other                                                     |\n| 12 | guardian   | student's guardian (nominal)                         | mother, father, other                                                                                          |\n| 13 | traveltime | home to school travel time (numeric)                 | 1 - <15 min, 2 - 15 to 30 min, 3 - 30 min to 1 hour, 4 - >1 hour                                               |\n| 14 | studytime  | weekly study time (numeric)                          | 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, 4 - >10 hours                                                |\n| 15 | failures   | number of past class failures (numeric)              | n if 1<=n<3, else 4                                                                                            |\n| 16 | schoolsup  | extra educational support (binary)                   | yes or no                                                                                                      |\n| 17 | famsup     | family educational support (binary)                  | yes or no                                                                                                      |\n| 18 | paid       | extra paid classes within the course subject (Math or Portuguese) (binary) | yes or no                     |\n| 19 | activities | extra-curricular activities (binary)                 | yes or no                                                                                                      |\n| 20 | nursery    | attended nursery school (binary)                     | yes or no                                                                                                      |\n| 21 | higher     | wants to take higher education (binary)              | yes or no                                                                                                      |\n| 22 | internet   | Internet access at home (binary)                     | yes or no                                                                                                      |\n| 23 | romantic   | with a romantic relationship (binary)                | yes or no                                                                                                      |\n| 24 | famrel     | quality of family relationships (numeric)            | from 1 - very bad to 5 - excellent                                                                             |\n| 25 | freetime   | free time after school (numeric)                     | from 1 - very low to 5 - very high                                                                             |\n| 26 | goout      | going out with friends (numeric)                     | from 1 - very low to 5 - very high                                                                             |\n| 27 | Dalc       | workday alcohol consumption (numeric)                | from 1 - very low to 5 - very high                                                                             |\n| 28 | Walc       | weekend alcohol consumption (numeric)                | from 1 - very low to 5 - very high                                                                             |\n| 29 | health     | current health status (numeric)                      | from 1 - very bad to 5 - very good                                                                             |\n| 30 | absences   | number of school absences (numeric)                  | from 0 to 93                                                                                                   |\n\nThere are three columns pertaining to output grades, but G3 is the main one:\n\n| #  | Feature | Description (Type)               | Details                    |\n|----|---------|----------------------------------|----------------------------|\n| 31 | G1      | first period grade (numeric)     | from 0 to 20               |\n| 32 | G2      | second period grade (numeric)    | from 0 to 20               |\n| 33 | G3      | final grade (numeric, output target) | from 0 to 20           |\n\n:::\n\n## Numerical Summaries\n\nTo begin, let us read in the dataset and generate numerical summaries of the \noutput variable of interest (G3). Numerical summaries include:\n\n1. Basic information about the data, e.g. number of observations and missing \n   values.\n2. Measures of central tendency, e.g. mean, median\n3. Measures of spread, e.g. standard deviation, IQR (interquartile range), range.\n\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-stud-perf-num}\n\n### Student Performance: Numerical Summaries\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstud_perf <- read.table(\"data/student/student-mat.csv\", sep=\";\", \n                        header=TRUE)\nsummary(stud_perf$G3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    8.00   11.00   10.42   14.00   20.00 \n```\n\n\n:::\n\n```{.r .cell-code}\nsum(is.na(stud_perf$G3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\nstud_perf  = pd.read_csv(\"data/student/student-mat.csv\", delimiter=\";\")\nstud_perf.G3.describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncount    395.000000\nmean      10.415190\nstd        4.581443\nmin        0.000000\n25%        8.000000\n50%       11.000000\n75%       14.000000\nmax       20.000000\nName: G3, dtype: float64\n```\n\n\n:::\n\n```{.python .cell-code}\n#stud_perf.G3.info()\n```\n:::\n\n\n\n\n\n\n:::\n\nFrom the output, we can understand that we have 395 observations, ranging from 0\nto 20. I would surmise that the data is more or less symmetric in the middle \n(distance from 3rd-quartile to median is identical to distance from median to \n1st-quartile). There are no missing values in the data.\n\nHowever, summaries of a single variable are rarely useful since we do not have \na basis for comparison. In this dataset, we are interested in how the grade varies\nwith one or some of the *other* variables. Let's begin with Mother's education.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(aggregate(G3 ~ Medu, data=stud_perf, FUN=summary), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Medu G3.Min. G3.1st Qu. G3.Median G3.Mean G3.3rd Qu. G3.Max.\n1    0    9.00      12.00     15.00   13.00      15.00   15.00\n2    1    0.00       7.50     10.00    8.68      11.00   16.00\n3    2    0.00       8.00     11.00    9.73      13.00   19.00\n4    3    0.00       8.00     10.00   10.30      13.00   19.00\n5    4    0.00       9.50     12.00   11.76      15.00   20.00\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(stud_perf$Medu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0   1   2   3   4 \n  3  59 103  99 131 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstud_perf[['Medu', 'G3']].groupby('Medu').describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         G3                                                  \n      count       mean       std  min   25%   50%   75%   max\nMedu                                                         \n0       3.0  13.000000  3.464102  9.0  12.0  15.0  15.0  15.0\n1      59.0   8.677966  4.364594  0.0   7.5  10.0  11.0  16.0\n2     103.0   9.728155  4.636163  0.0   8.0  11.0  13.0  19.0\n3      99.0  10.303030  4.623486  0.0   8.0  10.0  13.0  19.0\n4     131.0  11.763359  4.267646  0.0   9.5  12.0  15.0  20.0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n:::\n\nNow we begin to understand the context of G3 a little better. As the education\nlevel of the mother increases, the mean does increase. The middle 50-percent of\nthe grade does seem to increase as well. The exception is the case where the\nmother has no education, but we can see that there are only 3 observations in\nthat category so we should read too much into it.\n\n:::\n\n\nHere are some things to note about numerical summaries: \n\n* If the mean and the median are close, it indicates that the distribution of \n  the data is close to symmetric.\n* The mean is sensitive outliers but the median is not. We shall see more about \n  this in the topic on robust statistics.\n* When the mean is much larger than the median, it suggests that there could   \n  be a few very large observations. It has resulted in a *right-skewed*  \n  distribution. Conversely, if the mean is much smaller than the median, we \n  probably have a *left-skewed* distribution. \n  \nWhile numerical summaries provide us with some basic information about the data,\nthey also leave out a lot. Even for experts, it is possible to have a wrong \nmental idea about the data from the numerical summaries alone. For instance,\nall three histograms in @fig-sum-2 have a mean of 0 and standard deviation of 1!\n\n![Value of a picture](figs/summ_data-02.png){#fig-sum-2 fig-alt=\"3 histograms\" fig-align=\"center\" width=\"65%\"}\n\nThat's why we have to turn to *graphics* as well, in order to summarise our\ndata.\n\n## Graphical Summaries\n\n### Histograms \n\nThe first chart type we shall consider is a histogram. A histogram is a graph\nthat uses bars to portray the frequencies or relative frequencies of the\npossible outcomes for a quantitative variable.\n\nWhen we create a histogram, here are some things that we look for:\n\n1. *What is the overall pattern?*: Do the data cluster together, or is there a \n   gap such that one or more observations deviate from the rest?\n2. *Do the data have a single mound or peak?* If yes, then we have what is \n   known as a unimodal distribution. Data with two 'peaks' are referred to \n   as bimodal, and data with many peaks are referred to as multimodal.\n3. *Is the distribution symmetric or skewed?*\n4. *Are there any suspected outliers?*\n\nHere are some examples of histograms:\n\n::: {layout=\"[[1,1], [1]]\"}\n![Histogram with outliers](figs/summ_data-03.png)\n\n![Bimodal histogram](figs/summ_data-04.png)\n\n![Skewed histograms](figs/summ_data-05.png)\n:::\n\nNow let us turn to making histograms in R and Python.\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-stud-perf-hist}\n\n### Student Performance: Histograms\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(stud_perf$G3, main=\"G3 histogram\")\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-stud-perf-3-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfig = stud_perf.G3.hist(grid=False)\nfig.set_title('G3 histogram');\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/py-stud-perf-3-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n:::\n\nDo you notice anything different with the two histograms? \n\nIn general, we now have a little more information than the 5 number summaries.\nIt appears that there the histogram is not a basic unimodal one. There is a large \nspike of about 40 students who scored very low scores.\n\nAs we mentioned earlier, it is not useful to inspect a histogram in a silo.\nHence we shall condition on Mother's education once more, to create separate\nhistograms for each group. Remember that this is a case where the response\nvariable `G3` is quantitative, and the explanatory variable `Medu` is ordinal.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lattice)\nhistogram(~G3 | Medu, data=stud_perf, type=\"density\")\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-stud-perf-4-3.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nstud_perf.G3.hist(by=stud_perf.Medu, figsize=(15,10), density=True, \n                  layout=(2,3));\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/py-stud-perf-4-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n:::\n\nAlthough we it's not easy to adjust the heights of the two versions to make them\nidentical, we can, by looking at either one, see that the proportion of 0-scores \nreduces as the mother's education increases. Perhaps this is reading too much \ninto the dataset, but there seem to be more scores on the higher end for higher\neducated mothers.\n\n:::\n\n### Density Plots\n\nWhen using histograms, we have to vary the bin size. It is difficult to compare\nhistograms because they are not smooth. An alternative to histograms is the\nkernel density plot. Essentially, this is obtained by smoothing the heights of\nthe rectangles in a histogram.\n\nSuppose we have observed an i.i.d sample $x_1, x_2, \\ldots, x_n$ from a common\ncontinuous pdf $f(\\cdot)$. Then the kernel density estimate at $x$ is given by \n\n$$\n\\hat{f}(x)  = \\frac{1}{nh} \\sum_{i=1}^n K \\left( \\frac{x - x_i}{h} \\right)\n$$\nwhere\n\n* $K$ is a density function. A typical choice is the standard normal.\n* $h$ is a bandwidth, which determines which of the nearest points are used. The \n  effect is similar to the number of bins in a histogram.\n\nThere is no analytical expression for the final estimate. The kernel $K$ weights\nnearby points (to $x$) and returns the value of the density function.\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-stud-perf-dist}\n\n### Student Performance: Density Estimates\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndensityplot(~G3, groups=Medu, data=stud_perf, auto.key = TRUE, bw=1.5)\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-stud-perf-5-3.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nf, axs = plt.subplots(2, 3, squeeze=False, figsize=(15,6))\nout2 = stud_perf.groupby(\"Medu\")\nfor y,df0 in enumerate(out2):\n    tmp = plt.subplot(2, 3, y+1)\n    df0[1].G3.plot(kind='kde')#(kind=\"kde\", ax=tmp)\n    tmp.set_title(df0[0])\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/py-stud-perf-5-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n:::\n\nAs you can see, with density plots it is also possible to overlay them for closer\ncomparison. This is not possible with histograms without some transparency in the \nrectangle colours.\n\n:::\n\n### Boxplots \n\nNow we turn to boxplots. Boxplots provide a skeletal representation of a\ndistribution, and they are very well suited for showing distributions for\nmultiple variables.\n\nHere are the steps for drawing a boxplot: \n\n1. Determine Q1, Q2 and Q3. The box is made from Q1 and Q3. The median is drawn\n   as a line or a dot within the box.\n2. Determine the max-whisker reach: Q3 + 1.5IQR; the min-whisker reach \n   by Q1 − 1.5IQR.\n3. Any data point that is out of the range from the min to max whisker reach\n   is classified as a potential outlier.\n4. Except for the outliers, the maximum point determines the upper whisker and\n   the minimum points determines the lower whisker of a boxplot.\n   \nA boxplot helps us to identify the median, lower and upper quantiles and\noutlier(s) (see @fig-sum-8).\n\n![Boxplot construction](figs/summ_data-08.png){#fig-sum-8 fig-alt=\"Boxplot construction\" fig-align=\"center\" width=\"70%\"}\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-stud-perf-box}\n\n### Student Performance: Boxplots\n\nInstead of using mother's education once again, we use the number of times a \nstudent goes out (`goout`) as the explanatory variable this time. From the\nboxplot outputs, it appears there is no strong strictly increasing/decreasing \ntrend associated with G3. Instead, although the differences between the \ncategories are not large, it seems as though there is an \"optimal\" number of \ntimes that students could go out. Too little and too much leads to lower median\nG3 scores.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbwplot(G3 ~ goout, horizontal = FALSE, data=stud_perf)\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-stud-perf-6-3.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nstud_perf.plot.box(column='G3', by='goout')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nG3    Axes(0.125,0.11;0.775x0.77)\ndtype: object\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/py-stud-perf-6-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n:::\n\n\n:::\n\n### QQ-plots\n\nFinally, we turn to QQ-plots. A Quantile-Quantile plot is a graphical diagnostic \ntool for assessing if a dataset follows a particular distribution. Most of the time\nwe would be interested in comparing against a Normal distribution. \n\nA QQ-plot plots the standardized sample quantiles against the theoretical\nquantiles of a N(0; 1) distribution. If they fall on a straight line, then we\nwould say that there is evidence that the data came from a normal distribution.\n\nEspecially for unimodal datasets, the points in the middle will fall close to the \nline. The value of a QQ-plot is in judging if the tails of the data are fatter \nor thinner than the tails of the Normal. \n\n\n\n\n\n\n::: {#fig-qq-1 .cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![Thinner than Normal](03-summarising_data_files/figure-html/fig-qq-1-3.png){#fig-qq-1-1 fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![Fatter than Normal](03-summarising_data_files/figure-html/fig-qq-1-4.png){#fig-qq-1-2 fig-align='center' width=672}\n:::\n\nQQ-plots\n:::\n\n\n\n\n\n\n::: {.callout-note}\nPlease be careful! Some software/packages will switch the axes (i.e. plot the\nsample quantiles on the x-axis instead of the y-axis, unlike @fig-qq-1). Please\nobserve and interpret accordingly.\n:::\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-concrete-intro}\n\n### Concrete Slump: Dataset Introduction\n\nConcrete is a highly complex material. The slump flow of concrete is not only\ndetermined by the water content, but that is also influenced by other concrete\ningredients. The UCI page for this dataset is\n[here](https://archive.ics.uci.edu/dataset/182/concrete+slump+test). The reference\nfor this article is @yeh2007modeling.\n\nThe data set includes 103 data points. There are 7 input variables, and 3 output\nvariables in the data set. These are the columns in the data:\n\nInput variables (7)(component kg in one $m^3$ concrete):\n\n1.  Cement\n2.  Slag\n3.  Fly ash\n4.  Water\n5.  SP - a super plasticizer to improve consistency.\n6.  Coarse Aggr.\n7.  Fine Aggr.\n\nOutput variables (3):\n\n1. SLUMP (cm) You can read more about the [slump test](https://en.wikipedia.org/wiki/Concrete_slump_test) here. \n2. FLOW (cm) The previous wikipedia page has a link to the flow test too.\n3. 28-day Compressive Strength (Mpa) \n\n::: {.panel-tabset}\n\n#### R code \n\nTo read in the data in R:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconcrete <- read.csv(\"data/concrete+slump+test/slump_test.data\")\nnames(concrete)[c(1,11)] <- c(\"id\", \"Comp.Strength\")\n```\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nconcrete = pd.read_csv(\"data/concrete+slump+test/slump_test.data\")\nconcrete.rename(columns={'No':'id', 'Compressive Strength (28-day)(Mpa)':'Comp_Strength'}, \n                inplace=True)\n```\n:::\n\n\n\n\n\n\n\n:::\n\n:::\n\nLet us consider the Comp.Strength output variable. The histogram overlay\nin @fig-concrete-1 suggests some skewness and fatter tails than the Normal.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Comparing data with reference Normal (blue)](03-summarising_data_files/figure-html/fig-concrete-1-1.png){#fig-concrete-1 fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\nThe next chart is a QQ-plot, for assessing deviations from Normality.\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-concrete-qq}\n\n### Concrete: QQ-plots\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqqnorm(concrete$Comp.Strength)\nqqline(concrete$Comp.Strength)\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-concrete-1-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom scipy import stats\nimport statsmodels.api as sm\nsm.qqplot(concrete.Comp_Strength, line=\"q\");\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/py-concrete-1-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n\n:::\n\nThe deviation of the tails does not seem to be that large, judging from the QQ-plot.\n:::\n\n## Correlation \n\nWhen we are studying two quantitative variables, the most common numerical\nsummary to quantify the relationship between them is the correlation\ncoefficient. Suppose that $x_1, x_2, \\ldots, x_n$ and $y_1, \\ldots, y_n$ are two\nvariables from a set of $n$ objects or people. The sample correlation between\nthese two variables is computed as:\n\n$$\nr = \\frac{1}{n-1} \\sum_{i=1}^n \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\n$$\nwhere $s_x$ and $s_y$ are the sample standard deviations. $r$ is an estimate of \nthe correlation between random variables $X$ and $Y$.\n\n::: {layout-nrow=2}\n![Linear Relation](figs/summ_data-06.png){fig-align=center out-width=60%}\n\n![Non-linear relation](figs/summ_data-07.png){fig-align=center out-width=60%}\n:::\n\nA few things to note about the value $r$, which is also referred to as the \nPearson correlation:\n\n* $r$ is always between -1 and 1.\n* A positive value for $r$ indicates a positive association and a negative value for\n  $r$ indicates a negative association.\n* Two variables have the same correlation, no matter which one is treated as\n  the response and which is treated as the explanatory variable.\n\n## Scatterplot Matrices\n\n::: {style=\"background-color: #D5D1D164; padding: 20px\" #exm-concrete-scatter}\n\n### Concrete: Scatterplots\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncol_to_use <- c(\"Cement\", \"Slag\", \"Comp.Strength\", \"Water\", \"SLUMP.cm.\",\n                \"FLOW.cm.\")\npairs(concrete[, col_to_use], panel = panel.smooth)\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-concrete-2-3.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\npd.plotting.scatter_matrix(concrete[['Cement', 'Slag', 'Comp_Strength', 'Water', \n                                     'SLUMP(cm)', 'FLOW(cm)']], \n                           figsize=(12,12));\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/py-concrete-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\n:::\n\nThe scatterplots allow a visual understanding of the patterns, but it is usually \nalso good to compute the correlation of all pairs of variables. \n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(psych)\ncorPlot(cor(concrete[, col_to_use]), cex=0.8, show.legend = FALSE)\n```\n\n::: {.cell-output-display}\n![](03-summarising_data_files/figure-html/r-concrete-3-3.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n#### Python code\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\ncorr = concrete[['Cement', 'Slag', 'Comp_Strength', 'Water', \n                 'SLUMP(cm)', 'FLOW(cm)']].corr()\ncorr.style.background_gradient(cmap='coolwarm_r')\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style type=\"text/css\">\n#T_5e8db_row0_col0, #T_5e8db_row1_col1, #T_5e8db_row2_col2, #T_5e8db_row3_col3, #T_5e8db_row4_col4, #T_5e8db_row5_col5 {\n  background-color: #3b4cc0;\n  color: #f1f1f1;\n}\n#T_5e8db_row0_col1 {\n  background-color: #cb3e38;\n  color: #f1f1f1;\n}\n#T_5e8db_row0_col2, #T_5e8db_row3_col4 {\n  background-color: #c5d6f2;\n  color: #000000;\n}\n#T_5e8db_row0_col3 {\n  background-color: #f4c6af;\n  color: #000000;\n}\n#T_5e8db_row0_col4 {\n  background-color: #f7b89c;\n  color: #000000;\n}\n#T_5e8db_row0_col5 {\n  background-color: #f3c8b2;\n  color: #000000;\n}\n#T_5e8db_row1_col0, #T_5e8db_row1_col2, #T_5e8db_row1_col4, #T_5e8db_row1_col5, #T_5e8db_row2_col1, #T_5e8db_row2_col3, #T_5e8db_row5_col1 {\n  background-color: #b40426;\n  color: #f1f1f1;\n}\n#T_5e8db_row1_col3 {\n  background-color: #ea7b60;\n  color: #f1f1f1;\n}\n#T_5e8db_row2_col0 {\n  background-color: #cedaeb;\n  color: #000000;\n}\n#T_5e8db_row2_col4 {\n  background-color: #c53334;\n  color: #f1f1f1;\n}\n#T_5e8db_row2_col5, #T_5e8db_row5_col2 {\n  background-color: #e46e56;\n  color: #f1f1f1;\n}\n#T_5e8db_row3_col0 {\n  background-color: #f5c4ac;\n  color: #000000;\n}\n#T_5e8db_row3_col1 {\n  background-color: #f29072;\n  color: #f1f1f1;\n}\n#T_5e8db_row3_col2 {\n  background-color: #c83836;\n  color: #f1f1f1;\n}\n#T_5e8db_row3_col5 {\n  background-color: #96b7ff;\n  color: #000000;\n}\n#T_5e8db_row4_col0 {\n  background-color: #f7b194;\n  color: #000000;\n}\n#T_5e8db_row4_col1 {\n  background-color: #c12b30;\n  color: #f1f1f1;\n}\n#T_5e8db_row4_col2 {\n  background-color: #d0473d;\n  color: #f1f1f1;\n}\n#T_5e8db_row4_col3 {\n  background-color: #c7d7f0;\n  color: #000000;\n}\n#T_5e8db_row4_col5, #T_5e8db_row5_col4 {\n  background-color: #506bda;\n  color: #f1f1f1;\n}\n#T_5e8db_row5_col0 {\n  background-color: #f7bca1;\n  color: #000000;\n}\n#T_5e8db_row5_col3 {\n  background-color: #9dbdff;\n  color: #000000;\n}\n</style>\n<table id=\"T_5e8db\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_5e8db_level0_col0\" class=\"col_heading level0 col0\" >Cement</th>\n      <th id=\"T_5e8db_level0_col1\" class=\"col_heading level0 col1\" >Slag</th>\n      <th id=\"T_5e8db_level0_col2\" class=\"col_heading level0 col2\" >Comp_Strength</th>\n      <th id=\"T_5e8db_level0_col3\" class=\"col_heading level0 col3\" >Water</th>\n      <th id=\"T_5e8db_level0_col4\" class=\"col_heading level0 col4\" >SLUMP(cm)</th>\n      <th id=\"T_5e8db_level0_col5\" class=\"col_heading level0 col5\" >FLOW(cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_5e8db_level0_row0\" class=\"row_heading level0 row0\" >Cement</th>\n      <td id=\"T_5e8db_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n      <td id=\"T_5e8db_row0_col1\" class=\"data row0 col1\" >-0.243553</td>\n      <td id=\"T_5e8db_row0_col2\" class=\"data row0 col2\" >0.445725</td>\n      <td id=\"T_5e8db_row0_col3\" class=\"data row0 col3\" >0.221091</td>\n      <td id=\"T_5e8db_row0_col4\" class=\"data row0 col4\" >0.145913</td>\n      <td id=\"T_5e8db_row0_col5\" class=\"data row0 col5\" >0.186461</td>\n    </tr>\n    <tr>\n      <th id=\"T_5e8db_level0_row1\" class=\"row_heading level0 row1\" >Slag</th>\n      <td id=\"T_5e8db_row1_col0\" class=\"data row1 col0\" >-0.243553</td>\n      <td id=\"T_5e8db_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n      <td id=\"T_5e8db_row1_col2\" class=\"data row1 col2\" >-0.331588</td>\n      <td id=\"T_5e8db_row1_col3\" class=\"data row1 col3\" >-0.026775</td>\n      <td id=\"T_5e8db_row1_col4\" class=\"data row1 col4\" >-0.284037</td>\n      <td id=\"T_5e8db_row1_col5\" class=\"data row1 col5\" >-0.327231</td>\n    </tr>\n    <tr>\n      <th id=\"T_5e8db_level0_row2\" class=\"row_heading level0 row2\" >Comp_Strength</th>\n      <td id=\"T_5e8db_row2_col0\" class=\"data row2 col0\" >0.445725</td>\n      <td id=\"T_5e8db_row2_col1\" class=\"data row2 col1\" >-0.331588</td>\n      <td id=\"T_5e8db_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n      <td id=\"T_5e8db_row2_col3\" class=\"data row2 col3\" >-0.254235</td>\n      <td id=\"T_5e8db_row2_col4\" class=\"data row2 col4\" >-0.223358</td>\n      <td id=\"T_5e8db_row2_col5\" class=\"data row2 col5\" >-0.124029</td>\n    </tr>\n    <tr>\n      <th id=\"T_5e8db_level0_row3\" class=\"row_heading level0 row3\" >Water</th>\n      <td id=\"T_5e8db_row3_col0\" class=\"data row3 col0\" >0.221091</td>\n      <td id=\"T_5e8db_row3_col1\" class=\"data row3 col1\" >-0.026775</td>\n      <td id=\"T_5e8db_row3_col2\" class=\"data row3 col2\" >-0.254235</td>\n      <td id=\"T_5e8db_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n      <td id=\"T_5e8db_row3_col4\" class=\"data row3 col4\" >0.466568</td>\n      <td id=\"T_5e8db_row3_col5\" class=\"data row3 col5\" >0.632026</td>\n    </tr>\n    <tr>\n      <th id=\"T_5e8db_level0_row4\" class=\"row_heading level0 row4\" >SLUMP(cm)</th>\n      <td id=\"T_5e8db_row4_col0\" class=\"data row4 col0\" >0.145913</td>\n      <td id=\"T_5e8db_row4_col1\" class=\"data row4 col1\" >-0.284037</td>\n      <td id=\"T_5e8db_row4_col2\" class=\"data row4 col2\" >-0.223358</td>\n      <td id=\"T_5e8db_row4_col3\" class=\"data row4 col3\" >0.466568</td>\n      <td id=\"T_5e8db_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n      <td id=\"T_5e8db_row4_col5\" class=\"data row4 col5\" >0.906135</td>\n    </tr>\n    <tr>\n      <th id=\"T_5e8db_level0_row5\" class=\"row_heading level0 row5\" >FLOW(cm)</th>\n      <td id=\"T_5e8db_row5_col0\" class=\"data row5 col0\" >0.186461</td>\n      <td id=\"T_5e8db_row5_col1\" class=\"data row5 col1\" >-0.327231</td>\n      <td id=\"T_5e8db_row5_col2\" class=\"data row5 col2\" >-0.124029</td>\n      <td id=\"T_5e8db_row5_col3\" class=\"data row5 col3\" >0.632026</td>\n      <td id=\"T_5e8db_row5_col4\" class=\"data row5 col4\" >0.906135</td>\n      <td id=\"T_5e8db_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n    </tr>\n  </tbody>\n</table>\n```\n\n:::\n:::\n\n\n\n\n\n\n:::\n\nThe plots you see above are known as heatmaps. They enable us to pick out \ngroups of variables that are similar to one another. As you can see from the \nblue block in the lower right corner, `Water`, `SLUMP.cm` and `FLOW.cm` are \nvery similar to one another.\n\n:::\n\n\n\n## References\n\n### Website References\n\n1. [UCI Machine Learning Repository](https://archive.ics.uci.edu/)\n2. [Student Performance Dataset](https://archive.ics.uci.edu/dataset/320/student+performance).\n3. [Kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation)\n",
    "supporting": [
      "03-summarising_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}