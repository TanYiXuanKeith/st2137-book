{
  "hash": "9f5a2cbef45e78a0a62024283afedf7e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ANalysis Of VAriance (ANOVA)\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Introduction\n\nIn the previous topic, we learned how to run 2-sample $t$-tests. The objective \nof these procedures is to compare the means from two groups. Frequently, however,\nthe means of more than two groups need to be compared.\n\nIn this topic, we introduce the *one-way analysis of variance* (ANOVA), which \ngeneralises the $t$-test methodology to more than 2 groups. Hypothesis tests in\nthe ANOVA framework require the assumption of Normality. When this does not hold, \nwe turn to the Kruskal-Wallis test - the non-parametric version, to compare the \ndistributions within the groups.\n\nWhile the $F$-test in ANOVA provides a determination of whether or not the group\nmeans are different, in practice, we would always want to follow up with\nspecific comparisons between groups as well. This topic covers how we can\nconstruct confidence intervals in those cases as well.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-antibiotics-1}\n\n### Effect of Antibiotics\n\nThe following example was taken from @ekstrom2015statistical. An experiment with \ndung from heifers[^1] was carried out in order to explore the influence of antibiotics\non the decomposition of dung organic material. As part of the experiment, 36 heifers\nwere randomly assigned into six groups. \n\n[^1]: A heifer is a young, female cow that has not had her first calf yet.\n\nAntibiotics of different types were added to the feed for heifers in five of the \ngroups. The remaining group served as a control group. For each heifer, a bag of \ndung was dug into the soil, and after 8 weeks the amount of organic material was \nmeasured for each bag.\n\nHere is a boxplot of the data from each group, along with summary statistics in \na table below.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-anova_files/figure-pdf/heifer-plot-1.pdf){fig-align='center'}\n:::\n:::\n\n\n\n\nCompared to the control group, it does appear that the median organic weight of\nthe dung from the other heifer groups is higher. The following table \ndisplays the mean, standard deviation, and count from each group:\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      type   org.mean     org.sd  org.count\n1  Control 2.60333333 0.11877149 6.00000000\n2  Alfacyp 2.89500000 0.11674759 6.00000000\n3 Enroflox 2.71000000 0.16198765 6.00000000\n4 Fenbenda 2.83333333 0.12355835 6.00000000\n5 Ivermect 3.00166667 0.10943796 6.00000000\n6 Spiramyc 2.85500000 0.05446712 4.00000000\n```\n\n\n:::\n:::\n\n\n\n\nObserve that the Spiramycin group only yielded 4 readings instead of 6. Our goal\nin this topic is to understand a technique for assessing if group means are\nstatistically different from one another. Here are the specific analyses that we \nshall carry out:\n\n1. Is there any significant difference, at 5% level, between the mean decomposition\n   level of the groups?\n2. At 5% level, is the mean level for Enrofloxacin different from the control group?\n3. Pharmacologically speaking, Ivermectin and Fenbendazole are similar to each\n   other. Let us call this sub-group (A). They work differently than Enrofloxacin.\n   At 5% level, is there a significant difference between the mean from sub-group \n   A and Enrofloxacin?\n\n:::\n\n\n## One-Way Analysis of Variance\n\n### Formal Set-up\n\nSuppose there are $k$ groups with $n_i$ observations in the $i$-th group. The \n$j$-th observation in the $i$-th group will be denoted by $Y_{ij}$. In the One-Way\nANOVA, we assume the following model:\n\n\\begin{equation}\nY_{ij}  = \\mu + \\alpha_i + e_{ij},\\; i=1,\\ldots,k,\\; j=1,\\ldots,n_i\n\\end{equation}\n\n* $\\mu$ is a constant, representing the underlying mean of all groups taken together.\n* $\\alpha_i$ is a constant specific to the $i$-th group. It represents the difference \n  between the mean of the $i$-th group and the overall mean.\n* $e_{ij}$ represents random error about the mean $\\mu + \\alpha_i$ for an individual \n  observation from the $i$-th group.\n\nIn terms of distributions, we assume that the $e_{ij}$ are i.i.d from a Normal \ndistribution with mean 0 and variance $\\sigma^2$. This leads to the model for \neach observation:\n\n$$\nY_{ij} \\sim N(\\mu + \\alpha_i,\\; \\sigma^2)\n$$ {#eq-yij}\n\nIt is not possible to estimate both $\\mu$ and all the $k$ different $\\alpha_i$'s, \nsince we only have $k$ observed mean values for the $k$ groups. For identifiability \npurposes, we need to constrain the parameters. There are two common constraints used,\nand note that different software have different defaults:\n\n1. Setting $\\sum_{i=1}^k \\alpha_i = 0$, or\n2. Setting $\\alpha_1= 0$.\n\nContinuing on from @eq-yij, let us denote the mean for the $i$-th group as \n$\\overline{Y_i}$, and the overall mean of all observations as \n$\\overline{\\overline{Y}}$. We can then write the deviation of an individual \nobservation from the overall mean as:\n\n$$\nY_{ij} - \\overline{\\overline{Y}} = \\underbrace{(Y_{ij} - \\overline{Y_i})}_{\\text{within}} + \n\\underbrace{(\\overline{Y_i} - \\overline{\\overline{Y}})}_{\\text{between}}\n$$ {#eq-y-dev}\n\nThe first term on the right of the above equation is an indication of \n*within-group variability*. The second term on the right is an indication of \n*between-group variability*. The intuition behind the ANOVA procedure is that \nif the between-group variability is large and the within-group variability is \nsmall, then we have evidence that the group means are different.\n\nIf we square both sides of @eq-y-dev and sum over all observations, we arrive \nat the following equation; the essence of ANOVA:\n\n$$\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\overline{\\overline{Y}} \\right)^2 =\n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( Y_{ij} - \\overline{Y_i} \\right)^2 + \n\\sum_{i=1}^k \\sum_{j=1}^{n_i} \\left( \\overline{Y_i} - \n                                     \\overline{\\overline{Y}} \\right)^2 \n$$\n\nThe squared sums above are referred to as:\n$$\nSS_T = SS_W + SS_B\n$$\n\n* $SS_T$: Sum of Squares Total,\n* $SS_W$: Sum of Squares Within, and\n* $SS_B$: Sum of Squares Between.\n\nIn addition the following definitions are important for understanding the ANOVA \noutput:\n\n1. The Between Mean Square:\n$$\nMS_B = \\frac{SS_B}{k-1}\n$$\n2. The Within Mean Square:\n$$\nMS_W = \\frac{SS_W}{n - k}\n$$\n\nThe mean squares are estimates of the variability between and within groups. The \nratio of these quantities is the test statistic.\n\n### $F$-Test in One-Way ANOVA\n\nThe null and alternative hypotheses are:\n\n\\begin{eqnarray*}\nH_0 &:& \\alpha_i = 0 \\text{ for all } i \\\\\nH_1 &:& \\alpha_i \\ne 0 \\text{ for at least one } i\n\\end{eqnarray*}\n\nThe test statistic is given by \n$$\nF = \\frac{MS_B}{MS_W}\n$$\n\nUnder $H_0$, the statistic $F$ follows an $F$ distribution with $k-1$ and $n-k$\ndegrees of freedom.\n\n### Assumptions\n\nThese are the assumptions that will need to be validated.\n\n1. The observations are independent of each other. This is usually a characteristic \n   of the design of the experiment, and is not something we can always check from\n   the data.\n2. The errors are Normally distributed. Residuals can be calculated as follows:\n$$\nY_{ij} - \\overline{Y_i}\n$$\n   The distribution of these residuals should be checked for Normality.\n3. The variance within each group is the same. In ANOVA, the $MS_W$ is a pooled \n   estimate (across the groups) that is used; in order for this to be valid,\n   the variance within each group should be identical. As in the 2-sample situation,\n   we shall avoid separate hypotheses tests and proceed with the rule-of-thumb\n   that if the ratio of the largest to smallest standard deviation is less than\n   2, we can proceed with the analysis.\n   \n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-antibiotics-2}\n\n### F-test\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#R \nheifers <- read.csv(\"data/antibio.csv\")\nu_levels <- sort(unique(heifers$type))\nheifers$type <- factor(heifers$type, \n                       levels=u_levels[c(2, 1, 3, 4, 5, 6)])\nanova1 <- aov(org ~ type, data=heifers)\nsummary(anova1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ntype         5 0.5908 0.11816   7.973 8.95e-05 ***\nResiduals   28 0.4150 0.01482                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code \n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nheifers = pd.read_csv(\"data/antibio.csv\")\nheifer_lm = ols('org ~ type', data=heifers).fit()\nanova_tab = sm.stats.anova_lm(heifer_lm, type=3,)\nprint(anova_tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            df    sum_sq   mean_sq         F   PR(>F)\ntype       5.0  0.590824  0.118165  7.972558  0.00009\nResidual  28.0  0.415000  0.014821       NaN      NaN\n```\n\n\n:::\n:::\n\n\n\n\n#### SAS output\n\n![](figs/sas_anova_f_test.png){width=650 fig-align=\"center\"}\n\n:::\n\n\nAt the 5% significance level, we reject the null hypothesis to conclude that the\ngroup means are significantly different from one another. This answers question \n1 from @exm-antibiotics-1.\n\nTo extract the estimated parameters, we can use the following code:\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R\ncoef(anova1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)  typeAlfacyp typeEnroflox typeFenbenda typeIvermect typeSpiramyc \n   2.6033333    0.2916667    0.1066667    0.2300000    0.3983333    0.2516667 \n```\n\n\n:::\n:::\n\n\n\n\n#### Python code \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python\nheifer_lm.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    org   R-squared:                       0.587\nModel:                            OLS   Adj. R-squared:                  0.514\nMethod:                 Least Squares   F-statistic:                     7.973\nDate:                Mon, 07 Oct 2024   Prob (F-statistic):           8.95e-05\nTime:                        15:01:00   Log-Likelihood:                 26.655\nNo. Observations:                  34   AIC:                            -41.31\nDf Residuals:                      28   BIC:                            -32.15\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            2.8950      0.050     58.248      0.000       2.793       2.997\ntype[T.Control]     -0.2917      0.070     -4.150      0.000      -0.436      -0.148\ntype[T.Enroflox]    -0.1850      0.070     -2.632      0.014      -0.329      -0.041\ntype[T.Fenbenda]    -0.0617      0.070     -0.877      0.388      -0.206       0.082\ntype[T.Ivermect]     0.1067      0.070      1.518      0.140      -0.037       0.251\ntype[T.Spiramyc]    -0.0400      0.079     -0.509      0.615      -0.201       0.121\n==============================================================================\nOmnibus:                        2.172   Durbin-Watson:                   2.146\nProb(Omnibus):                  0.338   Jarque-Bera (JB):                1.704\nSkew:                          -0.545   Prob(JB):                        0.427\nKurtosis:                       2.876   Cond. No.                         6.71\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\nWhen estimating, both R and Python set one of the $\\alpha_i$ to be equal to 0. \nIn the case of R, it is the coefficient for `Control`, since we set it as the \nfirst level in the factor. For Python, we can tell from the output that the \nconstraint has been placed on the coefficient for `Alfacyp` (since it is missing).\n\nHowever, both estimates are identical. From the R output, we can compute that the \nestimate of the mean for the `Alfacyp` group is \n$$\n2.603 + 0.292 = 2.895\n$$\nFrom the Python output, we can read off (the Intercept term) that the estimate for \n`Alfacyp` is precisely\n$$\n2.895 + 0 = 2.895\n$$\n\nTo check the assumptions, we can use the following code:\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# R\nr1 <- residuals(anova1)\nhist(r1)\n```\n\n::: {.cell-output-display}\n![](08-anova_files/figure-pdf/r-normality-1.pdf)\n:::\n\n```{.r .cell-code}\nqqnorm(r1); qqline(r1)\n```\n\n::: {.cell-output-display}\n![](08-anova_files/figure-pdf/r-normality-2.pdf)\n:::\n:::\n\n\n\n\n#### Python code \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python\nimport matplotlib.pyplot as plt\n\nf, axs = plt.subplots(1, 2, figsize=(8,4))\ntmp = plt.subplot(121)\nheifer_lm.resid.hist();\ntmp = plt.subplot(122)\nsm.qqplot(heifer_lm.resid, line=\"q\", ax=tmp);\n```\n\n::: {.cell-output-display}\n![](08-anova_files/figure-pdf/py-normality-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n:::\n\nFor SAS, we have to create a new column containing the residuals in a temporary \ndataset before creating these plots.\n\n:::\n\n## Comparing specific groups {#sec-spec-grps}\n\nThe $F$-test in a One-Way ANOVA indicates if all means are equal, but does not\nprovide further insight into which particular groups differ. If we had specified\nbeforehand that we wished to test if two particular groups $i_1$ and $i_2$ had\ndifferent means, we could do so with a t-test. Here are the details to compute a\nConfidence Interval in this case:\n\n1. Compute the estimate of the difference between the two means:\n$$\n\\overline{Y_{i_1}} - \\overline{Y_{i_2}}\n$$\n2. Compute the standard error of the above estimator:\n$$\n\\sqrt{MS_W \\left( \\frac{1}{n_{i_1}} + \\frac{1}{n_{i_2}} \\right) }\n$$\n3. Compute the $100(1- \\alpha)%$ confidence interval as:\n$$\n\\overline{Y_{i_1}} - \\overline{Y_{i_2}} \\pm \nt_{n-k, \\alpha/2}  \\times\n\\sqrt{MS_W \\left( \\frac{1}{n_{i_1}} + \\frac{1}{n_{i_2}} \\right) }\n$$\n\n::: {.callout-important}\nIf you notice from the output in @exm-antibiotics-1, the rule-of-thumb regarding\nstandard deviations has not been satisfied. The ratio of largest to smallest standard \ndeviations is slightly more than 2. Hence *we should switch to the non-parametric  version of the test*; \nthe pooled estimate of the variance may not be valid.\n\nHowever, we shall proceed with this dataset just to demonstrate the next few \ntechniques, instead of introducing a new dataset.\n:::\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-antibiotics-3}\n\n### Enrofloxacin vs. Control\n\nLet us attempt to answer question (2), that we had set out earlier in \n@exm-antibiotics-1.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R \nsummary_out <- summary(anova1)\nest_coef <- coef(anova1)\nest1  <- unname(est_coef[3]) # coefficient for Enrofloxacin\nMSW <- summary_out[[1]]$`Mean Sq`[2]\ndf <- summary_out[[1]]$Df[2]\nq1 <- qt(0.025, df, 0, lower.tail = FALSE)\n\nlower_ci <- est1 - q1*sqrt(MSW * (1/6 + 1/4))\nupper_ci <- est1 + q1*sqrt(MSW * (1/6 + 1/4))\ncat(\"The 95% CI for the diff. between Enrofloxacin and Control is (\",\n    format(lower_ci, digits = 3), \",\", format(upper_ci, digits = 3), \").\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe 95% CI for the diff. between Enrofloxacin and Control is (-0.0543,0.268).\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Python\nest1  = heifer_lm.params.iloc[2] - heifer_lm.params.iloc[1]\nMSW = heifer_lm.mse_resid\ndf = heifer_lm.df_resid\nq1 = -stats.t.ppf(0.025, df)\n\nlower_ci = est1 - q1*np.sqrt(MSW * (1/6 + 1/4))\nupper_ci = est1 + q1*np.sqrt(MSW * (1/6 + 1/4))\nprint(f\"The 95% CI for the diff. between Enrofloxacin and control is ({lower_ci:.3f}, {upper_ci:.3f}).\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe 95% CI for the diff. between Enrofloxacin and control is (-0.054, 0.268).\n```\n\n\n:::\n:::\n\n\n\n\n::: \n\nAs the confidence interval contains the value 0, the binary conclusion would be \nto not reject the null hypothesis at the 5% level.\n\n:::\n\n## Contrast Estimation\n\nA more general comparison, such as the comparison of a collection of $l_1$ groups\nwith another collection of $l_2$ groups, is also possible. First, note that a linear \ncontrast is any linear combination of the individual group means such that the \nlinear coefficients add up to 0. In other words, consider $L$ such that \n\n$$\nL = \\sum_{i=1}^k c_i \\overline{Y_i}, \\text{ where } \\sum_{i=1}^k c_i = 0\n$$\n\nNote that the comparison of two groups in @sec-spec-grps is a special case of this\nlinear contrast. \n\nHere is the procedure for computing confidence intervals for a linear contrast:\n\n1. Compute the estimate of the contrast:\n$$\nL = \\sum_{i=1}^k c_i \\overline{Y_i}\n$$\n2. Compute the standard error of the above estimator:\n$$\n\\sqrt{MS_W \\sum_{i=1}^k \\frac{c_i^2}{n_i} }\n$$\n3. Compute the $100(1- \\alpha)%$ confidence interval as:\n$$\nL \\pm\nt_{n-k, \\alpha/2}  \\times\n\\sqrt{MS_W \\sum_{i=1}^k \\frac{c_i^2}{n_i} }\n$$\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-antibiotics-4}\n\n### Comparing collection of groups\n\nLet sub-group 1 consist of Ivermectin and Fenbendazole. \nHere is how we can compute a confidence interval for the difference between \nthis sub-group, and Enrofloxacin.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc1 <- c(-1, 0.5, 0.5)\nn_vals <- c(6, 6, 6)\nL <- sum(c1*est_coef[3:5])\n\nMSW <- summary_out[[1]]$`Mean Sq`[2]\ndf <- summary_out[[1]]$Df[2]\nse1 <- sqrt(MSW * sum( c1^2 / n_vals ) )\n\nq1 <- qt(0.025, df, 0, lower.tail = FALSE)\n\nlower_ci <- L - q1*se1\nupper_ci <- L + q1*se1\ncat(\"The 95% CI for the diff. between the two groups is (\",\n    format(lower_ci, digits = 2), \",\", format(upper_ci, digits = 2), \").\", sep=\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe 95% CI for the diff. between the two groups is (0.083,0.33).\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nc1 = np.array([-1, 0.5, 0.5])\nn_vals = np.array([6, 6, 6,])\nL = np.sum(c1 * heifer_lm.params.iloc[2:5])\n\nMSW = heifer_lm.mse_resid\ndf = heifer_lm.df_resid\nq1 = -stats.t.ppf(0.025, df)\nse1 = np.sqrt(MSW*np.sum(c1**2 / n_vals))\n\nlower_ci = L - q1*se1\nupper_ci = L + q1*se1\nprint(f\"The 95% CI for the diff. between the two groups is ({lower_ci:.3f}, {upper_ci:.3f}).\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe 95% CI for the diff. between the two groups is (0.083, 0.332).\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n## Multiple Comparisons\n\nThe procedures in the previous two subsections correspond to contrasts that we \nhad specified before collecting or studying the data. If, instead, we wished \nto perform particular comparisons after studying the group means, or if we wish \nto compute all pairwise contrasts, then we need to adjust for the fact that we are \nconducting multiple tests. If we do not do so, the chance of making at least one \nfalse positive increases greatly.\n\n### Bonferroni\n\nThe simplest method for correcting for multiple comparisons is to use the Bonferroni \ncorrection. Suppose we wish to perform $m$ pairwise comparisons, either as a test or \nby computing confidence intervals. If we wish to maintain the significance level\nof each test at $\\alpha$, then we should perform each of the $m$ tests/confidence \nintervals at $\\alpha/m$.\n\n### TukeyHSD\n\nThis procedure is known as Tukey's Honestly Significant Difference. It is designed \nto construct confidence intervals for **all** pairwise comparisons. For the same \n$\\alpha$-level, Tukey's HSD method provides shorter confidence intervals than\na Bonferroni correction for all pairwise comparisons.\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(anova1, ordered = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n    factor levels have been ordered\n\nFit: aov(formula = org ~ type, data = heifers)\n\n$type\n                        diff         lwr       upr     p adj\nEnroflox-Control  0.10666667 -0.10812638 0.3214597 0.6563131\nFenbenda-Control  0.23000000  0.01520695 0.4447930 0.0304908\nSpiramyc-Control  0.25166667  0.01152074 0.4918126 0.0358454\nAlfacyp-Control   0.29166667  0.07687362 0.5064597 0.0034604\nIvermect-Control  0.39833333  0.18354028 0.6131264 0.0000612\nFenbenda-Enroflox 0.12333333 -0.09145972 0.3381264 0.5093714\nSpiramyc-Enroflox 0.14500000 -0.09514593 0.3851459 0.4549043\nAlfacyp-Enroflox  0.18500000 -0.02979305 0.3997930 0.1225956\nIvermect-Enroflox 0.29166667  0.07687362 0.5064597 0.0034604\nSpiramyc-Fenbenda 0.02166667 -0.21847926 0.2618126 0.9997587\nAlfacyp-Fenbenda  0.06166667 -0.15312638 0.2764597 0.9488454\nIvermect-Fenbenda 0.16833333 -0.04645972 0.3831264 0.1923280\nAlfacyp-Spiramyc  0.04000000 -0.20014593 0.2801459 0.9953987\nIvermect-Spiramyc 0.14666667 -0.09347926 0.3868126 0.4424433\nIvermect-Alfacyp  0.10666667 -0.10812638 0.3214597 0.6563131\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.stats.multicomp as mc\n\ncp = mc.MultiComparison(heifers.org, heifers.type)\ntk = cp.tukeyhsd()\nprint(tk)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n========================================================\n group1   group2  meandiff p-adj   lower   upper  reject\n--------------------------------------------------------\n Alfacyp  Control  -0.2917 0.0035 -0.5065 -0.0769   True\n Alfacyp Enroflox   -0.185 0.1226 -0.3998  0.0298  False\n Alfacyp Fenbenda  -0.0617 0.9488 -0.2765  0.1531  False\n Alfacyp Ivermect   0.1067 0.6563 -0.1081  0.3215  False\n Alfacyp Spiramyc    -0.04 0.9954 -0.2801  0.2001  False\n Control Enroflox   0.1067 0.6563 -0.1081  0.3215  False\n Control Fenbenda     0.23 0.0305  0.0152  0.4448   True\n Control Ivermect   0.3983 0.0001  0.1835  0.6131   True\n Control Spiramyc   0.2517 0.0358  0.0115  0.4918   True\nEnroflox Fenbenda   0.1233 0.5094 -0.0915  0.3381  False\nEnroflox Ivermect   0.2917 0.0035  0.0769  0.5065   True\nEnroflox Spiramyc    0.145 0.4549 -0.0951  0.3851  False\nFenbenda Ivermect   0.1683 0.1923 -0.0465  0.3831  False\nFenbenda Spiramyc   0.0217 0.9998 -0.2185  0.2618  False\nIvermect Spiramyc  -0.1467 0.4424 -0.3868  0.0935  False\n--------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n::: \n\n## Kruskal-Wallis Procedure\n\nIf the assumptions of the ANOVA procedure are not met, we can turn to a\nnon-parametric version - the Kruskal Wallis test. This latter procedure is a\ngeneralisation of the Wilcoxon Rank-Sum test for 2 independent samples.\n\n### Formal Set-up\n\nThe test statistic compares the average ranks in the individual groups. If these\nare close together, we would be inclined to conclude the treatments are equally \neffective. \n\nThe null hypothesis is that all groups follow the same distribution. The alternative\nhypothesis is that at least one of the groups' distribution differs from another \nby a location shift. We then proceed with:\n\n1. Pool the observations over all samples, thus constructing a combined sample of \n   size $N = \\sum n_i$. Assign ranks to individual observations, using average rank\n   in the case of tied observations. Compute the rank sum $R_i$ for each of the $k$\n   samples.\n2. If there are no ties, compute the test statistic as \n$$\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^k \\frac{R_i^2}{n_i} - 3(N+1)\n$$\n3. If there *are* ties, compute the test statistic as \n$$\nH^* = \\frac{H}{1 - \\frac{\\sum_{j=1}^g (t^3_j - t_j)}{N^3 - N}}\n$$\n\n   where $t_j$ refers to the number of observations with the same value in the \n   $j$-th cluster of tied observations and $g$ is the number of tied groups.\n   \nUnder $H_0$, the test statistic follows a $\\chi^2$ distribution with $k-1$ \ndegrees of freedom.\n\n::: {.callout-important}\nThis test should only be used if $n_i \\ge 5$ for all groups.\n:::\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-antibiotics-4}\n\n### Kruskal-Wallis Test\n\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(heifers$org, heifers$type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  heifers$org and heifers$type\nKruskal-Wallis chi-squared = 19.645, df = 5, p-value = 0.001457\n```\n\n\n:::\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nout = [x[1] for x in heifers.org.groupby(heifers.type)]\nstats.kruskal(*out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKruskalResult(statistic=np.float64(19.644660342717277), pvalue=np.float64(0.0014568414041391754))\n```\n\n\n:::\n:::\n\n\n\n\n#### SAS output\n\n![](figs/sas_kruskal_wallis.png){width=700 fig-align=\"center\"}\n\n:::\n\n:::\n\n## Summary\n\nThe purpose of this topic is to introduce you to the one-way ANOVA model. While\nthere are restrictive distributional assumptions that it entails, I once again\nurge you to look past, at the information the method conveys. It attempts to\ncompare the within-group variance to the between-group variance. Try to avoid\nviewing statistical procedures as flowcharts. If an assumption does not hold, or\na p-value is borderline significant, try to investigate further on how sensitive\nthe result is to those assumptions.\n\nOur job as analysts does not end after reporting the p-value from the $F$-test. We \nshould try to dig deeper to uncover which groups are the ones that are different \nfrom the rest.\n\nFinally, take note that we should specify the contrasts we wish to test/estimate \nupfront, even before collecting the data. Only the Tukey comparison method (HSD) \nis valid if we perform multiple comparisons after inspecting the data.\n\nMost of the theoretical portions in this topic were taken from the textbook\n@rosner2015fundamentals.\n\n## References\n\n### Website References {#sec-web-ref}\n\n1. [Welch's ANOVA](https://statisticsbyjim.com/anova/welchs-anova-compared-to-classic-one-way-anova/) This website discusses an alternative test when the equal variance \n   assumption has not been satisfied. It is for information only; it will not be \n   tested.\n2. [scipy stats](https://docs.scipy.org/doc/scipy/reference/stats.html) This website\n   contains documentation on the distribution-related functions that we might \n   need from scipy stats, e.g. retrieving quantiles.\n3. [Contrast coding](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/)\n\n",
    "supporting": [
      "08-anova_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}