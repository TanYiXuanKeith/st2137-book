{
  "hash": "2cf09cda51479c058b475b0b8d60ffb9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Robust Statistics\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Introduction\n\nIntroductory statistics courses describe and discuss inferential methods based on \nthe assumption that data is Normally distributed. However, we never know the true \ndistribution from which our data has arisen; even from the sample, we may observe\nthat it deviates from Normality in various ways. To begin with, we might observe \nthat the data has heavier tails than a Normal distribution. Second, the data could\nsuggest that the originating distribution is heavily skewed, unlike the symmetric\nNormal. \n\nContinuing to use Normal based methods will result in confidence intervals and\nhypothesis tests that have low power. Instead, statisticians have developed a\nsuite of methods that are **robust** to the assumption of Normality. These\ntechniques may be sub-optimal when the data is truly Normal, but they quickly\noutperform the Normal-based method as soon as the distribution starts to deviate\nfrom Normality.\n\nA third way in which the Normal-based method could breakdown is when our dataset\nhas extreme values, referred to as outliers. In such cases, many investigators\nwill drop the anomalous points and proceed with the analysis on the remaining\nobservations. This is not ideal for the following reasons:\n\n1. The sharp decision to reject an observation is wasteful. We can do better by \ndown-weighting the dubious observations.\n2. It can be difficult to spot or detect outliers in multivariate data.\n\nRobust statistical techniques are those that have high efficiency over a\ncollection of distributions. Efficiency can be measured in terms of variance of\na particular estimator, or in terms of power of a statistical test. In this\ntopic, we shall introduce the concept of robustness and estimators of location\nand scale that have this property. Although we only touch on basic statistics\nin this topic, take note that robust techniques exist for regression and ANOVA\nas well. It is a vastly under-used technique.\n\n## Notation\n\nFor the rest of this topic, let us settle on some notation. Suppose we have an\ni.i.d sample $X_i$ from a continuous pdf $f$. \n\n* We use $q_{f,p}$ to refer to the $p$-th quantile of $f$, i.e. \n$$\nP( X \\le q_{f,p}) = p\n$$\n* For standard Normal quantiles, we use $z_p$.\n$$\n\\Phi(z_p) = P( Z \\le z_p) = p\n$$\n* We denote the order statistics from the sample with $X_{(i)}$. In other words,\n$$\nX_{(1)} \\le X_{(2)} \\le \\cdots \\le X_{(n)}\n$$\n\n## Datasets\n\nFor this topic, we shall use a couple of datasets that are clearly not Normal.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-cu-1}\n\n### Copper in Wholemeal Flour\n\nThe dataset `chem` comes from the package `MASS`. The data was recorded as part \nof an analytical chemistry experiment -- the amount of copper ($\\mu g^{-1}$) was \nmeasured in wholemeal flour was measured using 24 samples.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(MASS)\nhist(chem, breaks = 20)\n```\n\n::: {.cell-output-display}\n![Copper measurements dataset](05-robust_statistics_files/figure-pdf/cu_1-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code}\nsort(chem)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  2.20  2.20  2.40  2.40  2.50  2.70  2.80  2.90  3.03  3.03  3.10  3.37\n[13]  3.40  3.40  3.40  3.50  3.60  3.70  3.70  3.70  3.70  3.77  5.28 28.95\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(chem)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.280417\n```\n\n\n:::\n:::\n\n\n\n\nAlthough 22 out of the 24 points are less than 4, the mean is 4.28. This statistic\nis clearly being affected by the largest two values. Removing them would yield a \nsummary statistic that is more representative of the majority of observations.\nThis topic is about techniques that will work well even in the presence of such \nlarge anomalous values.\n\n:::\n\nThe second dataset is also from a textbook:\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-self-1}\n\n### Self-awareness Dataset\n\nFor a second dataset, we use one on self-awareness from [@wilcox2011introduction],\nwhere participants in a psychometric study were timed how long they could keep a \nportion of an apparatus in contact with a specified target. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nawareness <- c(77, 87, 88, 114, 151, 210, 219, 246, 253, 262, 296, 299, 306,\n               376, 428, 515, 666, 1310, 2611)\nhist(awareness, breaks=10)\n```\n\n::: {.cell-output-display}\n![Self-awareness study timing](05-robust_statistics_files/figure-pdf/self_1-1.pdf){fig-align='center' fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmean(awareness)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 448.1053\n```\n\n\n:::\n:::\n\n\n\n\nJust like the data in @exm-cu-1, this data too is highly skewed to the right. The \nmean of the full dataset is larger than the 3rd quartile!\n\n:::\n\n## Assessing Robustness{#sec-arob}\n\nThe focus of this course is on the computational aspects of performing data \nanalyses. However, this section is a little theoretical. It exists to \nprovide a better overview of robust statistical techniques.\n\n@sec-are introduces an approach for comparing two estimators in general. The \nremaining subsections (@sec-req-rob onwards) briefly list ways in which \nrobust statistics are evaluated.\n\n### Asymptotic Relative Efficiency {#sec-are}\n\nSuppose we wish to estimate a parameter $\\theta$ of a distribution using a\nsample of size $n$. We have two candidate estimators $\\hat{\\theta}$ and\n$\\tilde{\\theta}$.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #def-are}\n\n### Asymptotic Relative Efficiency (ARE)\n\nThe asymptotic relative efficiency of $\\tilde{\\theta}$ relative to $\\hat{\\theta}$ is\n\n$$\nARE(\\tilde{\\theta}; \\hat{\\theta}) = \\lim_{n \\rightarrow \\infty}\n\\frac{\\text{variance of } \\hat{\\theta}}{\\text{variance of } \\tilde{\\theta}}\n$$\n\nUsually, $\\hat{\\theta}$ is the optimal estimator according to some criteria. \n\nWhen using $\\hat{\\theta}$, we only need $ARE$ times as many observations as when \nusing $\\tilde{\\theta}$.\n:::\n\nHere are a couple of examples of commonly used estimators and their ARE.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-mean-med}\n\n### Median versus Mean\n\nIf our data is known to originate from a Normal distribution, due to its \nsymmetry, we can use the sample median *or* the sample mean to estimate $\\mu$. \nLet $\\hat{\\theta} = \\bar{X}$ and $\\tilde{\\theta} = X_{(1/2)}$.\n\nThen it can be shown that \n\n$$\nARE(\\tilde{\\theta}; \\hat{\\theta}) = 2/\\pi \\approx 64\\%\n$$\n\nThe sample median is *less efficient* than the sample mean, when the true\ndistribution is Normal.\n\n:::\n\nHere is an example of when robust statistics prove to be superior to non-robust\nones.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-s-d}\n\n### Contaminated Normal Variance Estimate\n\nSuppose first that we have $n$ observations $Y_i \\sim N(\\mu, \\sigma^2)$, and \nwe wish to estimate $\\sigma^2$. Consider the two estimators:\n\n1. $\\hat{\\sigma}^2 = s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (Y_i - \\bar{Y})^2$\n2. $\\tilde{\\sigma}^2 = d^2 \\pi/2$, where \n$$\nd = \\frac{1}{n} \\sum_i {|Y_i - \\bar{Y}|}\n$$\n\nIn this case, when the underlying distribution truly is Normal, we have that \n$$\nARE(\\tilde{\\sigma}^2; \\hat{\\sigma}^2) =  87.6\\%\n$$\n\nHowever, now consider a situation where $Y_i \\sim N(\\mu, \\sigma^2)$ with \nprobability $1 - \\epsilon$ and $Y_i \\sim N(\\mu, 9\\sigma^2)$ with probability \n$\\epsilon$. Let us refer to this as a *contaminated Normal* distribution.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-robust_statistics_files/figure-pdf/r-demo-cn-1.pdf){fig-align='center' width=75%}\n:::\n:::\n\n\n\n\nAs you can see from the diagram above, the two pdfs are almost indistinguishable\nby eye. However, the ARE values are very different.\n\n| $\\epsilon$ | ARE |\n|----------|----------|\n| 0        | 87.6%   |\n| 0.01     | 144%   |\n\nThe usual $s^2$ loses optimality very quickly; we can obtain more precise\nestimates using $\\tilde{\\sigma}^2$. This example was taken from section 5.5 of\n@venables2013modern.\n\n:::\n\nThere are three main approaches of assessing the robustness of an estimator. Let\nus cover the intuitive ideas here. In this section, $F$ refers to the cdf from \nwhich the sample was obtained.\n\n## Requirements of Robust Summaries {#sec-req-rob}\n\n1.  *Qualitative Robustness*:\n    The first requirement of a robust statistic is that it if the underlying \n    distribution $F$ changes slightly, then the estimate should not change too much.\n2.  *Infinitesimal Robustness*:\n    The second requirement is is tied to the concept of the *influence function*\n    of an estimator. Roughly speaking, the influence function measures the relative \n    extent that a small perturbation in $F$ has on the value of the estimate. In other\n    words, it reflects the influence of adding one more observation to a large \n    sample. \n3.  *Quantitative Robustness*:\n    The final requirement is related to the contaminated distribution we touched on \n    in @exm-s-d. Consider \n$$\nF_{x,\\epsilon} = (1- \\epsilon)F + \\epsilon \\Delta_x\n$$\n    where $\\Delta_x$ is the degenerate probability distribution at $x$. The minimum\n    value of $\\epsilon$ for which the estimator goes to infinity as $x$ gets large,\n    is referred to as the *breakdown point*. \n    For the sample mean, the breakdown point is $\\epsilon = 0$. For the sample \n    median, the breakdown point is $\\epsilon = 0.5$.\n\n## Measures of Location\n\nThe location parameter of a distribution is a value that characterises \"a typical\"\nobservation, or the middle of the distribution. It is not always the mean of the \ndistribution, but in the case of a symmetric distribution it will be.\n\n### M-estimators {#sec-m-mean}\n\nBefore we introduce robust estimators for the location, let us revisit the most\ncommonly used one - the sample mean. Suppose we have observed\n$x_1, x_2, \\ldots, x_n$, a random sample from a $N(\\mu,\\, \\sigma^2)$ distribution. \nAs a reminder, here is how we derive the MLE for $\\mu$.\n\nThe likelihood function is \n$$\nL(\\mu, \\sigma^2) = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-(x_i - \\mu)^2 / (2\\sigma^2) }\n$$\nThe log-likelihood is \n$$\n\\log L = l(\\mu, \\sigma^2) = -n \\log \\sigma - \\frac{n}{2} \\log(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n$$ {#eq-norm-lik}\n\nSetting the partial derivative with respect to $\\mu$ to be 0, we can solve for the \nMLE:\n\\begin{eqnarray*}\n\\frac{\\partial l }{\\partial \\mu} &=& 0 \\\\\n\\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\hat{\\mu}) &=& 0 \\\\\n\\hat{\\mu} &=& \\bar{x}\n\\end{eqnarray*}\n\nObserve that in @eq-norm-lik, we minimised the sum of squared errors, which \narose from *minimising* \n$$\n\\sum_{i=1}^n - \\log f (x_i - \\mu)\n$$\nwhere $f$ is the standard normal pdf. Instead of using $\\log f$, Huber proposed\nusing alternative functions (let's call the function $\\rho$) to derive\nestimators [@huber1992robust]. The new estimator corresponds to \n$$\n\\arg \\min_\\mu \\sum_{i=1}^n \\rho (x_i - \\mu)\n$$ {#eq-rho-min}\n\nThere are constraints on the choice of $\\rho$ above, but we can\nunderstand the resulting estimator through $\\rho$. For instance, $\\psi = \\rho'$\nis referred to as the *influence function*, which measures the relative change in a\nstatistic as a new observation is added. To find the $\\hat{\\mu}$ that minimised \n@eq-rho-min, it is equivalent to setting the derivative to zero and solving for \n$\\hat{\\mu}$:\n$$\n\\sum_{i=1}^n \\psi (x_i - \\mu) = 0 \n$$ {#eq-rho-min}\n\nNote that, in general, the use of the sample mean corresponds to the use of \n$\\rho(x) = x^2$. In that case, $\\psi$ is unbounded, which results in a lot of \nimportance/weight placed on very large values. Robust estimators should have a \nbounded $\\psi$ function.\n\n$$\n\\psi(x) = 2x,\\; x \\in \\mathbb{R}\n$$\n\nThe approach outlined above - the use of $\\rho$ and $\\psi$ to define estimators,\ngave rise to a class of estimators known as M-estimators, since they are\nMLE-like. In the following sections, we shall introduce estimators corresponding\nto various choices of $\\rho$. It is not always easy to identify the $\\rho$ being used,\nbut inspection of the form of $\\psi$ leads to an understanding of how much emphasis \nthe estimator places on large outlying values.\n\n### Trimmed mean \n\nThe $\\gamma$-trimmed mean $(0 < \\gamma \\le 0.5)$ is the mean of a *distribution*\nafter the distribution has been truncated at the $\\gamma$ and $1-\\gamma$\nquantiles. Note that the truncated function has to be renormalised in order to \nbe a pdf.\n\nIn formal terms, suppose that $X$ is a continuous random variable with pdf $f$. The\nusual mean is of course just $\\mu = \\int x f(x) dx$. The trimmed mean of the\ndistribution is\n\n$$\n\\mu_t = \\int_{q_{f,\\gamma}}^{q_{f,1-\\gamma}} x \\frac{f(x)}{1 - 2 \\gamma} dx\n$$ {#eq-tm-pop}\n\nUsing the trimmed mean focuses on the middle portion of a distribution. The\nrecommended value of $\\gamma$ is (0, 0.2]. For a sample $X_1, X_2, \\ldots, X_n$, \nthe estimate is computed (see @wilcox2011introduction page 54) using the\nfollowing algorithm:\n\n1. Compute the value $g = \\lfloor \\gamma n \\rfloor$, where $\\lfloor x \\rfloor$ \n   refers to the floor function[^2].\n2. Drop the largest $g$ and smallest $g$ values from the sample.\n3. Compute \n$$\n\\hat{\\mu_t} = X_t = \\frac{X_{(g+1)} + \\cdots X_{(n-g)}}{n - 2g}\n$$\n\n[^2]: The largest integer less than or equal to $x$.\n\nIt can be shown that the influence function for the trimmed mean is \n$$\n\\psi(x) = \\begin{cases}\nx, & -c < x < c \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nwhich indicates that, with this estimator, large outliers have **no** effect on\nthe estimator.\n\n### Winsorised Mean\n\nThe Winsorised mean is similar to the trimmed mean in the sense that it modifies\nthe tail of the distribution. However, it works by replacing extreme\nobservations with fixed moderate values. The corresponding $\\psi$ \nfunction is\n\n$$\n\\psi(x) = \\begin{cases}\n-c, & x < - c \\\\\nx, & |x| < c \\\\\nc, & x > c\n\\end{cases}\n$$\n\nJust like in the trimmed mean case, we decide on the value $c$ by choosing a\nvalue $\\gamma \\in (0, 0.2]$.  To calculate the Winsorised mean from a sample\n$X_1, X_2, \\ldots, X_n$, we use the following algorithm:\n\n1. Compute the value $g = \\lfloor \\gamma n \\rfloor$.\n2. Replace the smallest $g$ values in the sample with $X_{(g+1)}$ and the largest \n   $g$ values with $X_{(n-g)}$.\n3. Compute the arithmetic mean of the resulting $n$ values.\n$$\nX_w = \\frac{g\\cdot X_{(g+1)} + X_{(g+1)} + \\cdots + X_{(n-g)} + g \\cdot X_{(n-g)}}{n}\n$$\n\n::: {.callout-important}\nNote that the trimmed mean and the Winsorised mean are no longer estimating the \npopulation distribution mean $\\int x f(x) dx$. The three quantities coincide only \nif the population distribution is symmetric. \n\nWhen this is not the case, it is important to be aware of what we are estimating. For\ninstance, using the trimmed/winsorised mean is appropriate if we are interested in \nwhat a \"typical\" observation in the middle of the distribution looks like.\n:::\n\n\n## Measures of Scale\n\n### Sample Standard Deviation\n\nJust as in @sec-m-mean, the MLE of the population variance $\\sigma^2$ is not \nrobust to outliers. It is given by \n$$\ns^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n$$\n\nHere are a few robust alternatives to this estimator. However, take note that, \njust like in the case of location estimators, the following estimators are not \nestimating the standard deviation. We can modify them so that if the underlying \ndistribution truly is Normal, then they do estimate $\\sigma$. However, if the \ndistribution is not Normal, we should treat them as they are: robust measures \nof the spread of the distribution.\n\n### Median Absolute Deviation\n\nFor a random variable $X \\sim f$, the mean absolute deviation $w$ is defined by\n$$\nP(|X - q_{f,0.5} | \\le w) = 0.5\n$$\nWe sometimes refer to $w$ as $MAD(X)$. In other words, it is the median of the\ndistribution associated with $|X - q_{f,0.5}|$; it is the *median of absolute deviations from the median*.\n\nIf observations are truly from a Normal distribution, MAD does not estimate\n$\\sigma$. Instead, it can be shown in this case that $MAD$ estimates \n$z_{0.75} \\sigma$. Hence, in practice, MAD is divided by $z_{0.75}$ so that \nit coincides with $\\sigma$ *if the underlying distribution is Normal*.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #prp-iqr-s}\n\n### MAD for Normal\n\nFor $X \\sim N(\\mu, \\sigma^2)$, the following property holds:\n$$\n\\sigma \\approx 1.4826 \\times MAD(X)\n$$\n\n::: {.proof}\nNote that, since the distribution is symmetric, $\\text{median}(X) = \\mu$. Thus\n\\begin{eqnarray*}\nMAD(X) &=& \\text{median}(| X - \\text{median(X)}|) \\\\\n&=& \\text{median}(| X - \\mu |)\n\\end{eqnarray*}\n\nThus, the $MAD(X)$ is a value $q$ such that \n$$\nP(| X - \\mu | \\le q ) = 0.5\n$$\nEquivalently, we need $q$ such that \n$$\nP\\left( \\left| \\frac{X - \\mu}{\\sigma} \\right| \\le q/\\sigma \\right) = P(|Z| \\le q / \\sigma) = 0.5\n$$\nRemember that we can retrieve values for the standard Normal cdf easily from R\nor Python:\n\n\\begin{eqnarray*}\nP(-q / \\sigma \\le Z \\le q / \\sigma) &=& 0.5 \\\\\n1 - 2 \\times \\Phi(-q / \\sigma) &=& 0.5 \\\\\n-q / \\sigma &=& -0.6745 \\sigma \\\\\nq &=& 0.6745 \\sigma\n\\end{eqnarray*}\n\nThus $MAD(X) = 0.6745 \\sigma$. The implication is that we can estimate $\\sigma$\nin a standard Normal with\n$$\n\\hat{\\sigma} \\approx \\frac{1}{0.6745} MAD(X)\n$$\n\n:::\n\n:::\n\n### Interquartile Range\n\nThe general definition of $IQR(X)$ is \n$$\nq_{f, 0.75} - q_{f,0.25}\n$$\n\nIt is a linear combination of quantiles. Again, we can modify the IQR so that,\nif the underlying distribution is Normal, we are estimating the standard\ndeviation $\\sigma$.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #prp-iqr-s}\n\n### IQR for Normal\n\nFor $X \\sim N(\\mu, \\sigma^2)$, the following property holds:\n\n$$\nIQR(X) \\approx 1.35 \\times \\sigma\n$$\n\n::: {.proof}\nFor $X \\sim N(\\mu, \\sigma^2)$, let $q_{0.25}$ and $q_{0.75}$ represent the 1st \nand 3rd quartiles of the distribution.\n\n\\begin{eqnarray*}\nP(X \\le q_{0.25}) &=& 0.25 \\\\\nP \\left(\\frac{X - \\mu}{\\sigma} \\le \\frac{q_{0.25} - \\mu}{\\sigma} \\right) &=& 0.25 \\\\\nP \\left(Z \\le  \\frac{q_{0.25} - \\mu}{\\sigma} \\right) &=& 0.25\n\\end{eqnarray*}\n\nThus (from R or Python[^1]) we know that \n\n[^1]: In R: `qnorm(0.25)`\n\n\\begin{eqnarray*}\n\\frac{q_{0.25} - \\mu}{\\sigma}   = z_{0.25} &=& -0.675 \\\\\n\\therefore q_{0.25} &=& \\mu - 0.675 \\sigma \n\\end{eqnarray*}\n\nSimilarly, we can derive that $q_{0.75} = \\mu + 0.675 \\sigma$. Now we can\nderive that \n$$\nIQR(X) = q_{0.75} - q_{0.25} = 1.35 \\sigma\n$$\n\nThe implication is that, from sample data, we can estimate $\\sigma$ from the sample \nIQR using:\n$$\n\\hat{\\sigma} = \\frac{IQR(\\{X_1, \\ldots X_n\\})}{1.35}\n$$\n\n:::\n\n:::\n\n# Examples\n\n\nTo begin, let us apply the three estimators of location to the chemical dataset.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-cu-2}\n\n### Location Estimates: Copper Dataset\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(chem)\n## [1] 4.280417\n\nmean(chem, trim = 0.1) # using gamma = 0.1\n## [1] 3.205\n\nlibrary(DescTools)\nvals = quantile(chem, probs=c(0.05, 0.95))\nwin_sample <- Winsorize(chem, vals) # gamma = 0.1\nmean(win_sample)\n## [1] 3.277792\n```\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nchem = pd.read_csv(\"data/mass_chem.csv\")\n\nchem.chem.mean()\n## np.float64(4.2804166666666665)\n\nstats.trim_mean(chem, proportiontocut=0.1)\n## array([3.205])\n\nstats.mstats.winsorize(chem.chem, limits=0.1).mean()\n## np.float64(3.185)\n```\n:::\n\n\n\n\n\n:::\n\nAs we observe, the robust estimates are less affected by the extreme and \nisolate value 28.95. They are more indicative of the general set of observations.\n\n:::\n\nNow we turn the scale estimators for the self-awareness dataset.\n\n::: {style=\"background-color: #D5D1D1; padding: 20px\" #exm-self-2}\n\n### Scale Estimates: Copper Dataset\n\n::: {.panel-tabset}\n\n#### R code \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(awareness)\n## [1] 594.6295\n\nmad(awareness, constant=1) \n## [1] 114\n\nIQR(awareness)\n## [1] 221.5\n```\n:::\n\n\n\n\n#### Python code\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nawareness = np.array([77, 87, 88, 114, 151, 210, 219, 246, 253, 262, 296, 299, 306,\n                      376, 428, 515, 666, 1310, 2611])\n\nawareness.std()\n## np.float64(578.7698292373723)\n\nstats.median_abs_deviation(awareness)\n## np.float64(114.0)\n\nstats.iqr(awareness)\n## np.float64(221.5)\n```\n:::\n\n\n\n\n\n:::\n\n:::\n\n## Summary\n\nIn this topic, we have introduced the concept of robust statistics.\nUnderstanding how these methods work requires a large amount of theoretical\nderivations and set-up. However, although we have not gone into much depth in\nthe notes, we shall investigate the value of these methods in our tutorials.\n\nNotice also that we have not discussed standard errors for these estimators; we shall\nrevisit that idea in the topic on simulation.\n\n## References\n\n### Website References\n\n1. [Wikipedia on Robust Statistics](https://en.wikipedia.org/wiki/Robust_statistics)\n   This page contains visualisations of the $\\psi$ curves for the estimators \n   mentioned above, along with others.\n2. [Scipy stats](https://docs.scipy.org/doc/scipy/reference/stats.html#statsrefmanual):\n   This page contains information on the `median_abs_deviation()`, `trim_mean()`, \n   `mstats.winsorize()` methods in Python.",
    "supporting": [
      "05-robust_statistics_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}