---
title: "Simulation"
format: 
  beamer:
    aspectratio: 169
    theme: Boadilla
    navigation: empty
    colortheme: lily
    footer: "ST2137-2420"

execute:
  echo: true
---

```{r}
#| echo: false
library(knitr)
library(reticulate)
venv_paths <- read.csv("../venv_paths.csv")
id <- match(Sys.info()["nodename"], venv_paths$nodename)
use_virtualenv(venv_paths$path[id])
```

## Objective of Simulation Studies {.smaller}

* To estimate the an expectation $E(X)$.
* Simulation studies involve the use of a computer to generate independent copies 
  of the random variable of interest $X$.

## Example 1 {.smaller}

### Insurance Claims

* Before an FY, an insurance company has to decide how much cash to keep.
* Suppose that claims are independent of each other and are distributed as 
  Exp(1/200)[^1] dollars.
* An actuary recommends $12,000.
* What is the probability that the total claims will exceed the reserve fund?
* If we let $Y$ be the random variable representing the total sum of claims, we
  are interested in estimating $P(Y > 12000)$. 
* Since probabilities are expectations, we can use simulation to estimate this value.

[^1]: $f_X(x) = \frac{1}{200} \exp(-x/200),\; x > 0$

## Example 2 {.smaller}

### Sandwich Shop Closing Time

* Suppose that you run a sandwich shop, which is open from 9am till 5pm. 
* You would like to estimate the mean amount of time you have to work overtime.
* You are willing to assume that the inter-arrival times of customers is 
  $Exp(3)$ hours,  
* Then it is possible to simulate this process to estimate the mean time that
  you would have to remain after 5pm.

## Basic Steps {.smaller}

The basic steps in a simulation study are:

1. Identify the random variable of interest and write a program to simulate it.
2. Generate an iid sample $X_1, X_2, \ldots, X_n$ using this program.
3. Estimate $E(X)$ using $\bar{X}$.

This is sometimes referred to as Monte Carlo Simulation.

## Strong Law of Large Numbers {.smaller}

If $X_1, X_2, \ldots, X_n$ are independent and identically distributed with 
$E(X) < \infty$, then 
$$
\bar{X} =\frac{1}{n} \sum_{i=1}^n X_i\rightarrow E(X) \quad \text{with probability 1.}
$$

## Central Limit Theorem {.smaller}


Let $X_1, X_2, \ldots, X_n$ be i.i.d., and suppose that 

*  $-\infty < E(X_1) = \mu < \infty$. 
* $Var(X_1) = \sigma^2 < \infty$.

Then 
$$
\frac{\sqrt{n} (\bar{X} - \mu)}{\sigma} \Rightarrow N(0,1)
$$
where $\Rightarrow$ denotes convergence in distribution.

## Properties of Estimators {.smaller}

### Sample Estimates

It can be shown that both the sample mean and sample standard deviation are 
unbiased estimators.
$$
E(\bar{X}) = E(X), \quad E(s^2) = \sigma^2
$$
where $s^2 = \frac{\sum (X_i - \bar{X})^2}{n-1}$.

## Properties of Estimators {.smaller}

## Confidence Intervals for Mean {.smaller}

To obtain a $(1-\alpha)100%$ confidence interval for $\mu$, we use the following 
formula, from the CLT:

$$
\bar{X} \pm z_{1-\alpha/2} \frac{s}{\sqrt{n}}
$$

## Confidence Intervals for Probability {.smaller}

$$
X = 
\begin{cases}
1 & \text{with probability $p$} \\
0 & \text{with probability $1- p$}
\end{cases}
$$

In this case, the formula for the CI becomes 
$$
\bar{X} \pm z_{1-\alpha/2} \sqrt{\frac{\bar{X}(1-\bar{X})}{n}}
$$

## Pseudo-Random Number Generators {.smaller}

* Both R and Python contain built-in routines for generating random variables from
  the "named" distributions, 
* PRNGs are routines that generate sequences of deterministic numbers with very very 
  long cycles.
* In both software, we can set the "seed". 

## Random Variable Generation {.smaller}

### R code 

```{r r-rng-1}
#| fig-align: center
#| out-width: 50%
#| echo: false
#R 
set.seed(2137)

opar <- par(mfrow=c(1,3))
Y <- rgamma(50, 2, 3)
hist(Y)

W <- rpois(50, 1.3) # 50 obs from Pois(1.3)
barplot(table(W))

Z <- rbinom(50, size=2, 0.3) # 50 obs from Binom(2, 0.3)
barplot(table(Z))

par(opar)
```

## Random Variable Generation {.smaller}

### Python code 

```{python py-rng-1}
#| fig-align: center
#| echo: false
#| out-width: 50%
#Python
import numpy as np
import pandas as pd
from scipy.stats import binom, gamma, norm, poisson
from scipy import stats
import matplotlib.pyplot as plt

rng = np.random.default_rng(2137)
fig, ax = plt.subplots(1, 3, figsize=(8,4))

ax1 = plt.subplot(131)
r = gamma.rvs(2, 3, size=50, random_state=rng)
ax1.hist(r);

ax1 = plt.subplot(132)
r = poisson.rvs(1.3, size=50, random_state=rng)
ax1.hist(r);

ax1 = plt.subplot(133)
r = binom.rvs(2, 0.3, size=50, random_state=rng)
ax1.hist(r);
```

## Monte-Carlo Integration

* Suppose we wish to evaluate
$$
\int_{-\infty}^{\infty} h(x) f(x) \text{d}x
$$
  where $f(x)$ is a pdf. 
* This is in fact equal to $E(h(X))$, where $X \sim f$.
* Everything depends on:
    1. Being able to introduce a pdf to the integral
    2. Being to able to simulate from that pdf. 

##  MC Integration Example 1 {.smaller}

## Monte Carlo Integration over (0,1)

Suppose Suppose we wish to evaluate
$$
\theta = \int_0^1 e^{2x} dx = \int_0^1 e^{2x} \cdot 1\; dx 
$$

We can identify that this is equal to $E(h(X))$ where 

* $X \sim Unif(0,1)$.   
* $h(X) = e^{2x}$

##  MC Integration Example 1 {.smaller}

Thus we can follow this pseudo-code:

1. Generate $X_1,X_2,\ldots,X_n \sim Unif(0,1)$.
2. Estimate the integral using
$$
\frac{1}{n} \sum_{i=1}^n e^{2 X_i}
$$

In this simple case, we can in fact work out analytically that the integral is 
equal to
$$
\frac{1}{n}(e^2 - 1) = 3.195
$$

##  MC Integration Example 1 {.smaller}

### R code 

```{r r-mc-1}
set.seed(2138)
X <- runif(50000, 0, 1)
hX <- exp(2*X)
(mc_est <- mean(hX))
```

##  MC Integration Example 1 {.smaller}

### Python code

```{python py-mc-1}
from scipy.stats import uniform

X = uniform.rvs(0,1, size=50000, random_state=rng)
hX = np.exp(2*X)
hX.mean()
```

## Confidence Intervals {.smaller}

* The usual 95% confidence interval for a mean is given by 
$$
\bar{X} \pm t_{0.025} s/\sqrt{n}
$$
  where $t_{0.025}$ is the 0.025 quantile of the t-distribution with $n-1$ degrees.
* Let us see if it still works if the data is from an asymmetric distribution,
  $Pois(2.5)$.

## Confidence Intervals {.smaller}

#### R code 

```{r r-ci-95}
# R 
set.seed(2139)
output_vec <- rep(0, length=100)
n <- 20
lambda <- 0.5
for(i in 1:length(output_vec)) {
  X <- rpois(15, .5)
  Xbar <- mean(X)
  s <- sd(X)
  t <- qt(0.975, n-1)
  CI <- c(Xbar - t*s/sqrt(n), Xbar + t*s/sqrt(n))
  if(CI[1] < lambda & CI[2] > lambda) {
    output_vec[i] <- 1
  }
}
mean(output_vec)
```

## Confidence Intervals {.smaller}

#### Python code

```{python py-ci-95}
rng = np.random.default_rng(2137)
output_vec = np.zeros(100)
n = 20
lambda_ = 0.5
for i in range(100):
    X = poisson.rvs(0.5, size=15, random_state=rng)
    Xbar = X.mean()
    s = X.std()
    t = norm.ppf(0.975)
    CI = [Xbar - t*s/np.sqrt(n), Xbar + t*s/np.sqrt(n)]
    if CI[0] < lambda_ and CI[1] > lambda_:
        output_vec[i] = 1
output_vec.mean()

```

## Type I Error {.smaller}

* According to the theory of the $t$-test, if both groups have the
  same mean, we should *falsely* reject the null hypothesis 10% of the time 
  if we perform it at 10% significance level. 
* Let us assess if this is what actually happens.

## Type I Error {.smaller}

### R code 

```{r r-test-1a}
generate_one_test <- function(n=100) {
  X <- rnorm(n)
  Y <- rnorm(n)
  t_test <- t.test(X, Y,var.equal = TRUE)
  # extract the p-value from the t_test
  if(t_test$p.value < 0.10) 
    return(1L) 
  else 
    return(0L)
}
```

## Type I Error {.smaller}

### R code 

```{r r-test-1b}
set.seed(11)
output_vec <- vapply(1:2000, 
                     function(x) generate_one_test(), 
                     1L)
mean(output_vec)
```

## Type I Error {.smaller}

### Python code

```{python py-test-1}
def generate_one_test(n=100):
    X = norm.rvs(0, 1, size=n, random_state=rng)
    Y = norm.rvs(0, 1, size=n, random_state=rng)
    t_test = stats.ttest_ind(X, Y, equal_var=True)
    if t_test.pvalue < 0.10:
        return 1
    else:
        return 0
output_vec = np.array([generate_one_test() for j in range(2000)])
output_vec.mean()
```

## Newspaper Inventory {.smaller}

* Suppose that daily demand for newspaper is approximately gamma distributed, 
  with mean 10,000 and variance 1,000,000. 
* The newspaper prints and distributes 11,000 copies each day. 
* The profit on each newspaper sold is $1, and the loss on each unsold newspaper is 
  0.25. 
* Formally, the daily profit function h is

$$
h(X) = 
\begin{cases}
11000 & \text{if } X ≥ 11000 \\
\lfloor X \rfloor + (11000 - \lfloor X \rfloor)(−0.25) & \text{if } X < 11000
\end{cases}
$$

## Newspaper Inventory {.smaller}

### R code 

```{r r-newspaper-1}
# R code to estimate the expected daily profit
set.seed(2141)
n <- 10000
X <- rgamma(n, 100, rate=1/100)
hX <- ifelse(X >= 11000, 11000, floor(X) + (11000 - floor(X)) * (-0.25))
#mean(hX)

# 90% CI for the mean
s <- sd(hX)
q1 <- qnorm(0.95)
CI <- c(mean(hX) - q1*s/sqrt(n), mean(hX) + q1*s/sqrt(n))
cat("The 90% CI for the mean is (", format(CI[1], digits=2, nsmall=2), ", ", 
    format(CI[2], digits=2, nsmall=2), ").\n", sep="")
```

## Newspaper Inventory {.smaller}

### Python code

```{python py-newspaper-1}
n = 10000
X = gamma.rvs(100, scale=100, size=n, random_state=rng)
hX = np.where(X >= 11000, 11000, np.floor(X) + (11000 - np.floor(X)) * (-0.25))

# 90% CI for the mean
Xbar = hX.mean()
s = hX.std()
t = norm.ppf(0.95)
CI = [Xbar - t*s/np.sqrt(n), Xbar + t*s/np.sqrt(n)]
print(f"The 90% CI for the mean is ({CI[0]: .3f}, {CI[1]: .3f}).")
```
